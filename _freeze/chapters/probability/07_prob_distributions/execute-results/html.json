{
  "hash": "8c17b17cd44f129063fce03508c2e019",
  "result": {
    "engine": "knitr",
    "markdown": "# Distribuzioni di massa e di densità {#sec-prob-distr}\n\n::: {.epigraph}\n> “Ma una densità non è una probabilità. Integrando una densità su un intervallo continuo si ottiene una probabilità.”\n>\n> — **Allen B. Downey**, *Think Bayes*.  \n:::\n\n\n## Introduzione {.unnumbered .unlisted} \n\nNel @sec-prob-random-var abbiamo introdotto il concetto di variabile casuale, distinguendo tra variabili casuali discrete e continue. Per le prime, abbiamo descritto formalmente come assegnare una *distribuzione di massa di probabilità*, mentre per le seconde abbiamo introdotto la nozione di *funzione di densità di probabilità*. Fino a questo punto, i concetti di distribuzione di massa e densità sono stati trattati in termini prevalentemente formali e matematici.\n\nLo scopo di questo capitolo è quello di approfondire queste idee, fornendo un'interpretazione più intuitiva e concreta di tali concetti. Attraverso esempi ed analisi pratiche, cercheremo di chiarire il significato sottostante alle distribuzioni di probabilità, rendendo più accessibili queste fondamentali strutture della teoria delle probabilità.\n\n### Panoramica del capitolo {.unnumbered .unlisted}\n\n- La variabilità di variabili discrete e continue.  \n- La differenza tra massa di probabilità (distribuzioni discrete) e densità di probabilità (distribuzioni continue).  \n- Perché, per una variabile continua, la probabilità di osservare un valore esatto è pari a zero.  \n- Dagli istogrammi alle funzioni di densità di probabilità.  \n- La funzione di ripartizione. \n\n::: {.callout-tip collapse=true}\n## Prerequisiti\n\n- Leggere il capitolo *Random variables and their distributions* del testo di @blitzstein2019introduction.\n:::\n\n::: {.callout-caution collapse=true title=\"Preparazione del Notebook\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n```\n:::\n\n:::\n\n\n## Variabili casuali discrete e continue\n\nUn elemento fondamentale nella comprensione delle distribuzioni di probabilità è la distinzione tra variabili casuali discrete e continue, poiché le distribuzioni di probabilità associate differiscono in modo sostanziale.\n\n- *Variabili casuali discrete:* assumono un numero finito o numerabile di valori. Ad esempio, il numero di successi in una serie di esperimenti o il risultato del lancio di un dado.\n- *Variabili casuali continue:* possono assumere un numero infinito di valori all'interno di un intervallo. Esempi includono il tempo di attesa per un evento o il quoziente intellettivo (QI) di una persona.\n\nQuesta distinzione è fondamentale perché le relative distribuzioni probabilistiche si comportano in modi diversi.\n\n\n### Distribuzioni di probabilità discrete\n\nLe *distribuzioni di probabilità discrete* descrivono fenomeni aleatori con un numero finito o numerabile di esiti possibili. Queste distribuzioni sono rappresentate da una *funzione di massa di probabilità (PMF)*, che assegna una probabilità a ciascun valore della variabile casuale.\n\n::: {.callout-note collapse=true title=\"Esercizio\"}\nConsideriamo un dado sbilanciato con la seguente distribuzione di probabilità:\n\n| Valore di $X$ | Probabilità $p(x)$ |\n|---------------|--------------------|\n| 1             | 0.10              |\n| 2             | 0.15              |\n| 3             | 0.20              |\n| 4             | 0.25              |\n| 5             | 0.20              |\n| 6             | 0.10              |\n\nQuesta tabella rappresenta la *funzione di massa di probabilità* (PMF).\n\nPer visualizzare questa distribuzione, possiamo simulare 1000 lanci del dado e creare un diagramma a barre che rappresenta le frequenze relative osservate. In R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Dati\nset.seed(123)\nprob <- c(0.10, 0.15, 0.20, 0.25, 0.20, 0.10)\nlanci <- sample(1:6, size = 1000, replace = TRUE, prob = prob)\n\n# Creazione di un data frame\ndf <- data.frame(Valore = factor(lanci))\n\n# Creazione del diagramma a barre\nggplot(df, aes(x = Valore)) +\n  geom_bar(aes(y = after_stat(count) / sum(after_stat(count))), fill = \"lightblue\", color=\"black\") +\n  labs(\n    x = \"Valore\",\n    y = \"Frequenza relativa\"\n  )\n```\n\n::: {.cell-output-display}\n![](07_prob_distributions_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nQuando il numero di lanci aumenta, le frequenze relative si avvicinano sempre più alle probabilità teoriche.\n:::\n\n### Distribuzioni di probabilità continue\n\nLe *distribuzioni di probabilità continue* descrivono variabili casuali che possono assumere un numero infinito di valori in un intervallo. In questo caso, la probabilità è rappresentata da una *funzione di densità di probabilità (PDF)*, che descrive la probabilità che la variabile assuma valori in un dato intervallo.\n\n\n#### Probabilità come area sotto la curva\n\nLe distribuzioni continue sono descritte dalla *funzione di densità di probabilità* (PDF). Per una variabile casuale continua $X$, la probabilità che $X$ assuma un valore compreso tra $a$ e $b$ è data dall'area sotto la curva della PDF tra $a$ e $b$:\n\n$$\nP(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx.\n$$\n\n\n::: {.callout-note collapse=true title=\"Esercizio\"}\nIl quoziente intellettivo (QI) è spesso modellato come una variabile casuale continua con distribuzione normale, con media $\\mu = 100$ e deviazione standard $\\sigma = 15$. Possiamo simulare questa distribuzione e confrontare l'istogramma dei dati con la PDF teorica.\n\nSimulazione con 50 osservazioni.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri della distribuzione normale\nmu <- 100\nsigma <- 15\nsize <- 50\n\n# Generare i dati\nset.seed(123)\nx <- rnorm(size, mean = mu, sd = sigma)\n\n# Istogramma e densità\ndata_frame <- data.frame(X = x)\nxmin <- min(x)\nxmax <- max(x)\ndensity_data <- data.frame(\n  X = seq(xmin, xmax, length.out = 100),\n  Density = dnorm(seq(xmin, xmax, length.out = 100), mean = mu, sd = sigma)\n)\n\nggplot(data_frame, aes(x = X)) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    bins = 25,\n    fill = \"lightblue\", color = \"black\"\n  ) +\n  geom_line(\n    data = density_data,\n    aes(x = X, y = Density),\n    color = \"black\",\n    size = 1\n  ) +\n  labs(\n    x = \"Valori del QI\",\n    y = \"Densità\"\n  )\n```\n\n::: {.cell-output-display}\n![](07_prob_distributions_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nCon un campione piccolo, l'istogramma non corrisponde perfettamente alla PDF teorica. Tuttavia, aumentando il numero di osservazioni, l'approssimazione migliora.\n\nSimulazione con 20000 osservazioni. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Generare un campione più grande\nsize <- 20000\nset.seed(123)\nx <- rnorm(size, mean = mu, sd = sigma)\n\n# Aggiornare media e deviazione standard\nmu <- mean(x)\nsigma <- sd(x)\n\n# Creare il grafico\ndata_frame <- data.frame(X = x)\nxmin <- min(x)\nxmax <- max(x)\ndensity_data <- data.frame(\n  X = seq(xmin, xmax, length.out = 100),\n  Density = dnorm(seq(xmin, xmax, length.out = 100), mean = mu, sd = sigma)\n)\n\nggplot(data_frame, aes(x = X)) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    bins = 25,\n    fill = \"lightblue\",\n    color = \"black\"\n  ) +\n  geom_line(\n    data = density_data,\n    aes(x = X, y = Density),\n    color = \"black\",\n    size = 1\n  ) +\n  labs(\n    x = \"Valori del QI\",\n    y = \"Densità\"\n  )\n```\n\n::: {.cell-output-display}\n![](07_prob_distributions_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nCon un campione di grandi dimensioni, l'istogramma riflette molto meglio la PDF teorica.\n:::\n\n\n#### Interpretazione della funzione di densità\n\nLa *funzione di densità di probabilità (PDF)* rappresenta un'astrazione continua dell'istogramma. Quando il numero di osservazioni tende a infinito e la larghezza degli intervalli tende a zero, il profilo dell'istogramma si avvicina alla PDF.\n\n\n#### Proprietà della PDF\n\n1. *Area Totale*: L'area totale sotto la curva della PDF è uguale a 1, poiché rappresenta la probabilità totale.\n2. *Probabilità per Intervalli*: La probabilità che la variabile assuma un valore in un intervallo $[a, b]$ è data dall'area sotto la curva tra $a$ e $b$.\n3. *Probabilità per Singoli Valori*: Per una variabile continua, la probabilità di un singolo valore è sempre zero, poiché corrisponde all'area sotto la curva in un punto.\n\n\n#### Parametri delle distribuzioni di probabilità\n\nLe distribuzioni di probabilità, sia discrete che continue, sono definite da *parametri* che ne determinano le proprietà fondamentali. Questi parametri consentono di adattare il modello probabilistico ai dati osservati.\n\n\n#### Proprietà influenzate dai parametri\n\n1. *Posizione (Tendenza Centrale)*: Indica il valore attorno al quale si concentra la distribuzione. Ad esempio, nella distribuzione normale, la media ($\\mu$) rappresenta il centro della distribuzione.\n2. *Dispersione*: Misura quanto i valori della distribuzione si allontanano dalla posizione centrale. Nella distribuzione normale, la deviazione standard ($\\sigma$) controlla la larghezza della curva.\n3. *Forma*: Determina l'asimmetria o la curtosi della distribuzione. Alcune distribuzioni, come quella gamma o beta, hanno parametri specifici per regolare la forma.\n\n\n## Il paradosso delle variabili casuali continue\n\nUn aspetto controintuitivo delle variabili casuali continue è che la probabilità di osservare esattamente un determinato valore è sempre pari a zero. Per esempio, se consideriamo una variabile continua che rappresenta l'altezza di una persona, la probabilità che l'altezza sia esattamente 170 cm è espressa da\n\n$$\nP(X = 170) = 0.\n$$\n\nPerché accade questo? La risposta sta nel concetto di \"esattezza\". Se riscriviamo 170 cm come 170.00000000000000000000000000000000000 cm (con infiniti decimali), diventa chiaro che stiamo cercando un singolo punto in un continuum infinito.\n\nQuesto non significa che l'evento sia impossibile, ma che nelle variabili continue la probabilità ha senso solo se riferita a intervalli di valori. Infatti, se sommiamo infinite probabilità diverse da zero, supereremmo 1, cosa impossibile.\n\n\n### Due implicazioni importanti\n\nQuesto modo di definire la probabilità nelle variabili continue comporta due implicazioni chiave:\n\n1. *Calcolo della probabilità su intervalli:*  \n   Nelle variabili continue, le probabilità si calcolano solo su intervalli (es.: tra 169.5 cm e 170.5 cm). Questo perché, se ogni singolo valore avesse probabilità > 0, la somma di infiniti valori supererebbe 1 (il che è impossibile).\n2. *Eventi con probabilità zero:*  \n   Il fatto che un evento (ad esempio, $X = 170$) abbia probabilità zero non implica che l’evento sia impossibile. È come cercare un granello di sabbia specifico su una spiaggia infinita: tecnicamente possibile, ma praticamente improbabile.\n\n\n### Il paradosso della probabilità zero\n\nQuesto ragionamento porta a un apparente paradosso: se la probabilità che l'altezza di una persona sia esattamente 170 è zero, come possiamo mai osservare un valore specifico, come 170 (o un qualsiasi altro valore), nella realtà? \n\nUna metafora utile per comprendere questo fenomeno è data dal celebre paradosso di Zenone della freccia. Nel paradosso, si sostiene che, in ogni istante, la freccia sia immobile, e dunque non si dovrebbe mai muovere. Analogamente, ogni singolo valore (es.: 170 cm) ha probabilità zero, ma l’insieme di infiniti valori in un intervallo crea un’area sotto la curva (probabilità) misurabile.\n\n\n### La prospettiva degli infinitesimi\n\nNegli anni '60, il matematico Abraham Robinson sviluppò una teoria matematica rigorosa degli *infinitesimi*, ovvero numeri infinitamente piccoli, diversi da zero. In questo quadro, possiamo reinterpretare la probabilità dei singoli punti nel seguente modo:\n\n1. *Probabilità infinitesimale:*  \n   Un singolo valore puntuale non ha probabilità strettamente zero, bensì infinitamente piccola (un infinitesimo). Pur essendo praticamente indistinguibile da zero nella teoria classica, l'aggregazione (tramite integrazione) di infiniti eventi con probabilità infinitesimali può produrre un valore di probabilità finito e positivo per un intervallo. In altre parole, infiniti punti infinitamente piccoli sommati insieme generano un intervallo di probabilità misurabile e significativa.\n\nIn conclusione, il cosiddetto \"paradosso della probabilità zero\" non rappresenta un vero paradosso, ma evidenzia piuttosto i limiti delle nostre intuizioni quando affrontiamo concetti inerenti variabili continue. La chiave per la comprensione risiede nella distinzione tra il contributo di un singolo punto (infinitesimale o zero, nell’analisi classica) e l’area complessiva calcolata mediante l’integrazione.\n\n\n## La funzione di ripartizione per una variabile casuale continua\n\nLa *funzione di ripartizione*, nota anche come *distribuzione cumulativa*, è uno strumento fondamentale per descrivere il comportamento di una variabile casuale, sia essa discreta o continua. Per una variabile casuale continua $\\Theta$, la funzione di ripartizione $F_{\\Theta}(\\theta)$ è definita come:\n\n$$\nF_{\\Theta}(\\theta) = P(\\Theta \\leq \\theta).\n$$\n\nIn altre parole, $F_{\\Theta}(\\theta)$ rappresenta la probabilità che la variabile $\\Theta$ assuma un valore minore o uguale a $\\theta$. Questa definizione è identica a quella utilizzata per le variabili casuali discrete, ma nel caso continuo assume un significato particolare a causa della natura continua della variabile.\n\n\n### Proprietà della funzione di ripartizione\n\nLa funzione di ripartizione per una variabile casuale continua gode di alcune proprietà importanti:\n\n1. *Monotonicità Crescente*: $F_{\\Theta}(\\theta)$ è una funzione non decrescente. Ciò significa che, all'aumentare di $\\theta$, la probabilità $P(\\Theta \\leq \\theta)$ non diminuisce.\n2. *Limiti agli Estremi*:\n   - Quando $\\theta \\to -\\infty$, $F_{\\Theta}(\\theta) \\to 0$.\n   - Quando $\\theta \\to +\\infty$, $F_{\\Theta}(\\theta) \\to 1$.\n3. *Continuità*: Per una variabile casuale continua, $F_{\\Theta}(\\theta)$ è una funzione continua. Questo differisce dal caso discreto, dove la funzione di ripartizione è a gradini.\n\n\n### Calcolo delle probabilità per intervalli\n\nUna delle applicazioni più utili della funzione di ripartizione è il calcolo della probabilità che la variabile casuale $\\Theta$ assuma valori all'interno di un intervallo specifico. Dati due valori $\\theta_1$ e $\\theta_2$ (con $\\theta_1 < \\theta_2$), la probabilità che $\\Theta$ sia compreso tra $\\theta_1$ e $\\theta_2$ è data da:\n\n$$\nP(\\theta_1 < \\Theta \\leq \\theta_2) = F_{\\Theta}(\\theta_2) - F_{\\Theta}(\\theta_1).\n$$\n\nQuesta formula è particolarmente utile perché, nel caso delle variabili continue, la probabilità di un singolo punto è sempre zero. Pertanto, per calcolare probabilità significative, è necessario considerare intervalli di valori.\n\n\n### Relazione con la funzione di densità di probabilità (PDF)\n\nLa funzione di ripartizione è strettamente legata alla *funzione di densità di probabilità (PDF)*, $f(\\theta)$. Mentre la PDF descrive la densità di probabilità in ogni punto, la funzione di ripartizione rappresenta l'area sotto la curva della PDF fino a un certo valore $\\theta$. Formalmente, la funzione di ripartizione si ottiene integrando la PDF:\n\n$$\nF_{\\Theta}(\\theta) = \\int_{-\\infty}^{\\theta} f(t) \\, dt.\n$$\n\nQuesta relazione evidenzia come la funzione di ripartizione sia una rappresentazione cumulativa della probabilità, ottenuta sommando (o integrando) i contributi della densità di probabilità fino al valore $\\theta$.\n\n\n::: {.callout-note collapse=true title=\"Esercizio\"}\nConsideriamo una variabile casuale $\\Theta$ con distribuzione normale standard (media $\\mu = 0$ e deviazione standard $\\sigma = 1$). La PDF è data da:\n\n$$\nf(\\theta) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\theta^2 / 2}.\n$$\n\nLa funzione di ripartizione $F_{\\Theta}(\\theta)$ è l'integrale di questa funzione da $-\\infty$ a $\\theta$:\n\n$$\nF_{\\Theta}(\\theta) = \\int_{-\\infty}^{\\theta} \\frac{1}{\\sqrt{2\\pi}} e^{-t^2 / 2} \\, dt.\n$$\n\nQuesta funzione non ha una forma chiusa semplice, ma può essere calcolata numericamente o consultata in tabelle statistiche. Ad esempio, per $\\theta = 1$, $F_{\\Theta}(1) \\approx 0.8413$, il che significa che la probabilità che $\\Theta$ sia minore o uguale a 1 è circa l'84.13%.\n:::\n\n\n### Interpretazione grafica\n\nGraficamente, la funzione di ripartizione rappresenta l'area sotto la curva della PDF a sinistra del valore $\\theta$. Ad esempio, se consideriamo la distribuzione normale standard:\n\n- Per $\\theta = 0$, $F_{\\Theta}(0) = 0.5$, poiché la media della distribuzione è 0 e la curva è simmetrica.\n- Per $\\theta = 1$, $F_{\\Theta}(1) \\approx 0.8413$, come visto sopra.\n- Per $\\theta = -1$, $F_{\\Theta}(-1) \\approx 0.1587$, poiché la coda sinistra della distribuzione contiene il 15.87% della probabilità.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07_prob_distributions_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIn conclusione, la funzione di ripartizione è uno strumento essenziale per comprendere e lavorare con variabili casuali continue. Essa non solo fornisce una rappresentazione cumulativa della probabilità, ma permette anche di calcolare probabilità per intervalli e di collegare la PDF alla distribuzione complessiva della variabile. Attraverso la sua relazione con la PDF, la funzione di ripartizione offre un ponte tra la descrizione locale (densità) e quella globale (probabilità cumulativa) di una variabile casuale continua.\n\n## Interpretazioni bayesiana e frequentista della PDF\n\nIn questo capitolo, abbiamo introdotto la funzione di densità di probabilità come limite del profilo di un istogramma, una descrizione intuitiva e utile per comprendere il concetto di densità. Questa interpretazione corrisponde, tuttavia, alla visione frequentista della densità di probabilità. Nella statistica Bayesiana, l’interpretazione è diversa e merita una spiegazione separata.\n\n\n### Interpretazione frequentista \n\n**Concetto di ripetizione degli esperimenti:**\n\n- *Idea di frequenza relativa:*  \n  Nel paradigma frequentista la probabilità è intesa come il limite della frequenza relativa di un evento ottenuto al ripetere un esperimento un numero molto elevato di volte. Immaginiamo di eseguire un esperimento molte volte, ad ogni ripetizione si ottiene un valore di $x$. Se costruiamo un istogramma di questi valori, questo istogramma diventa sempre più “liscio” man mano che il numero delle ripetizioni aumenta, fino a convergere alla PDF $p(x)$.\n\n- *PDF come istogramma limite:*  \n  La PDF rappresenta la distribuzione dei valori osservati in una serie di ripetizioni dell’esperimento. In altre parole, essa descrive quanto frequentemente, in una ipotetica serie infinita di esperimenti, il valore $x$ assume un determinato intervallo.\n\n- *Esempio intuitivo:*  \n  Se misuriamo l'altezza degli individui in una popolazione, nel contesto frequentista, la PDF ci dice quale frazione di individui cade in un certo intervallo di altezza se potessimo misurare ogni possibile individuo (o eseguire ripetutamente misurazioni indipendenti in una popolazione \"ideale\").\n\n\n### Interpretazione bayesiana \n\n**Concetto di incertezza e credenza:**\n\n- *Parametro come variabile casuale:*  \n  In statistica bayesiana, i parametri non sono visti come quantità fisse, ma come incerti. Si assume che ogni parametro (o dato osservato) abbia una propria distribuzione che riflette la nostra incertezza su di esso.  \n  - Ad esempio, se stiamo stimando un parametro $\\theta$ (ad esempio la media di una distribuzione), in un approccio bayesiano attribuiamo a $\\theta$ una distribuzione di probabilità che esprime quanto sia plausibile ciascun valore di $\\theta$, dati i dati osservati e le nostre conoscenze pregresse.\n\n- *PDF come distribuzione di credenze:*  \n  La PDF, in questo contesto, non descrive una frequenza relativa osservabile sperimentalmente (perché l’esperimento non viene ripetuto infinite volte, o perché $x$ è un valore fisso ma incerto), ma esprime il grado di fiducia o la plausibilità che il valore “vero” di $x$ (o di un parametro) si trovi in un certo intervallo.  \n  - È come “spalmare” la nostra incertezza su tutti i valori possibili: la sfumatura lungo l’asse $x$ rappresenta la distribuzione delle nostre credenze.\n\n- *Analogia con la densità di materia:*  \n  Un’utile analogia è quella della densità di materia $\\rho(x)$ in meccanica classica: la densità non descrive la posizione precisa di ogni atomo, ma come la materia (o, in questo caso, la probabilità) è distribuita lungo l’asse $x$. Allo stesso modo, in una PDF bayesiana, non sono i “valori di $x$” ad essere distribuiti (in termini di frequenza osservabile), ma è la nostra “incertezza” a essere distribuita sui possibili valori.\n\n- *Esempio intuitivo:*  \n  Immagina di dover stimare la probabilità che una certa ipotesi sia vera, ad esempio la media dell’altezza in una popolazione. Invece di pensare a misurazioni ripetute, consideri il valore medio come fisso ma incerto. La PDF bayesiana esprime il grado di credenza per ciascun possibile valore della media, in base ai dati raccolti e alle informazioni a priori.\n\n\n### Confronto \n\n- *Frequentista:*  \n  - *Focus:* Distribuzione dei dati.\n  - *Interpretazione:* La PDF descrive come i valori di $x$ sarebbero distribuiti se ripetessimo l’esperimento infinite volte.\n  - *Esempio:* L’istogramma dei dati osservati in una lunga serie di esperimenti.\n\n- *Bayesiano:*  \n  - *Focus:* Distribuzione della nostra incertezza o credenza.\n  - *Interpretazione:* La PDF riflette quanto sia plausibile ciascun valore di $x$ (o di un parametro) dato l’informazione disponibile, senza necessità di ripetere l’esperimento.\n  - *Esempio:* La distribuzione a posteriori di un parametro dopo aver combinato dati osservati e informazioni a priori.\n\n\n::: {#fig-loredo2024}\n![](../../figures/loredo_2024.png){width=\"80%\"}\n\nInterpretazioni frequentista e bayesiana di una PDF (curva blu) per una quantità reale $x$. A sinistra: interpretazione frequentista come istogramma limite dei valori di $x$ nelle ripetizioni; i valori di $x$ sono distribuiti secondo la PDF. A destra: interpretazione bayesiana, con $x$ che assume un valore fisso ma incerto per il caso specifico (rappresentato dal punto sull'asse $x$), con la probabilità distribuita sui valori possibili (raffigurata con una sfumatura lungo l'asse $x$). [Figura tratta da @loredo2024bayesian]\n:::\n\n\nIn sintesi, questa distinzione tra interpretazioni non è solo una questione di semantica, ma ha implicazioni pratiche nella formulazione di modelli statistici e nell'interpretazione dei risultati. Mentre l'approccio frequentista è spesso utilizzato quando si può concettualmente pensare a ripetizioni infinite dell’esperimento, l'approccio bayesiano è particolarmente utile quando si vuole esprimere e aggiornare la propria incertezza su una quantità basandosi sia su dati che su conoscenze pregresse.\n\n\n## Riflessioni conclusive {.unnumbered .unlisted} \n\nLa funzione di densità di probabilità (PDF) costituisce il fondamento per la descrizione delle variabili casuali continue, consentendo di associare le probabilità ad intervalli, tramite il calcolo dell'area sottesa alla curva. In questo contesto, la probabilità di osservare un valore esatto risulta zero, non per impossibilità dell’evento, ma perché in un insieme continuo ogni singolo punto contribuisce con un’area infinitesimale.\n\nIl paradosso apparente, secondo cui la somma di infiniti contributi nulli porta a una probabilità totale positiva, si risolve grazie alla teoria dell’integrazione. Integrando i contributi infinitesimali lungo un intervallo, si ottiene una quantità finita che rappresenta la probabilità complessiva dell’evento. Un’interpretazione alternativa, fornita dalla teoria degli infinitesimi di Abraham Robinson, consente di attribuire a tali eventi probabilità infinitesimali, distinguendo tra diverse “grandezze” e chiarendo ulteriormente il processo di aggregazione verso un valore unitario.\n\nNel campo della *data science*, le distribuzioni di probabilità—formalmente rappresentate da $p(x)$—sono strumenti indispensabili per modellare la variabilità osservabile in una popolazione. Queste distribuzioni non mirano a riprodurre in maniera dettagliata ogni aspetto della realtà, ma offrono un modello semplificato che consente di generalizzare i dati osservati e di formulare previsioni rigorose sui fenomeni futuri. In altre parole, $p(x)$ non rappresenta la popolazione nel suo complesso, bensì un’astrazione matematica che cattura l’incertezza e la variabilità del fenomeno studiato.\n\n\n::: {.callout-tip title=\"Esercizio\" collapse=\"true\"}\n**Esercizio 1: Variabili Casuali Discrete e Continue**\n\nUtilizzando i dati raccolti sulla **Satisfaction with Life Scale (SWLS)** e sulla **Scala della Rete Sociale di Lubben a 6 item (LSNS-6)**, classifica le seguenti variabili come **discrete** o **continue**:\n\n1. Il punteggio totale della SWLS.\n2. Il numero di amici con cui uno studente si sente a proprio agio nel parlare di questioni personali.\n3. Il tempo (in minuti) che uno studente trascorre con amici durante una settimana.\n4. Il numero di volte che uno studente ha contattato un parente nell'ultimo mese.\n5. Il livello di soddisfazione della vita misurato su una scala da 1 a 7.\n\nSpiega il motivo della tua classificazione per ciascuna variabile.\n\n**Esercizio 2: Distribuzioni di Probabilità Discrete**\n\nConsideriamo la distribuzione del numero di amici con cui uno studente si sente a proprio agio nel parlare di questioni personali, misurata attraverso la LSNS-6. Supponiamo che la distribuzione sia la seguente (ma nell'esercizio **usa le frequenze relative trovate nel campione di dati raccolto**):\n\n| Numero di amici | Probabilità |\n|---------------|--------------|\n| 0             | 0.05         |\n| 1             | 0.15         |\n| 2             | 0.25         |\n| 3             | 0.30         |\n| 4             | 0.15         |\n| 5             | 0.10         |\n\n1. Verifica che questa sia una distribuzione di probabilità valida.\n2. Qual è la probabilità che uno studente abbia almeno 3 amici con cui si sente a proprio agio nel parlare di questioni personali?\n3. Qual è la probabilità che abbia meno di 2 amici?\n4. Calcola il valore atteso (media) e la varianza di questa distribuzione.\n\n**Esercizio 3: Distribuzioni di Probabilità Continue**\n\nIl punteggio totale della SWLS può essere approssimato da una distribuzione normale con media 20 e deviazione standard 5.\n\n1. Qual è la probabilità che un individuo scelto a caso abbia un punteggio superiore a 25?\n2. Qual è la probabilità che un individuo abbia un punteggio compreso tra 15 e 25?\n3. Qual è il valore del punteggio che delimita il 10% superiore della distribuzione?\n\n(Suggerimento: utilizza la funzione di ripartizione della distribuzione normale standard per calcolare queste probabilità.)\n\n**Esercizio 4: Legge della Probabilità Totale**\n\nSi sa che il 60% degli studenti proviene da un ambiente con un forte supporto sociale, mentre il 40% ha un supporto sociale limitato. Inoltre, si sa che:\n- La probabilità che uno studente con forte supporto sociale abbia un punteggio SWLS superiore a 20 è 0.75.\n- La probabilità che uno studente con supporto sociale limitato abbia un punteggio SWLS superiore a 20 è 0.50.\n\nQual è la probabilità che uno studente scelto a caso abbia un punteggio SWLS superiore a 20?\n\n**Esercizio 5: Teorema di Bayes e Supporto Sociale**\n\nRiprendendo l'esercizio precedente, calcola la probabilità che uno studente provenga da un ambiente con forte supporto sociale **dato** che il suo punteggio SWLS è superiore a 20.\n:::\n\n::: {.callout-tip title=\"Soluzioni\" collapse=\"true\"}\n**Esercizio 1: Variabili Casuali Discrete e Continue**\n\nUtilizzando i dati raccolti sulla **Satisfaction with Life Scale (SWLS)** e sulla **Scala della Rete Sociale di Lubben a 6 item (LSNS-6)**, classifica le seguenti variabili come **discrete** o **continue**:\n\n1. Il punteggio totale della SWLS. (**Continuo**)\n2. Il numero di amici con cui uno studente si sente a proprio agio nel parlare di questioni personali. (**Discreto**)\n3. Il tempo (in minuti) che uno studente trascorre con amici durante una settimana. (**Continuo**)\n4. Il numero di volte che uno studente ha contattato un parente nell'ultimo mese. (**Discreto**)\n5. Il livello di soddisfazione della vita misurato su una scala da 1 a 7. (**Discreto**)\n\n**Esercizio 2: Distribuzioni di Probabilità Discrete**\n\nConsideriamo la distribuzione del numero di amici con cui uno studente si sente a proprio agio nel parlare di questioni personali, misurata attraverso la LSNS-6. Supponiamo che la distribuzione sia la seguente:\n\n| Numero di amici | Probabilità |\n|---------------|--------------|\n| 0             | 0.05         |\n| 1             | 0.15         |\n| 2             | 0.25         |\n| 3             | 0.30         |\n| 4             | 0.15         |\n| 5             | 0.10         |\n\n1. **Verifica della distribuzione:** La somma delle probabilità deve essere 1:\n   \n   $$ 0.05 + 0.15 + 0.25 + 0.30 + 0.15 + 0.10 = 1.00 $$\n   \n   Poiché la somma è 1, la distribuzione è valida.\n\n2. **Probabilità di almeno 3 amici:**\n   \n   $$ P(X \\geq 3) = P(3) + P(4) + P(5) = 0.30 + 0.15 + 0.10 = 0.55 $$\n   \n3. **Probabilità di meno di 2 amici:**\n   \n   $$ P(X < 2) = P(0) + P(1) = 0.05 + 0.15 = 0.20 $$\n   \n4. **Valore atteso e varianza:**\n   \n   $$ E(X) = \\sum x P(x) = (0 \\times 0.05) + (1 \\times 0.15) + (2 \\times 0.25) + (3 \\times 0.30) + (4 \\times 0.15) + (5 \\times 0.10) = 2.65 $$\n   \n   $$ Var(X) = E(X^2) - (E(X))^2 $$\n   \n   $$ E(X^2) = (0^2 \\times 0.05) + (1^2 \\times 0.15) + (2^2 \\times 0.25) + (3^2 \\times 0.30) + (4^2 \\times 0.15) + (5^2 \\times 0.10) = 8.05 $$\n   \n   $$ Var(X) = 8.05 - (2.65)^2 = 1.06 $$\n\n**Esercizio 3: Distribuzioni di Probabilità Continue**\n\nIl punteggio totale della SWLS può essere approssimato da una distribuzione normale con media 20 e deviazione standard 5.\n\n1. **Probabilità che il punteggio sia superiore a 25:**\n   \n   $$ P(X > 25) = 1 - P(X \\leq 25) $$\n   \n   Standardizziamo:\n   \n   $$ Z = \\frac{25 - 20}{5} = 1 $$\n   \n   Usando le tabelle della distribuzione normale:\n   \n   $$ P(Z \\leq 1) = 0.8413 \\Rightarrow P(X > 25) = 1 - 0.8413 = 0.1587 $$\n   \n2. **Probabilità che il punteggio sia tra 15 e 25:**\n   \n   $$ P(15 \\leq X \\leq 25) = P(Z \\leq 1) - P(Z \\leq -1) $$\n   \n   $$ = 0.8413 - 0.1587 = 0.6826 $$\n   \n3. **Percentile 90 della distribuzione:**\n   \n   Il valore di Z per il 90% è 1.28.\n   \n   $$ X = 20 + (1.28 \\times 5) = 26.4 $$\n\n**Esercizio 4: Legge della Probabilità Totale**\n\n$$ P(SWLS > 20) = P(SWLS > 20 | S) P(S) + P(SWLS > 20 | \\neg S) P(\\neg S) $$\n   \n$$ = (0.75 \\times 0.60) + (0.50 \\times 0.40) $$\n   \n$$ = 0.45 + 0.20 = 0.65 $$\n\n**Esercizio 5: Teorema di Bayes e Supporto Sociale**\n\n$$ P(S | SWLS > 20) = \\frac{P(SWLS > 20 | S) P(S)}{P(SWLS > 20)} $$\n   \n$$ = \\frac{(0.75 \\times 0.60)}{0.65} $$\n   \n$$ = \\frac{0.45}{0.65} = 0.6923 $$\n   \nQuindi, la probabilità che uno studente provenga da un ambiente con forte supporto sociale dato che il suo punteggio SWLS è superiore a 20 è circa **69.2%**.\n:::\n\n\n::: {.callout-note collapse=true title=\"Informazioni sull'ambiente di sviluppo\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#>  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#>  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#> [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#> [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#> [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#> [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#> [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#> [25] here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#>  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#>  [7] digest_0.6.37         timechange_0.3.0      estimability_1.5.1   \n#> [10] lifecycle_1.0.4       survival_3.8-3        magrittr_2.0.3       \n#> [13] compiler_4.5.1        rlang_1.1.6           tools_4.5.1          \n#> [16] yaml_2.3.10           knitr_1.50            labeling_0.4.3       \n#> [19] bridgesampling_1.1-2  htmlwidgets_1.6.4     curl_7.0.0           \n#> [22] pkgbuild_1.4.8        RColorBrewer_1.1-3    abind_1.4-8          \n#> [25] multcomp_1.4-28       withr_3.0.2           purrr_1.1.0          \n#> [28] grid_4.5.1            stats4_4.5.1          colorspace_2.1-1     \n#> [31] xtable_1.8-4          inline_0.3.21         emmeans_1.11.2-8     \n#> [34] scales_1.4.0          MASS_7.3-65           cli_3.6.5            \n#> [37] mvtnorm_1.3-3         rmarkdown_2.29        ragg_1.5.0           \n#> [40] generics_0.1.4        RcppParallel_5.1.11-1 cachem_1.1.0         \n#> [43] stringr_1.5.1         splines_4.5.1         parallel_4.5.1       \n#> [46] vctrs_0.6.5           V8_7.0.0              Matrix_1.7-4         \n#> [49] sandwich_3.1-1        jsonlite_2.0.0        arrayhelpers_1.1-0   \n#> [52] systemfonts_1.2.3     glue_1.8.0            codetools_0.2-20     \n#> [55] distributional_0.5.0  lubridate_1.9.4       stringi_1.8.7        \n#> [58] gtable_0.3.6          QuickJSR_1.8.0        htmltools_0.5.8.1    \n#> [61] Brobdingnag_1.2-9     R6_2.6.1              textshaping_1.0.3    \n#> [64] rprojroot_2.1.1       evaluate_1.0.5        lattice_0.22-7       \n#> [67] backports_1.5.0       memoise_2.0.1         broom_1.0.9          \n#> [70] snakecase_0.11.1      rstantools_2.5.0      coda_0.19-4.1        \n#> [73] gridExtra_2.3         nlme_3.1-168          checkmate_2.3.3      \n#> [76] xfun_0.53             zoo_1.8-14            pkgconfig_2.0.3\n```\n:::\n\n:::\n\n## Bibliografia {.unnumbered .unlisted} \n\n\n",
    "supporting": [
      "07_prob_distributions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}