{
  "hash": "5afa1bf6dc5c3cfab8a867664f2c56eb",
  "result": {
    "engine": "knitr",
    "markdown": "# Distribuzioni di v.c. continue {#sec-prob-cont-prob-distr}\n\n\n::: callout-important\n## In questo capitolo imparerai a:\n\n- comprendere le principali distribuzioni di densit√† di probabilit√†;\n- utilizzare R per manipolare e analizzare queste distribuzioni.\n::: \n\n::: callout-tip\n## Prerequisiti\n\n- Leggere il capitolo *Continuous random variables* [@blitzstein2019introduction].\n- Leggere il capitolo *Special Distributions* [@schervish2014probability].\n- Leggere il capitolo *Random Variables and Probability Distributions* [@schervish2014probability].\n:::\n\n::: callout-caution\n## Preparazione del Notebook\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n```\n:::\n\n:::\n\n\n## Introduzione\n\nProprio come per le variabili casuali discrete, anche per le variabili casuali continue √® possibile rappresentare la variabilit√† di una popolazione attraverso un modello statistico. Tuttavia, mentre le distribuzioni discrete si applicano a fenomeni con un numero finito o numerabile di esiti, le variabili casuali continue richiedono l'uso di **funzioni di densit√† di probabilit√†** (pdf), che descrivono fenomeni in cui i valori possono assumere un continuum di possibilit√†. Queste funzioni ci permettono di modellare e analizzare situazioni in cui i risultati non sono discreti, ma possono variare in modo continuo.\n\nLa funzione di densit√† di probabilit√† $f(x)$ associata a una variabile casuale continua $X$ rappresenta la distribuzione della probabilit√† all'interno della popolazione. A differenza delle distribuzioni discrete, dove la probabilit√† √® assegnata direttamente a singoli valori, la pdf non fornisce la probabilit√† di un singolo punto, ma descrive la probabilit√† che $X$ assuma valori all'interno di un intervallo specifico. Questo approccio consente di costruire un modello matematico della popolazione, utile per fare previsioni e comprendere meglio i fenomeni aleatori continui.\n\n## La Distribuzione Uniforme Continua\n\nLa **distribuzione uniforme continua** rappresenta un pilastro della teoria delle probabilit√†, caratterizzata da una densit√† di probabilit√† costante su un intervallo definito. Questo modello √® particolarmente utile per descrivere fenomeni casuali dove ogni esito possibile ha identica probabilit√† di verificarsi, come nel caso di uno spinner perfettamente bilanciato o di un generatore di numeri casuali ideale.\n\n### Un esempio intuitivo: lo spinner  \nConsideriamo uno spinner circolare con valori angolari compresi tra 0¬∞ e 360¬∞. Se il dispositivo √® **perfettamente equilibrato**, ogni angolo ha la stessa probabilit√† di essere selezionato dopo una rotazione. Questo esperimento costituisce un‚Äôimplementazione concreta della distribuzione uniforme sull‚Äôintervallo $[0, 360)$.\n\n### Simulazione della distribuzione: dal campione piccolo alla convergenza teorica  \n\nPer illustrare il comportamento della distribuzione, analizziamo due scenari distinti attraverso simulazioni numeriche.\n\n**Caso 1: Campione piccolo (n = 20)**  \n\nGeneriamo 20 valori casuali e visualizziamoli con un istogramma:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)  # Riproducibilit√† dei risultati\nspinner_results <- runif(20, min = 0, max = 360)\nprint(spinner_results)  # Output dei dati\n#>  [1] 103.5 283.8 147.2 317.9 338.6  16.4 190.1 321.3 198.5 164.4 344.5 163.2\n#> [13] 243.9 206.1  37.1 323.9  88.6  15.1 118.1 343.6\n\n# Creazione dell'istogramma\nggplot(data.frame(Valori = spinner_results), aes(x = Valori)) +\n  geom_histogram(\n    binwidth = 10, \n    fill = \"skyblue\", \n    color = \"black\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"Angolo (gradi)\", \n    y = \"Frequenza relativa\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nL‚Äôistogramma mostra un andamento **irregolare**, riflettendo la variabilit√† intrinseca dei piccoli campioni. Questa disomogeneit√† √® attesa e diminuisce all‚Äôaumentare della dimensione campionaria.\n\n**Caso 2: Campione grande (n = 100,000)**  \n\nRipetiamo la simulazione con un campione esteso:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspinner_results_large <- runif(100000, min = 0, max = 360)\n\nggplot(data.frame(Valori = spinner_results_large), aes(x = Valori)) +\n  geom_histogram(\n    binwidth = 10, \n    fill = \"skyblue\", \n    color = \"black\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"Angolo (gradi)\", \n    y = \"Frequenza relativa\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nL‚Äôistogramma ora rivela un profilo **piatto e regolare**, in accordo con la forma teorica della distribuzione. Questo risultato dimostra empiricamente la Legge dei Grandi Numeri, dove all‚Äôaumentare delle osservazioni la distribuzione empirica converge a quella teorica.\n\n### La Funzione di Densit√† di Probabilit√† (PDF) \n\nPer una variabile casuale $X \\sim \\mathcal{U}(a, b)$, la PDF √® definita come:\n\n$$\nf(x) = \n\\begin{cases} \n  \\displaystyle \\frac{1}{b - a} & \\text{se } x \\in [a, b], \\\\\n  0 & \\text{altrimenti}.\n\\end{cases}\n$$\n\n**Propriet√† chiave:**  \n\n- l‚Äôarea totale sotto la curva √® unitaria: $\\int_{a}^{b} \\frac{1}{b - a} \\, dx = 1$;  \n- la densit√† √® nulla al di fuori dell‚Äôintervallo $[a, b]$.  \n\n**Applicazione allo spinner:**  \n\n$$\nf(x) = \\frac{1}{360} \\quad \\text{per } x \\in [0, 360].\n$$\n\n**Visualizzazione grafica in R:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- seq(-50, 410, length.out = 500)  \ndensity_uniform <- dunif(x, min = 0, max = 360)\n\nggplot(data.frame(x = x, y = density_uniform), aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"blue\") +\n  geom_vline(xintercept = c(0, 360), linetype = \"dashed\", color = \"red\") +\n  labs(\n    x = \"x (gradi)\", \n    y = \"Densit√† f(x)\"\n  ) +\n  xlim(-50, 410) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIl grafico evidenzia la densit√† costante nell‚Äôintervallo $[0, 360]$ e l‚Äôassenza di probabilit√† al di fuori di esso.\n\n### Calcolo delle Probabilit√†: Metodo Geometrico e Funzionale  \n\nLa probabilit√† che $X$ assuma valori in un sottointervallo $[c, d] \\subseteq [a, b]$ √® data da:  \n\n$$\nP(c \\leq X \\leq d) = \\frac{d - c}{b - a}.\n$$\n\n**Esempio applicativo:**  \n\nCalcoliamo la probabilit√† che lo spinner si fermi tra 150¬∞ e 250¬∞:  \n\n$$\nP(150 \\leq X \\leq 250) = \\frac{250 - 150}{360} = \\frac{100}{360} = \\frac{5}{18} \\approx 0.2778.\n$$\n\n**Conferma numerica in R:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Approccio manuale\nprob_manuale <- (250 - 150) / 360  \n\n# Utilizzo della funzione cumulativa (CDF)\nprob_cdf <- punif(250, min = 0, max = 360) - punif(150, min = 0, max = 360)  \n```\n:::\n\n\n**Rappresentazione grafica dell‚Äôarea di probabilit√†:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data.frame(x = x, fx = density_uniform), aes(x = x, y = fx)) +\n  geom_line(linewidth = 1.2, color = \"blue\") +\n  geom_area(\n    data = subset(data.frame(x, fx = density_uniform), x >= 150 & x <= 250),\n    aes(x = x, y = fx), \n    fill = \"gray\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"x (gradi)\", \n    y = \"Densit√† f(x)\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nL‚Äôarea grigia corrisponde esattamente al valore di probabilit√† calcolato, illustrando visivamente il concetto di integrazione della PDF.\n\n### Propriet√† Fondamentali: Media e Varianza  \n\nPer $X \\sim \\mathcal{U}(a, b)$ valgono le seguenti relazioni:  \n\n1. **Valore atteso (centro della distribuzione):**  \n\n   $$\n   E(X) = \\frac{a + b}{2}.\n   $$  \n   \n   *Esempio:* Per lo spinner, $E(X) = (0 + 360)/2 = 180$ gradi.\n\n2. **Varianza (misura di dispersione):**  \n\n   $$\n   \\text{Var}(X) = \\frac{(b - a)^2}{12}.\n   $$  \n   \n   *Esempio:* Per lo spinner, $\\text{Var}(X) = (360 - 0)^2 / 12 = 10,\\!800$ gradi¬≤.\n\n### Implementazione in R: Funzioni Principali  \n\nR fornisce quattro funzioni per lavorare con la distribuzione uniforme:  \n\n| Funzione  | Descrizione                           | Esempio d‚Äôuso                   |\n|-----------|---------------------------------------|----------------------------------|\n| `runif()` | Genera valori casuali                 | `runif(5, min=0, max=1)`        |\n| `dunif()` | Calcola la densit√† $f(x)$             | `dunif(180, min=0, max=360)`    |\n| `punif()` | Calcola la CDF $P(X \\leq x)$          | `punif(250, min=0, max=360)`    |\n| `qunif()` | Determina il quantile per una probabilit√† | `qunif(0.9, min=0, max=360)`    |\n\n**Esempi operativi:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 1. Generazione di 5 numeri casuali in [0, 1]\nrunif(5, min = 0, max = 1)  \n#> [1] 0.0372 0.9278 0.4480 0.9159 0.3558\n\n# 2. Valore della densit√† in x = 0.5 per U(0,1)\ndunif(0.5, min = 0, max = 1)  \n#> [1] 1\n\n# 3. Probabilit√† cumulativa fino a x = 0.8 per U(0,1)\npunif(0.8, min = 0, max = 1)  \n#> [1] 0.8\n\n# 4. Calcolo del quantile corrispondente al 50¬∞ percentile (mediana)\nqunif(0.5, min = 0, max = 360)  \n#> [1] 180\n```\n:::\n\n\n## La Distribuzione Esponenziale  \n\nLa **distribuzione esponenziale** √® una distribuzione continua fondamentale per modellare il **tempo di attesa** fino al verificarsi di un evento casuale. La sua caratteristica distintiva √® la **propriet√† di assenza di memoria**, che la rende unica nel panorama delle distribuzioni probabilistiche.  \n\n### La propriet√† di assenza di memoria: un concetto chiave  \n\nL‚Äô**assenza di memoria** implica che la probabilit√† che un evento si verifichi in un intervallo futuro √® indipendente dal tempo gi√† trascorso.  \n\n**Esempio intuitivo:**  \nimmaginiamo una persona che sperimenta **attacchi di ansia improvvisi**, il cui tempo tra un episodio e il successivo segue una distribuzione esponenziale. Se l‚Äôindividuo non ha avuto un attacco nelle ultime 2 settimane, la probabilit√† che ne sperimenti uno nei prossimi 3 giorni √® **identica** a quella di una persona appena uscita da un episodio, nel medesimo intervallo di 3 giorni.  \n\nQuesta analogia illustra la **propriet√† di assenza di memoria**: il \"tempo trascorso dall‚Äôultimo evento\" (in questo caso, un attacco di ansia) non influenza la probabilit√† futura. Il sistema non \"accumula stress\" n√© riduce il rischio col passare del tempo senza episodi, riflettendo dinamiche tipiche di processi psicologici **non legati a meccanismi di apprendimento o adattamento**.  \n\n**Parametri chiave nell‚Äôesempio:** \n\n- **$\\lambda$ (tasso):** Frequenza media degli attacchi (es. 0.1 episodi/giorno). \n- **$\\mu = 1/\\lambda$ (media):** Tempo medio tra due episodi (es. 10 giorni).  \n\nLa distribuzione esponenziale modellizza cos√¨ situazioni in cui il comportamento √® **puramente stocastico** e non influenzato dalla storia precedente, come certi pattern di ansia, impulsivit√† o reazioni fisiologiche a stimoli neutri.\n\n### Struttura matematica: PDF e parametri  \n\nLa **funzione di densit√† di probabilit√† (PDF)** di una variabile $X \\sim \\text{Exp}(\\lambda)$ √®:  \n\n$$\nf(x) = \n\\begin{cases} \n\\lambda e^{-\\lambda x} & \\text{se } x \\geq 0, \\\\\n0 & \\text{altrimenti},\n\\end{cases}\n$$  \n\ndove:  \n\n- **$\\lambda$ (tasso):** numero medio di eventi per unit√† di tempo (es. 0.25 episodi/ora).  \n- **$\\mu = 1/\\lambda$ (media):** tempo medio di attesa per l‚Äôevento (es. 4 ore/episodio).  \n\n**Forma alternativa con $\\mu$:**  \n\n$$\nf(x) = \\frac{1}{\\mu} e^{-x/\\mu} \\quad \\text{per } x \\geq 0.\n$$  \n\n### Propriet√† fondamentali  \n\nPer $X \\sim \\text{Exp}(\\lambda)$:  \n\n| Propriet√†               | Formula                  | Interpretazione                          |  \n|-------------------------|--------------------------|------------------------------------------|  \n| **Valore atteso (Œº)**   | $E(X) = \\frac{1}{\\lambda}$ | Tempo medio di attesa per l‚Äôevento.      |  \n| **Varianza**            | $\\text{Var}(X) = \\frac{1}{\\lambda^2}$ | Dispersione cresce col quadrato di 1/Œª. |  \n| **Deviazione standard** | $\\sigma_X = \\frac{1}{\\lambda}$       | Spread lineare attorno alla media.       |  \n\n**Esempio applicato.**  \nSe il tempo medio di pubblicazione dei voti di un esame universitario √® $\\mu = 4$ giorni ($\\lambda = 0.25$), la PDF √®:  \n\n$$\nf(x) = \\frac{1}{4} e^{-x/4} \\quad (x \\geq 0).\n$$  \n\n### Visualizzazione della densit√† in R  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Definizione dei parametri\nmu <- 4\nlambda <- 1 / mu  # 0.25\n\n# Generazione dei punti per il grafico\nx <- seq(0, 20, by = 0.1)\npdf <- dexp(x, rate = lambda)\n\n# Creazione del grafico\nggplot(data.frame(x = x, y = pdf), aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  labs(\n    x = \"Tempo di attesa (giorni)\", \n    y = \"Densit√† f(x)\",\n    title = paste(\"PDF esponenziale (Œº =\", mu, \"giorni)\")\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIl grafico mostra un **decadimento esponenziale**: la probabilit√† decresce rapidamente all‚Äôaumentare del tempo.  \n\n### Calcolo delle Probabilit√†: Tre Scenari   \n\n**1. Probabilit√† cumulativa: $P(X \\leq 1.5)$** -- qual √® la probabilit√† che il voto venga pubblicato entro un giorno e mezzo?\n\nUtilizziamo la **funzione di ripartizione (CDF)**:  \n\n$$\nP(X \\leq 1.5) = 1 - e^{-\\lambda \\cdot 1.5} = 1 - e^{-0.25 \\cdot 1.5} \\approx 0.312.\n$$  \n\n**Codice R:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npexp(1.5, rate = lambda)  # Restituisce 0.312\n#> [1] 0.313\n```\n:::\n\n\n**Visualizzazione:** \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Area sotto la curva per X <= 1.5\nggplot(data.frame(x = x, y = pdf), aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_area(\n    data = subset(data.frame(x, y = pdf), x <= 1.5),\n    aes(x = x, y = y), \n    fill = \"gray\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"Tempo (giorni)\", \n    y = \"Densit√†\",\n    title = \"Probabilit√† P(X ‚â§ 1.5)\"\n  )\n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**2. Probabilit√† intervallo: $P(1 \\leq X \\leq 6)$** -- qual √® la probabilit√† che il voto venga pubblicato in un intervallo compreso tra 1 e 6 giorni dopo l'esame?\n\nCalcoliamo la differenza tra due CDF:  \n\n$$\nP(1 \\leq X \\leq 6) = F(6) - F(1) = e^{-0.25 \\cdot 1} - e^{-0.25 \\cdot 6} \\approx 0.491.\n$$  \n\n**Codice R:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npexp(6, rate = lambda) - pexp(1, rate = lambda)  # 0.491\n#> [1] 0.556\n```\n:::\n\n\n**Visualizzazione:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Area per 1 <= X <= 6\nggplot(data.frame(x = x, y = pdf), aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_area(\n    data = subset(data.frame(x, y = pdf), x >= 1 & x <= 6),\n    aes(x = x, y = y), \n    fill = \"gray\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"Tempo (giorni)\", \n    y = \"Densit√†\",\n    title = \"Probabilit√† P(1 ‚â§ X ‚â§ 6)\"\n  )\n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**3. Probabilit√† della coda: $P(X \\geq 5.5)$** -- qual √® la probabilit√† di un ritardo nella pubblicazione del voto superiore a 5.5 giorni dall'esame?\n\nUsiamo il complemento della CDF:  \n\n$$\nP(X \\geq 5.5) = 1 - P(X \\leq 5.5) = e^{-0.25 \\cdot 5.5} \\approx 0.252.\n$$  \n\n**Codice R:** \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n1 - pexp(5.5, rate = lambda)  # 0.252\n#> [1] 0.253\n# Alternativa equivalente:\npexp(5.5, rate = lambda, lower.tail = FALSE)\n#> [1] 0.253\n```\n:::\n\n\n**Visualizzazione:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Area per X >= 5.5\nggplot(data.frame(x = x, y = pdf), aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_area(\n    data = subset(data.frame(x, y = pdf), x >= 5.5),\n    aes(x = x, y = y), \n    fill = \"gray\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"Tempo (giorni)\", \n    y = \"Densit√†\",\n    title = \"Probabilit√† P(X ‚â• 5.5)\"\n  )\n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Simulazione e convergenza alla teoria  \n\nGeneriamo 1,000,000 di osservazioni da $\\text{Exp}(\\lambda = 0.25)$ e confrontiamo l‚Äôistogramma con la PDF teorica:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\nsimulated_data <- rexp(1e6, rate = lambda)\n\nggplot(data.frame(x = simulated_data), aes(x = x)) +\n  geom_histogram(\n    aes(y = after_stat(density)), \n    bins = 100, \n    fill = \"skyblue\", \n    color = \"black\",\n    alpha = 0.6\n  ) +\n  geom_line(\n    data = data.frame(x = x, y = pdf),\n    aes(x = x, y = y), \n    color = \"red\", \n    linewidth = 1.2\n  ) +\n  coord_cartesian(xlim = c(0, 20)) +  # Escludiamo code estreme\n  labs(\n    x = \"Tempo di attesa (giorni)\", \n    y = \"Densit√†\",\n    title = \"Dati simulati e PDF teorica\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nL‚Äôistogramma si allinea perfettamente alla curva rossa, dimostrando la **Legge dei Grandi Numeri**.  \n\n### Funzioni R per la distribuzione esponenziale  \n\nR offre quattro funzioni essenziali:  \n\n| Funzione  | Descrizione                           | Esempio d‚Äôuso                    | Output Esempio      |  \n|-----------|---------------------------------------|-----------------------------------|---------------------|  \n| `dexp()`  | Calcola la densit√† $f(x)$             | `dexp(2, rate = 0.25)`           | 0.1516              |  \n| `pexp()`  | Calcola la CDF $P(X \\leq x)$          | `pexp(4, rate = 0.25)`           | 0.632 (‚âà1 - e‚Åª¬π)    |  \n| `qexp()`  | Trova il quantile $x$ per una probabilit√† | `qexp(0.5, rate = 0.25)`       | ~2.773 (mediana)    |  \n| `rexp()`  | Genera valori casuali                 | `rexp(5, rate = 0.25)`           | [3.1, 0.8, 5.2, ...] |  \n\n\n## Distribuzione Normale  \n\nLa **distribuzione normale** (o gaussiana) √® fondamentale in statistica per modellare fenomeni naturali, sociali e psicologici. La sua importanza deriva dal **Teorema del Limite Centrale**, che garantisce la convergenza alla normalit√† per somme di variabili casuali indipendenti.  \n\n### La Famiglia delle Distribuzioni Normali  \n\nOgni distribuzione normale √® definita da due parametri:  \n\n- **$\\mu$ (media):** centro della distribuzione;  \n- **$\\sigma$ (deviazione standard):** dispersione dei dati attorno alla media.  \n\nLa funzione di densit√† √®:  \n\n$$\nf(y; \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(y - \\mu)^2}{2\\sigma^2}}.\n$$ {#eq-gaussian}\n\n### Distribuzione Normale Standardizzata  \n\nLa **normale standardizzata** √® un caso speciale con $\\mu = 0$ e $\\sigma = 1$. Qualsiasi variabile $Y \\sim \\mathcal{N}(\\mu, \\sigma)$ pu√≤ essere standardizzata tramite:  \n\n$$\nZ = \\frac{Y - \\mu}{\\sigma}.\n$$ {#eq-z-score}\n\nQuesta trasformazione preserva la forma della distribuzione ma riporta i valori in unit√† di deviazione standard (**Z-score**), permettendo confronti universali.  \n\n#### Relazione tra Deviazione Standard e Distribuzione  \n\n**La regola empirica 68-95-99.7 vale per tutte le distribuzioni normali**, indipendentemente da $\\mu$ e $\\sigma$:  \n\n- **68.3%** dei dati cade entro $\\pm 1\\sigma$ dalla media;  \n- **95.4%** entro $\\pm 2\\sigma$;  \n- **99.7%** entro $\\pm 3\\sigma$.  \n\nPer intervalli specifici legati a test statistici:  \n\n- **$\\pm 1.96\\sigma$** copre il **95%** dei dati (intervallo di confidenza al 95%);\n- **$\\pm 2.576\\sigma$** copre il **99%** (intervallo al 99%).  \n\n\n### Origini storiche e connessione alla binomiale  \n\nAbraham de Moivre osserv√≤ che distribuzioni binomiali con $n$ elevato approssimano una normale. Ad esempio:  \n\n- con $n=10$ e $p=0.9$, la distribuzione √® asimmetrica; \n- con $n=1000$, la forma diventa simmetrica e campanulare.  \n\n### Simulazione di Passeggiate Casuali  \n\nLa distribuzione normale emerge naturalmente come risultato della **somma di un gran numero di effetti casuali indipendenti**, un principio formalizzato dal **Teorema del Limite Centrale**. Questo la rende ideale per modellare:  \n\n- **errori di misurazione**, dove piccole fluttuazioni casuali (strumentali, ambientali, umane) si combinano;  \n- **fenomeni biologici multifattoriali** come altezza, peso o QI, influenzati da decine di fattori genetici, ambientali e nutrizionali che interagiscono in modo additivo;  \n- **processi sociali** come i punteggi dei test, dove il risultato finale √® il prodotto cumulativo di abilit√† innate, studio, stato emotivo e altro.  \n\n**Simulazione con passeggiate casuali**\n\nPer visualizzare concretamente questo fenomeno, consideriamo una **passeggiata casuale unidimensionale** semplificata:  \n\n1. **Impostazione:**  \n   - 1,000 partecipanti partono dalla posizione 0;  \n   - ogni partecipante compie **16 passi consecutivi**;  \n   - ogni passo √® determinato da un generatore casuale che assegna uno spostamento compreso tra -1 e +1 unit√† (*simulando l'effetto di piccole perturbazioni indipendenti*).  \n\n2. **Dinamica:**  \n   la posizione finale di ciascun partecipante √® la **somma algebrica** degli spostamenti casuali. Nonostante ogni passo individuale segua una distribuzione uniforme, la posizione finale aggregata di tutti i partecipanti mostrer√† una distribuzione a campana tipica della normale.  \n   \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri\nnumero_passi <- 16\nripetizioni <- 1000\n\n# Generazione di passeggiate casuali\nset.seed(123)\nx <- matrix(0, nrow = numero_passi + 1, ncol = ripetizioni)\n\nfor (i in 1:ripetizioni) {\n  passi <- runif(numero_passi, min = -1, max = 1)\n  x[-1, i] <- cumsum(passi)\n}\n\n# Grafico delle passeggiate casuali\ndf <- data.frame(\n  Passo = rep(0:numero_passi, times = ripetizioni), \n  Distanza = as.vector(x)\n)\n\nggplot(\n  df, \n  aes(\n    x = Passo, \n    y = Distanza, \n    group = rep(1:ripetizioni, each = numero_passi + 1))\n  ) +\n  geom_line(color = \"blue\", alpha = 0.05) +\n  labs(\n    title = \"Passeggiate Casuali\", \n    x = \"Numero di Passi\", y = \"Distanza dall'Origine\"\n  )\n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Codice di simulazione (esempio concettuale)  \nset.seed(123)  \nn_partecipanti <- 1000  \nn_passi <- 16  \n\n# Genera spostamenti casuali (-1 a +1)  \nspostamenti <- matrix(runif(n_partecipanti * n_passi, min = -1, max = 1), ncol = n_passi)  \n\n# Calcola le posizioni finali  \nposizioni_finali <- rowSums(spostamenti)  \n\n# Visualizzazione  \nggplot(data.frame(Posizione = posizioni_finali), aes(x = Posizione)) +  \n  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = \"lightblue\", alpha = 0.7) +  \n  stat_function(fun = dnorm, args = list(mean = mean(posizioni_finali), sd = sd(posizioni_finali)), color = \"red\", linewidth = 1) +  \n  labs(title = \"Distribuzione delle posizioni finali\", x = \"Posizione\", y = \"Densit√†\")  \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**Risultato atteso:**  \nl'istogramma delle posizioni finali aderir√† alla curva rossa (normale teorica), dimostrando come la **combinazione di piccole variazioni casuali** produca una distribuzione gaussiana, anche partendo da passi non-normali. Questo esperimento illustra l‚Äôonnipresenza della normale in contesti reali governati da molteplici fattori indipendenti.\n\n**Perch√© 16 passi?**  \nLa scelta di 16 passi non √® arbitraria:  \n\n- un numero ridotto di passi (es. 3-5) produrrebbe una distribuzione ancora vicina all‚Äôuniforme; \n- con 16 passi, la **simmetria** e la **curvatura tipica della gaussiana** diventano chiaramente riconoscibili senza richiedere simulazioni massicce.\n\n### Propriet√† fondamentali  \n\n- **Media**: $\\mathbb{E}(Y) = \\mu$;  \n- **Varianza**: $\\mathbb{V}(Y) = \\sigma^2$.  \n\n### Funzioni R per la Normale  \n\n| Funzione  | Descrizione                          | Esempio                       |  \n|-----------|--------------------------------------|-------------------------------|  \n| `dnorm()` | Densit√† a un punto $y$               | `dnorm(115, mean=100, sd=15)` |  \n| `pnorm()` | Probabilit√† cumulativa $P(Y \\leq y)$ | `pnorm(115, mean=100, sd=15)` |  \n| `qnorm()` | Quantile per una probabilit√† $p$     | `qnorm(0.975, mean=100, sd=15)` |  \n| `rnorm()` | Genera valori casuali                | `rnorm(10, mean=100, sd=15)`  |  \n\n\n### Visualizzazione delle aree critiche  \n\nLe aree sotto la curva corrispondenti a $\\pm 1\\sigma$, $\\pm 1.96\\sigma$, e $\\pm 3\\sigma$ possono essere visualizzate in R:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Esempio per ¬±1.96œÉ (95% di confidenza)  \nmu <- 100  \nsigma <- 15  \nx <- seq(mu - 4*sigma, mu + 4*sigma, length.out=1000)  \ndf <- data.frame(x=x, pdf=dnorm(x, mu, sigma))  \n\nggplot(df, aes(x=x, y=pdf)) +  \n  geom_line(color=\"blue\") +  \n  geom_area(data=subset(df, x >= mu - 1.96*sigma & x <= mu + 1.96*sigma),  \n            fill=\"gray\", alpha=0.5) +  \n  labs(title=\"95% dei dati entro ¬±1.96œÉ\", x=\"Valori\", y=\"Densit√†\")  \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**In sintesi**, la distribuzione normale standardizzata permette di standardizzare qualsiasi fenomeno Gaussiano, rendendo confrontabili dati eterogenei. La relazione tra deviazioni standard e aree sottese √® universale: **indipendentemente dalla media e varianza originale, il 68-95-99.7% dei dati cadr√† sempre entro 1-2-3œÉ**. \n\n\n\n::: {.callout-important title=\"Esercizio\" collapse=\"true\"}\nUna psicologa vuole studiare i **livelli di ansia** tra gli studenti universitari durante la settimana degli esami. Dalle ricerche precedenti si sa che nella **popolazione universitaria**:\n\n- il **punteggio medio di ansia** √® di **50 punti** su una scala da 0 a 100;\n- la **deviazione standard** dei punteggi di ansia √® **10 punti**.\n\nLa psicologa decide di **estrarre un campione casuale di 25 studenti**.\n\nVogliamo usare la **distribuzione campionaria della media** per rispondere a due domande:\n\n1. Qual √® la probabilit√† di ottenere una media campionaria maggiore di 54 punti?\n2. Quale media campionaria rappresenta il **95¬∞ percentile** della distribuzione campionaria?\n\nüìò **Concetti chiave.**\n\nLa **distribuzione campionaria della media** ha:\n\n- la **stessa media** della popolazione ($\\mu$),\n- una **deviazione standard pi√π piccola**, detta *errore standard della media* (SE):\n\n$$\nSE = \\frac{\\sigma}{\\sqrt{n}} = \\frac{10}{\\sqrt{25}} = 2 .\n$$\n\nUseremo due funzioni importanti in R:\n\n- `dnorm(x, mean, sd)`: calcola la **densit√†** della normale in un punto $x$.\n- `qnorm(p, mean, sd)`: calcola il valore di $x$ corrispondente a una certa **probabilit√† cumulativa** $p$.\n\n\n‚úÖ **Codice base.**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri della popolazione e del campione\nmu <- 50       # media della popolazione\nsigma <- 10    # deviazione standard\nn <- 25        # dimensione campione\n\n# Errore standard della media\nSE <- sigma / sqrt(n)\nSE\n#> [1] 2\n```\n:::\n\n\nüîç **Domanda 1: Probabilit√† di ottenere una media > 54.**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Probabilit√† che la media campionaria sia maggiore di 54\np_oltre_54 <- pnorm(54, mean = mu, sd = SE, lower.tail = FALSE)\np_oltre_54\n#> [1] 0.0228\n```\n:::\n\n\nLa probabilit√† √® molto bassa. Questo vuol dire che, se la vera media della popolazione fosse 50, ottenere una media campionaria superiore a 54 sarebbe raro.\n\nüîç **Domanda 2: Media al 95¬∞ percentile.**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo del valore soglia al 95¬∞ percentile\nq_95 <- qnorm(0.95, mean = mu, sd = SE)\nq_95\n#> [1] 53.3\n```\n:::\n\n\nAll'interno della distribuzione campionaria, solo il 5% dei campioni ha una media superiore a questo valore.\n\nüìä Grafico 1: Probabilit√† di media > 54\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Dati per la distribuzione normale\nx_vals <- seq(44, 56, length.out = 300)\ndens_vals <- dnorm(x_vals, mean = mu, sd = SE)\ndf <- data.frame(x = x_vals, y = dens_vals)\n\n# Grafico\nggplot(df, aes(x, y)) +\n  geom_line(color = \"black\") +\n  geom_area(data = subset(df, x >= 54), aes(x, y), fill = \"red\", alpha = 0.4) +\n  geom_vline(xintercept = 54, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Distribuzione campionaria della media (n = 25)\",\n    subtitle = \"Area rossa = P(media > 54)\",\n    x = \"Media campionaria\",\n    y = \"Densit√†\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\nüìä **Grafico 2: Valore al 95¬∞ percentile**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Grafico con il 95¬∞ percentile evidenziato\nggplot(df, aes(x, y)) +\n  geom_line(color = \"black\") +\n  geom_area(data = subset(df, x <= q_95), aes(x, y), fill = \"blue\", alpha = 0.4) +\n  geom_vline(xintercept = q_95, color = \"blue\", linetype = \"dashed\") +\n  labs(\n    title = \"Distribuzione campionaria della media (n = 25)\",\n    subtitle = \"Area blu = 95% dei campioni (valore critico ‚âà 53.29)\",\n    x = \"Media campionaria\",\n    y = \"Densit√†\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-23-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**Domande di approfondimento.**\n\n1. Perch√© l'errore standard della media √® pi√π piccolo della deviazione standard della popolazione?\n2. Se la dimensione del campione aumentasse a 100, come cambierebbe l'errore standard?\n3. Che cosa rappresenta `pnorm(54, ...)` nel nostro contesto?\n4. In quali casi, in psicologia, potresti voler calcolare il 95¬∞ percentile di una distribuzione campionaria?\n\n**Simulazione Monte Carlo.**\n\nSimuliamo 10.000 campioni casuali, ciascuno di **25 studenti**, estratti da una popolazione normale con media = 50 e deviazione standard = 10. Per ogni campione calcoliamo la media. Alla fine, visualizziamo la distribuzione di queste medie.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)  # per rendere la simulazione replicabile\n\n# Parametri\nmu <- 50\nsigma <- 10\nn <- 25\nn_sim <- 10000  # numero di campioni\n\n# Simulazione: 10.000 medie campionarie\ncampioni <- replicate(n_sim, mean(rnorm(n, mean = mu, sd = sigma)))\n\n# Visualizza le prime 5 medie\nhead(campioni)\n#> [1] 49.7 51.0 50.1 52.8 47.2 47.7\n```\n:::\n\n\nüìä **Istogramma delle medie campionarie.**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n\ndf_sim <- data.frame(media_campionaria = campioni)\n\nggplot(df_sim, aes(x = media_campionaria)) +\n  geom_histogram(aes(y = ..density..), bins = 40, fill = \"lightblue\", color = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma / sqrt(n)),\n                color = \"red\", size = 1) +\n  labs(\n    title = \"Distribuzione delle medie campionarie\",\n    subtitle = \"Istogramma di 10.000 medie di campioni di 25 studenti\",\n    x = \"Media campionaria\",\n    y = \"Densit√†\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nCosa si osserva?\n\n- Le medie **non sono tutte uguali**, ma si distribuiscono **intorno alla media vera** (50).\n- La forma della distribuzione delle medie √® **normale**, anche se i dati originali **non devono necessariamente esserlo** (grazie al Teorema del Limite Centrale).\n- La **deviazione standard** della distribuzione simulata √® vicina all'**errore standard teorico**:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Confronto tra errore standard teorico e osservato\nSE_teorico <- sigma / sqrt(n)\nSE_osservato <- sd(campioni)\n\nc(SE_teorico = SE_teorico, SE_osservato = SE_osservato)\n#>   SE_teorico SE_osservato \n#>         2.00         1.97\n```\n:::\n\n\n**Domande di approfondimento.**\n\n1. Perch√© la forma dell‚Äôistogramma √® simile a una curva normale?\n2. Cosa succederebbe alla **larghezza della distribuzione** se aumentassimo la dimensione del campione?\n3. Se la media osservata in un esperimento reale fosse **fuori dalla zona centrale**, come potremmo interpretarla?\n:::\n\n::: {.callout-important title=\"Esercizio\" collapse=\"true\"}\nConsideriamo un esercizio in cui si utilizza la **distribuzione normale** e si consultano le **tavole della normale standard** (z) per risolvere il problema dopo la **standardizzazione**.\n\nIn uno studio su un campione di 600 studenti universitari, i punteggi ottenuti a un test di ansia da esame seguono una distribuzione normale con **media** $\\mu = 50$ e **deviazione standard** $\\sigma = 10$.\n\n1. Qual √® la **probabilit√†** che uno studente scelto a caso ottenga un punteggio **inferiore a 65**?\n\n2. Qual √® la **percentuale di studenti** che ottengono un punteggio **compreso tra 45 e 60**?\n\n3. Qual √® il punteggio minimo che uno studente deve ottenere per rientrare nel **10% superiore** della distribuzione?\n\n**1. Probabilit√† che $X < 65$.**\n\nStandardizziamo:\n\n$$\nZ = \\frac{X - \\mu}{\\sigma} = \\frac{65 - 50}{10} = \\frac{15}{10} = 1.5 .\n$$\n\nCerchiamo $P(Z < 1.5)$ nella **tavola della normale standard**:\n\n$$\nP(Z < 1.5) \\approx 0.9332 .\n$$\n\n**Risposta**: La probabilit√† che uno studente ottenga meno di 65 √® circa **93.32%**.\n\n**2. Probabilit√† che $45 < X < 60$.**\n\nCalcoliamo gli z-score:\n\n$$\nZ_1 = \\frac{45 - 50}{10} = -0.5 \\quad ; \\quad Z_2 = \\frac{60 - 50}{10} = 1.0 .\n$$\n\nCerchiamo nelle tavole:\n\n- $P(Z < 1.0) \\approx 0.8413$\n- $P(Z < -0.5) \\approx 0.3085$\n\nQuindi:\n\n$$\nP(45 < X < 60) = P(Z < 1.0) - P(Z < -0.5) = 0.8413 - 0.3085 = 0.5328\n$$\n\n**Risposta**: Circa il **53.28%** degli studenti ha un punteggio tra 45 e 60.\n\n**3. Punteggio minimo per rientrare nel 10% superiore.**\n\nIl 10% superiore corrisponde a:\n\n$$\nP(Z > z) = 0.10 \\Rightarrow P(Z < z) = 0.90 .\n$$\n\nDalla tavola:  \n$P(Z < 1.28) \\approx 0.8997$,  \n$P(Z < 1.29) \\approx 0.9015$\n\nPrendiamo $z = 1.28$\n\nOra risolviamo per $X$:\n\n$$\nX = z \\cdot \\sigma + \\mu = 1.28 \\cdot 10 + 50 = 62.8\n$$\n\n**Risposta**: Il punteggio minimo per rientrare nel 10% superiore √® circa **62.8**.\n:::\n\n## Distribuzione Chi-Quadrato\n\nLa distribuzione **$\\chi^2$** deriva dalla distribuzione normale e descrive la somma dei quadrati di $k$ variabili casuali indipendenti e identicamente distribuite (i.i.d.) che seguono la distribuzione normale standard $\\mathcal{N}(0, 1)$. Una variabile casuale $\\chi^2_{~k}$ con $k$ gradi di libert√† √® definita come:\n\n$$\nZ_1^2 + Z_2^2 + \\dots + Z_k^2,\n$$ {#eq-chisq-def}\n\ndove $Z_1, Z_2, \\dots, Z_k \\sim \\mathcal{N}(0, 1)$. Il parametro $k$, detto **gradi di libert√†** ($\\nu$), determina la forma della distribuzione.\n\n### Funzione di densit√†\n\nLa densit√† di probabilit√† della distribuzione $\\chi^2_{~\\nu}$ √® data da:\n\n$$\nf(x) = C_{\\nu} x^{\\nu/2 - 1} \\exp(-x/2), \\quad \\text{per } x > 0,\n$$ {#eq-chisq-def2}\n\ndove $C_{\\nu}$ √® una costante di normalizzazione. \n\n### Simulazione della Distribuzione Chi-Quadrato\n\nUtilizziamo la definizione per simulare la distribuzione $\\chi^2$ con 3 gradi di libert√†.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Impostare il seed per la riproducibilit√†\nset.seed(1234)\n\n# Generare 1000 valori casuali per 3 variabili gaussiane standard\nn <- 1000\nvar1 <- rnorm(n, mean = 0, sd = 1)\nvar2 <- rnorm(n, mean = 0, sd = 1)\nvar3 <- rnorm(n, mean = 0, sd = 1)\n\n# Calcolare la somma dei quadrati\nchi_sq_values <- var1^2 + var2^2 + var3^2\n\n# Creare un dataframe per il grafico\ndata <- data.frame(chi_sq_values = chi_sq_values)\n\n# Istogramma e densit√† teorica\nggplot(data, aes(x = chi_sq_values)) +\n  geom_histogram(\n    aes(y = after_stat(density)), \n    bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7\n    ) +\n  stat_function(fun = dchisq, args = list(df = 3), color = \"red\", size = 1) +\n  labs(\n    title = \"Distribuzione Chi-Quadrato (df = 3)\",\n    x = \"Valore\",\n    y = \"Densit√†\"\n  )\n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n- L'istogramma rappresenta i valori empirici simulati;\n- la curva rossa rappresenta la densit√† teorica della distribuzione $\\chi^2_{~3}$.\n\n### Media e Varianza Empiriche\n\nCalcoliamo la media e la varianza dei valori simulati:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Media empirica\nmean(chi_sq_values)\n#> [1] 2.98\n\n# Varianza empirica\nvar(chi_sq_values)\n#> [1] 5.97\n```\n:::\n\n\nQuesti valori possono essere confrontati con le propriet√† teoriche della distribuzione $\\chi^2$:\n\n- **media**: $\\nu = 3$;\n- **varianza**: $2\\nu = 6$.\n\n### Grafico per Diversi Gradi di Libert√†\n\nConfrontiamo le distribuzioni $\\chi^2$ per diversi valori di $\\nu$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Intervallo di x\nx <- seq(0, 40, by = 0.1)\n\n# Gradi di libert√†\nnus <- c(2, 4, 8, 16)\n\n# Creare un dataframe\ndata <- do.call(rbind, lapply(nus, function(nu) {\n  data.frame(x = x, f_x = dchisq(x, df = nu), nu = as.factor(nu))\n}))\n\n# Grafico\nggplot(data, aes(x = x, y = f_x, color = nu)) +\n  geom_line(size = 1) +\n  labs(\n    x = \"x\",\n    y = \"f(x)\",\n    color = expression(nu)\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Propriet√† della Distribuzione Chi-Quadrato\n\n1. **Asimmetria**: La distribuzione $\\chi^2_{\\nu}$ √® asimmetrica, ma diventa pi√π simmetrica al crescere di $\\nu$.\n2. **Media**: $\\mathbb{E}[\\chi^2_{\\nu}] = \\nu$.\n3. **Varianza**: $\\mathbb{V}[\\chi^2_{\\nu}] = 2\\nu$.\n4. **Convergenza**: Per $\\nu \\to \\infty$, $\\chi^2_{\\nu} \\to \\mathcal{N}(\\nu, 2\\nu)$.\n5. **Somma**: La somma di variabili $\\chi^2$ indipendenti con gradi di libert√† $\\nu_1, \\nu_2, \\dots, \\nu_k$ segue una distribuzione $\\chi^2$ con $\\nu = \\sum_{i=1}^k \\nu_i$.\n\n### Applicazioni\n\nLa distribuzione $\\chi^2$ √® utilizzata in molteplici ambiti statistici, tra cui:\n\n- **test di indipendenza**: per verificare se due variabili categoriche sono indipendenti;\n- **test di adattamento**: per confrontare una distribuzione empirica con una teorica.\n\n## Distribuzione $t$ di Student\n\nLa **distribuzione $t$ di Student** √® una delle distribuzioni fondamentali della statistica inferenziale. Deriva dalle distribuzioni Normale e Chi-quadrato ed √® particolarmente utile per analizzare campioni di piccole dimensioni o situazioni in cui la varianza della popolazione √® sconosciuta.\n\n### Definizione Formale\n\nSe:\n\n- $Z \\sim \\mathcal{N}(0, 1)$ (distribuzione Normale standard),\n- $W \\sim \\chi^2_{\\nu}$ (distribuzione Chi-quadrato con $\\nu$ gradi di libert√†),\n\ne $Z$ e $W$ sono indipendenti, allora la variabile casuale\n\n$$\nT = \\frac{Z}{\\sqrt{\\frac{W}{\\nu}}}\n$$ {#eq-t-def}\n\nsegue una **distribuzione $t$ di Student** con $\\nu$ gradi di libert√†. Si indica come $T \\sim t_{\\nu}$.\n\n### Propriet√† della Distribuzione $t$ di Student\n\n1. **Forma della distribuzione**:\n\n   - la distribuzione $t$ √® simmetrica rispetto a zero, come la Normale standard ($\\mathcal{N}(0, 1)$);\n   - presenta **code pi√π pesanti** rispetto alla Normale, riflettendo una maggiore probabilit√† di osservare valori estremi.\n\n2. **Code pesanti e gradi di libert√†**:\n\n   - la pesantezza delle code diminuisce con l'aumentare dei gradi di libert√† ($\\nu$);\n   - per $\\nu \\to \\infty$, la distribuzione $t$ converge alla distribuzione Normale standard.\n\n3. **Media e varianza**:\n\n   - la **media** √® $0$ per $\\nu > 1$;\n   - la **varianza** √®:\n   \n     $$\n     \\text{Var}(T) = \\frac{\\nu}{\\nu - 2}, \\quad \\text{per } \\nu > 2.\n     $$\n     \n     Per $\\nu \\leq 2$, la varianza non √® definita.\n\n4. **Applicazioni principali**:\n\n   - **test t di Student**: Confronto delle medie di due gruppi o test per una singola media;\n   - **intervalli di confidenza**: Stima dell'intervallo per la media quando la varianza √® sconosciuta.\n\n### Differenze tra la Distribuzione $t$ e la Normale\n\n| **Caratteristica**            | **Distribuzione Normale**     | **Distribuzione $t$ di Student**  |\n|-------------------------------|-------------------------------|--------------------------------------|\n| Forma                         | Simmetrica, a campana         | Simmetrica, a campana               |\n| Code                          | Sottili                       | Pesanti                             |\n| Dipendenza dai gradi di libert√† | No                            | S√¨                                  |\n| Convergenza                   | Non varia                     | Con $\\nu \\to \\infty$, converge alla Normale |\n\n### Visualizzazione della Distribuzione $t$\n\nConfrontiamo graficamente la distribuzione $t$ con diversi gradi di libert√† e la distribuzione Normale standard:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Creazione dei dati\nx <- seq(-4, 4, length.out = 1000)\ndf <- c(1, 2, 5, 10)  # Gradi di libert√†\n\n# Dataframe con curve di densit√†\ndata <- data.frame(\n  x = rep(x, length(df) + 1),\n  density = c(\n    dnorm(x),\n    dt(x, df[1]),\n    dt(x, df[2]),\n    dt(x, df[3]),\n    dt(x, df[4])\n  ),\n  distribution = rep(c(\"Normale\", paste(\"t (df =\", df, \")\")), each = length(x))\n)\n\n# Plot\nggplot(data, aes(x = x, y = density, color = distribution)) +\n  geom_line(size = 1) +\n  labs(\n    x = \"Valore\",\n    y = \"Densit√†\",\n    color = \"Distribuzione\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-30-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Simulazione della Distribuzione $t$\n\nSimuliamo una distribuzione $t$ con 10 gradi di libert√† e confrontiamola con la densit√† teorica.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Impostare il seed per la riproducibilit√†\nset.seed(123)\n\n# Simulare 1000 valori da una distribuzione t\nn <- 1000\ndf <- 10  # Gradi di libert√†\nt_values <- rt(n, df = df)\n\n# Creare un dataframe per il grafico\ndata <- data.frame(t_values = t_values)\n\n# Istogramma con densit√† teorica\nggplot(data, aes(x = t_values)) +\n  geom_histogram(\n    aes(y = after_stat(density)), \n    bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7\n  ) +\n  stat_function(fun = dt, args = list(df = df), color = \"red\", size = 1) +\n  labs(\n    x = \"Valore\",\n    y = \"Densit√†\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-31-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Propriet√† Teoriche della Distribuzione $t$\n\n1. **Media**:\n   $$\n   \\mathbb{E}[T] = 0, \\quad \\text{per } \\nu > 1.\n   $$\n\n2. **Varianza**:\n   $$\n   \\mathbb{V}[T] = \\frac{\\nu}{\\nu - 2}, \\quad \\text{per } \\nu > 2.\n   $$\n\n3. **Simmetria**:\n   - la distribuzione √® simmetrica rispetto a zero, come la Normale.\n\n4. **Code**:\n   - le code sono pi√π pesanti rispetto alla Normale, riflettendo una maggiore incertezza per piccoli campioni.\n\nIn conclusione, la distribuzione $t$ di Student √® uno strumento versatile nell'inferenza statistica, trovando applicazione in contesti sia frequentisti che bayesiani. √à particolarmente utile in situazioni in cui la conoscenza della varianza √® limitata o i campioni sono di dimensioni ridotte. Grazie alla sua forma simmetrica e alle code pi√π pesanti rispetto alla distribuzione Normale, la distribuzione $t$ pu√≤ modellare meglio l'incertezza, includendo una maggiore probabilit√† per valori estremi.\n\nNel contesto bayesiano, la distribuzione $t$ viene utilizzata come:\n\n- **prior informativo robusto**, per modellare parametri con valori plausibili lontani dalla media ma con una penalizzazione graduale per valori estremi.\n- **distribuzione predittiva** per sintetizzare l'incertezza derivante da campioni piccoli o con variabilit√† elevata.\n\nIn entrambi i paradigmi, la distribuzione $t$ rappresenta una scelta robusta, capace di riflettere in modo flessibile la natura dei dati. Inoltre, per valori elevati dei gradi di libert√†, la distribuzione $t$ converge alla distribuzione Normale, un caso limite che ne estende ulteriormente l‚Äôutilit√† in vari contesti analitici.\n\n## Funzione Beta di Eulero\n\nLa **funzione Beta di Eulero** √® una funzione matematica, non una densit√† di probabilit√†, ma √® strettamente collegata alla distribuzione Beta, poich√© appare nella sua definizione. Indicata comunemente con il simbolo $\\mathcal{B}(\\alpha, \\beta)$, la funzione Beta pu√≤ essere espressa in vari modi. Per i nostri scopi, utilizziamo la seguente definizione:\n\n$$\n\\mathcal{B}(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\,,\n$$ {#eq-eulero-beta-def}\n\ndove $\\Gamma(x)$ rappresenta la funzione Gamma, una generalizzazione del fattoriale definita per numeri reali positivi. Quando $x$ √® un numero intero, la funzione Gamma si riduce al fattoriale traslato:\n\n$$\n\\Gamma(x) = (x-1)!.\n$$\n\n::: {#exm-}\nSupponiamo di voler calcolare $\\mathcal{B}(3, 9)$. Utilizzando la definizione, abbiamo:\n\n$$\n\\mathcal{B}(3, 9) = \\frac{\\Gamma(3) \\cdot \\Gamma(9)}{\\Gamma(3 + 9)}.\n$$\n\nIn R, possiamo calcolarla in tre modi diversi.\n\n1. Utilizzando la definizione con la funzione `gamma()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nalpha <- 3\nbeta <- 9\n\nbeta_function <- gamma(alpha) * gamma(beta) / gamma(alpha + beta)\nbeta_function\n#> [1] 0.00202\n```\n:::\n\n\n2. Utilizzando direttamente la funzione `beta()` di R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbeta(alpha, beta)\n#> [1] 0.00202\n```\n:::\n\n\n3. Calcolo manuale con fattoriali:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(factorial(alpha - 1) * factorial(beta - 1)) / factorial(alpha + beta - 1)\n#> [1] 0.00202\n```\n:::\n\n\nTutti e tre i metodi restituiscono lo stesso risultato, confermando la correttezza della definizione.\n:::\n\nLa funzione Beta √® utilizzata nella definizione della **densit√† di probabilit√† Beta**. Essa serve a normalizzare la densit√†, garantendo che l'area sotto la curva sia pari a $1$. \n\n## Distribuzione Beta\n\nLa **distribuzione Beta**, indicata come $\\mathcal{Beta}(\\alpha, \\beta)$, √® una distribuzione di probabilit√† continua definita sull‚Äôintervallo $(0, 1)$. √à particolarmente utile per modellare proporzioni, probabilit√†, o in generale qualsiasi fenomeno che assume valori compresi tra 0 e 1.\n\nQuesta distribuzione √® molto flessibile: a seconda dei valori dei parametri $\\alpha$ e $\\beta$, pu√≤ assumere forme simmetriche, asimmetriche, concave, convesse, ecc. √à frequentemente utilizzata come distribuzione a priori nei modelli bayesiani per parametri che rappresentano probabilit√†.\n\n### Definizione\n\n:::: {.definition #def-beta}\nSia $\\theta$ una variabile casuale continua. Se $\\theta$ segue una distribuzione Beta con parametri $\\alpha > 0$ e $\\beta > 0$, scriviamo:\n\n$$\n\\theta \\sim \\mathcal{Beta}(\\alpha, \\beta),\n$$\n\ne la sua **funzione di densit√† di probabilit√†** (pdf) √® data da:\n\n$$\n\\mathcal{Beta}(\\theta \\mid \\alpha, \\beta) = \\frac{1}{\\mathcal{B}(\\alpha, \\beta)} \\, \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}, \\quad \\text{per } \\theta \\in (0, 1),\n$$ {#eq-beta-distr-def}\n\ndove $\\mathcal{B}(\\alpha, \\beta)$ √® la funzione Beta (o funzione beta di Eulero).\n::::\n\n### Rappresentazione alternativa\n\nUn‚Äôespressione equivalente della densit√†, che mette in evidenza il legame con la funzione Gamma, √®:\n\n$$\n\\mathcal{Beta}(\\theta \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\, \\Gamma(\\beta)} \\, \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}, \\quad \\text{per } \\theta \\in (0, 1),\n$$ {#eq-beta-distr-def2}\n\ndove $\\Gamma(\\cdot)$ √® la funzione Gamma, che generalizza il fattoriale: $\\Gamma(n) = (n - 1)!$ per ogni intero positivo $n$.\n\n\n### Ruolo dei Parametri $\\alpha$ e $\\beta$\n\nI parametri $\\alpha$ e $\\beta$ determinano la forma della distribuzione:\n\n- **$\\alpha > 1$**: favorisce valori di $\\theta$ vicini a 1.\n- **$\\beta > 1$**: favorisce valori di $\\theta$ vicini a 0.\n- **$\\alpha = \\beta = 1$**: corrisponde alla distribuzione uniforme sull'intervallo $[0, 1]$.\n- **$\\alpha, \\beta < 1$**: la distribuzione √® bimodale, concentrandosi agli estremi (vicino a 0 e 1).\n\n### Propriet√† della Distribuzione Beta\n\n1. **Valore atteso**:\n   $$\n   \\mathbb{E}(\\theta) = \\frac{\\alpha}{\\alpha + \\beta}.\n   $$\n\n2. **Varianza**:\n   $$\n   \\mathbb{V}(\\theta) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}.\n   $$\n\n3. **Moda** (se $\\alpha, \\beta > 1$):\n   $$\n   \\text{Moda}(\\theta) = \\frac{\\alpha - 1}{\\alpha + \\beta - 2}.\n   $$\n\nQueste propriet√† evidenziano come $\\alpha$ e $\\beta$ possano essere interpretati come \"successi\" e \"fallimenti\" in una serie di prove, fornendo un collegamento intuitivo con la distribuzione binomiale.\n\n### Relazione con la Distribuzione Binomiale\n\nLa **distribuzione Beta** pu√≤ essere interpretata come una generalizzazione continua della distribuzione binomiale. Mentre la distribuzione binomiale descrive la **probabilit√† di osservare un certo numero di successi** in un numero fissato di prove ($n$), la distribuzione Beta descrive l‚Äô**incertezza sulla probabilit√† di successo** $\\theta$ stessa, trattandola come una variabile casuale.\n\n#### Contesto bayesiano\n\nIn un contesto di **inferenza bayesiana**, la distribuzione Beta viene comunemente utilizzata come **distribuzione a priori coniugata** per il modello binomiale. Ci√≤ significa che, se si assume una distribuzione Beta come prior per $\\theta$, anche la distribuzione a posteriori (dopo aver osservato i dati) sar√† una Beta, ma con parametri aggiornati.\n\nSupponiamo:\n\n- che $\\theta$ sia la probabilit√† di successo in un compito con esiti binari (es. risposta corretta o errata),\n- di assumere una distribuzione a priori:  \n  $$\n  \\theta \\sim \\mathcal{Beta}(\\alpha, \\beta),\n  $$\n- e di osservare $y$ successi su $n$ prove, con modello di verosimiglianza:\n  $$\n  y \\sim \\mathcal{Binom}(n, \\theta).\n  $$\n\nAllora, per il teorema di Bayes, la distribuzione a posteriori di $\\theta$ sar√†:\n\n$$\n\\theta \\mid \\text{dati} \\sim \\mathcal{Beta}(\\alpha + y, \\beta + n - y).\n$$\n\n#### Vantaggi della coniugatezza\n\nQuesto aggiornamento √® particolarmente comodo perch√©:\n\n- si ottiene in forma chiusa (senza dover ricorrere a metodi numerici),\n- i parametri $\\alpha$ e $\\beta$ possono essere interpretati come **conteggi fittizi di successi e insuccessi** prima dell‚Äôosservazione dei dati,\n- l‚Äôinformazione a priori e quella empirica si combinano sommando i rispettivi ‚Äúconteggi.‚Äù\n\nQuesta propriet√† rende la distribuzione Beta una scelta naturale nei modelli bayesiani con dati binomiali, come in contesti psicologici in cui si vogliono modellare incertezze sulla probabilit√† di una risposta corretta, sull‚Äôesito di una scelta, o sul successo di un comportamento.\n\n### Visualizzazione della Distribuzione Beta\n\nDi seguito mostriamo come la forma della distribuzione Beta varia al variare dei parametri $\\alpha$ e $\\beta$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri\nx <- seq(0, 1, length.out = 200)\nalphas <- c(0.5, 5.0, 1.0, 2.0, 2.0)\nbetas <- c(0.5, 1.0, 3.0, 2.0, 5.0)\n\n# Creare un dataframe\ndf <- do.call(rbind, lapply(1:length(alphas), function(i) {\n  data.frame(\n    x = x,\n    density = dbeta(x, alphas[i], betas[i]),\n    label = paste0(\"Œ± = \", alphas[i], \", Œ≤ = \", betas[i])\n  )\n}))\n\n# Plot\nggplot(df, aes(x = x, y = density, color = label)) +\n  geom_line(size = 1) +\n  labs(\n    x = \"x\",\n    y = \"f(x)\"\n  ) +\n  theme(legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-35-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Costante di Normalizzazione\n\nLa costante di normalizzazione della distribuzione Beta √® il reciproco della funzione Beta di Eulero, $B(\\alpha, \\beta)$. Questa garantisce che:\n\n$$\n\\int_0^1 \\mathcal{Beta}(\\theta \\mid \\alpha, \\beta) \\, d\\theta = 1.\n$$\n\n::: {#exm-}\nDi seguito viene proposto un esempio in R per calcolare l'area sottesa alla distribuzione Beta non normalizzata e, con gli stessi parametri, ottenere il valore della funzione Beta di Eulero. L'obiettivo √® mostrare come la costante di normalizzazione, pari al reciproco di $B(\\alpha, \\beta)$, garantisca che l'integrale della densit√† normalizzata su $[0,1]$ sia pari a 1.\n\nSupponiamo di voler utilizzare i parametri:\n\n- $\\alpha = 2$\n- $\\beta = 5$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri della distribuzione Beta\nalpha <- 2\nbeta  <- 5\n\n# Definiamo la funzione non normalizzata della distribuzione Beta\nunnormalized_beta <- function(theta) {\n  theta^(alpha - 1) * (1 - theta)^(beta - 1)\n}\n\n# Calcoliamo l'integrale della funzione non normalizzata su [0, 1]\nintegrale <- integrate(unnormalized_beta, lower = 0, upper = 1)$value\ncat(\"Integrale della funzione non normalizzata:\", integrale, \"\\n\")\n#> Integrale della funzione non normalizzata: 0.0333\n\n# Calcoliamo il valore della funzione Beta usando la funzione beta() di R\nvalore_beta <- beta(alpha, beta)\ncat(\"Valore della funzione Beta B(alpha, beta):\", valore_beta, \"\\n\")\n#> Valore della funzione Beta B(alpha, beta): 0.0333\n```\n:::\n\n\nSpiegazione del Codice\n\n1. **Definizione dei Parametri e della Funzione**  \n   Impostiamo $\\alpha = 2$ e $\\beta = 5$ e definiamo la funzione non normalizzata:\n   $$\n   f(\\theta) = \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}.\n   $$\n\n2. **Calcolo dell'Integrale**  \n   Utilizzando la funzione `integrate()`, calcoliamo l'area sottesa a $f(\\theta)$ nell'intervallo $[0,1]$, che corrisponde a $\\mathcal{B}(\\alpha, \\beta)$.\n\n3. **Verifica con la Funzione Beta**  \n   La funzione `beta(alpha, beta)` di R restituisce direttamente il valore di $\\mathcal{B}(\\alpha, \\beta)$. La stampa dei due valori conferma che l'integrale calcolato e il valore della funzione Beta coincidono.\n\n4. **Costante di Normalizzazione**  \n   Il reciproco di $\\mathcal{B}(\\alpha, \\beta)$ √® calcolato e utilizzato per definire la densit√† normalizzata della distribuzione Beta. L'integrazione della densit√† normalizzata su $[0,1]$ restituisce 1, confermando la corretta normalizzazione.\n\nQuesto esempio in R mostra in modo pratico come la costante di normalizzazione derivi dalla funzione Beta di Eulero e come essa venga applicata per ottenere una densit√† di probabilit√† correttamente normalizzata.\n:::\n\nIn conclusione, la distribuzione Beta si rivela particolarmente utile per modellare variabili continue comprese nell'intervallo [0, 1]. Grazie alla sua parametrizzazione tramite $\\alpha$ e $\\beta$, consente di adattare la forma della densit√† in modo specifico alle caratteristiche osservate dei dati, facilitando la stima di proporzioni. Inoltre, essendo il coniugato della distribuzione binomiale, permette un aggiornamento analitico nei modelli bayesiani, semplificando l'inferenza quando si raccolgono dati incrementali, come nella stima della probabilit√† di successo in esperimenti o studi psicologici.\n\n## Distribuzione di Cauchy\n\nLa **distribuzione di Cauchy** √® un caso speciale della distribuzione $t$ di Student con un solo grado di libert√† ($t_1$). Questa distribuzione √® caratterizzata da code molto pesanti e da una media e varianza non definite, rendendola particolarmente utile in contesti dove valori estremi possono avere un'influenza importante.\n\n::: {#def-}\nLa funzione di densit√† di probabilit√† della distribuzione di Cauchy √® definita da due parametri:\n\n- **$\\alpha$**: posizione (location), che determina il centro della distribuzione.\n- **$\\beta > 0$**: scala (scale), che controlla la larghezza della distribuzione.\n\nLa densit√† √® data da:\n\n$$\nf(x \\mid \\alpha, \\beta) = \\frac{1}{\\pi \\beta \\left[1 + \\left( \\frac{x - \\alpha}{\\beta} \\right)^2\\right]} ,\n$$\n\ndove:\n\n- $x \\in \\mathbb{R}$,\n- $\\alpha \\in \\mathbb{R}$,\n- $\\beta > 0$.\n\nQuesta funzione descrive una distribuzione simmetrica attorno a $\\alpha$, con code pi√π pesanti rispetto alla distribuzione Normale.\n:::\n\n### Propriet√† della Distribuzione di Cauchy\n\n1. **Simmetria**: La distribuzione √® simmetrica rispetto a $\\alpha$.\n2. **Code Pesanti**: Le code sono significativamente pi√π pesanti rispetto alla distribuzione Normale, con una decrescita pi√π lenta ($\\propto x^{-2}$).\n3. **Media e Varianza**: La distribuzione non ha una media n√© una varianza definita.\n4. **Relazione con $t_1$**: La distribuzione di Cauchy √® equivalente a una distribuzione $t$ di Student con 1 grado di libert√†.\n5. **Caratteristiche Estreme**: I valori estremi hanno una probabilit√† pi√π alta rispetto ad altre distribuzioni comuni, rendendola utile per modellare fenomeni con outlier significativi.\n\n### Visualizzazione della Distribuzione di Cauchy\n\nPer comprendere l'effetto dei parametri $\\alpha$ e $\\beta$ sulla forma della distribuzione, consideriamo alcuni esempi con:\n\n- $\\alpha = 0.0, 0.0, 0.0, -2.0$,\n- $\\beta = 0.5, 1.0, 2.0, 1.0$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Definire i parametri\nx <- seq(-5, 5, length.out = 500)\nalphas <- c(0.0, 0.0, 0.0, -2.0)\nbetas <- c(0.5, 1.0, 2.0, 1.0)\n\n# Creare un data frame per i risultati\ndf <- do.call(rbind, lapply(1:length(alphas), function(i) {\n  data.frame(\n    x = x,\n    density = dcauchy(x, location = alphas[i], scale = betas[i]),\n    label = paste0(\"Œ± = \", alphas[i], \", Œ≤ = \", betas[i])\n  )\n}))\n\n# Grafico\nggplot(df, aes(x = x, y = density, color = label)) +\n  geom_line(size = 1) +\n  labs(\n    x = \"x\",\n    y = \"f(x)\"\n  ) +\n  theme(\n    legend.title = element_blank()\n  )\n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-37-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Applicazioni della Distribuzione di Cauchy\n\n1. **Inferenza Bayesiana**:\n\n   - Utilizzata come prior **robusto** in modelli bayesiani, particolarmente quando si vuole attribuire una probabilit√† maggiore a valori estremi rispetto a una distribuzione Normale.\n\n2. **Modellazione di Fenomeni con Outlier**:\n\n   - La distribuzione di Cauchy √® adatta per descrivere dati con valori estremi significativi che possono influenzare fortemente altre distribuzioni.\n\nIn conclusione, la distribuzione di Cauchy, con le sue propriet√† uniche come code pesanti e l'assenza di media e varianza definite, √® uno strumento fondamentale per modellare fenomeni in cui i valori estremi giocano un ruolo importante. La sua relazione con la distribuzione $t$ di Student e la sua utilit√† nei modelli bayesiani ne ampliano ulteriormente le applicazioni in contesti statistici e probabilistici avanzati.\n\n## Distribuzione Gamma\n\nLa **distribuzione Gamma** √® una distribuzione di probabilit√† continua utilizzata principalmente per modellare variabili strettamente positive, come tassi, varianze, o tempi di attesa. √à usata nella statistica bayesiana come distribuzione a priori per parametri positivi e trova applicazione in ambiti come la modellazione di eventi rari.\n\n::: {#def-}\nLa distribuzione Gamma √® caratterizzata da due parametri principali:\n\n- **Parametro di forma** ($\\alpha$): determina la forma generale della distribuzione.\n- **Parametro di scala** ($\\theta$) o, alternativamente, il **parametro di tasso** ($\\beta = 1/\\theta$): regola la larghezza o la dispersione della distribuzione.\n\nLa funzione di densit√† di probabilit√† (PDF) √® data da:\n\n$$\nf(x \\mid \\alpha, \\theta) = \\frac{x^{\\alpha-1} e^{-x/\\theta}}{\\theta^\\alpha \\Gamma(\\alpha)}, \\quad x > 0,\n$$\n\ndove:\n\n- $x$ √® la variabile casuale continua,\n- $\\Gamma(\\alpha)$ √® la funzione Gamma di Eulero, definita come:\n\n$$\n\\Gamma(\\alpha) = \\int_0^\\infty t^{\\alpha-1} e^{-t} dt.\n$$\n\nSe utilizziamo il parametro di tasso $\\beta = 1/\\theta$, la PDF pu√≤ essere scritta come:\n\n$$\nf(x \\mid \\alpha, \\beta) = \\frac{\\beta^\\alpha x^{\\alpha-1} e^{-\\beta x}}{\\Gamma(\\alpha)}, \\quad x > 0.\n$$\n:::\n\n### Propriet√† della Distribuzione Gamma\n\n1. **Media**:\n   $$\n   \\mathbb{E}[X] = \\alpha \\cdot \\theta = \\frac{\\alpha}{\\beta}.\n   $$\n\n2. **Varianza**:\n   $$\n   \\text{Var}(X) = \\alpha \\cdot \\theta^2 = \\frac{\\alpha}{\\beta^2}.\n   $$\n\n3. **Moda** (per $\\alpha > 1$):\n   $$\n   \\text{Moda}(X) = (\\alpha - 1) \\cdot \\theta.\n   $$\n\n\nDi seguito, mostriamo un esempio per $\\alpha = 3$ e $\\beta = 5/3$, calcolando e rappresentando graficamente la distribuzione.\n\n1. **Calcolo della Media e della Deviazione Standard**:\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   # Parametri\n   alpha <- 3\n   beta <- 5 / 3\n   \n   # Calcolo\n   mean <- alpha / beta\n   sigma <- sqrt(alpha / beta^2)\n   \n   cat(\"Media:\", mean, \"\\n\")\n   #> Media: 1.8\n   cat(\"Deviazione Standard:\", sigma, \"\\n\")\n   #> Deviazione Standard: 1.04\n   ```\n   :::\n\n\n2. **Generazione e Plot dei Dati**:\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   # Generazione di dati\n   set.seed(123)\n   data <- rgamma(100000, shape = alpha, rate = beta)\n   \n   # Data frame per ggplot\n   df <- data.frame(values = data)\n   \n   # Plot\n   ggplot(df, aes(x = values)) +\n     geom_histogram(aes(y = ..density..), bins = 30, fill = \"green\", alpha = 0.6) +\n     stat_function(fun = function(x) dgamma(x, shape = alpha, rate = beta),\n                   color = \"red\", size = 1) +\n     labs(\n       x = \"Valore\",\n       y = \"Densit√† di probabilit√†\"\n     ) \n   ```\n   \n   ::: {.cell-output-display}\n   ![](14_cont_rv_distr_files/figure-html/unnamed-chunk-39-1.png){fig-align='center' width=85%}\n   :::\n   :::\n\n\n### Applicazioni della Distribuzione Gamma\n\n1. **Modellazione del Tempo di Attesa**:  \n   La distribuzione Gamma √® ideale per modellare tempi di attesa, ad esempio, il tempo necessario affinch√© si verifichino $n$ eventi in un processo di Poisson.\n\n2. **Inferenza Bayesiana**:  \n   - Utilizzata come prior per parametri positivi, come tassi ($\\lambda$) o varianze ($\\sigma^2$).\n   - Ad esempio, nella modellazione bayesiana dei processi di Poisson, una distribuzione Gamma √® una scelta naturale per il prior su $\\lambda$.\n\n\n## Riflessioni conclusive\n\nLe distribuzioni di probabilit√† costituiscono il cuore dell'inferenza statistica, sia bayesiana che frequentista. In questo capitolo, abbiamo esplorato come R offre un insieme completo di strumenti per lavorare con diverse distribuzioni, permettendo di modellare e analizzare una vasta gamma di fenomeni.\n\n### Principali Applicazioni\n\n1. **Inferenza Bayesiana**:  \n   Le distribuzioni di probabilit√†, come la Beta, la Gamma e la Normale, sono essenziali per definire priors, calcolare posteriori e quantificare l'incertezza nei modelli bayesiani. Ad esempio:\n   - La distribuzione Beta √® ideale per modellare credenze a priori su proporzioni o probabilit√†.\n   - La distribuzione Gamma √® ampiamente usata per modellare parametri positivi come tassi o varianze.\n\n2. **Analisi Statistica e Modellazione**:  \n   Le distribuzioni, come la $t$ di Student, sono fondamentali per il confronto tra campioni, mentre la Normale √® indispensabile per modellare fenomeni che seguono la legge del limite centrale.\n\n3. **Generazione e Simulazione di Dati**:  \n   R permette di generare campioni casuali da distribuzioni comuni, utili per simulazioni, bootstrap e validazione di modelli.\n\n### Funzionalit√† di R\n\nCon poche funzioni, R consente di:\n\n- **Generare campioni casuali**: con funzioni come `rnorm`, `rgamma`, `rbeta`, possiamo simulare dati da distribuzioni specifiche.\n- **Calcolare densit√†**: ad esempio, con `dnorm`, `dgamma`, `dbeta`, possiamo visualizzare le funzioni di densit√†.\n- **Calcolare probabilit√† cumulate**: con funzioni come `pnorm`, `pbeta`, possiamo determinare probabilit√† su intervalli specifici.\n- **Determinare quantili**: con funzioni come `qnorm`, `qgamma`, possiamo calcolare i punti corrispondenti a specifici livelli di probabilit√†.\n\n### Versatilit√† delle Distribuzioni\n\nLe distribuzioni esplorate non solo descrivono fenomeni naturali, ma sono anche i \"mattoncini\" per costruire modelli statistici complessi. Le loro propriet√†, come la media, la varianza, la simmetria o le code pesanti, consentono di adattare il modello al fenomeno studiato.\n\nIn conclusione, il linguaggio R, con la sua flessibilit√† e ricchezza di strumenti, permette di padroneggiare le distribuzioni di probabilit√†, non solo come oggetti matematici, ma anche come strumenti pratici per rispondere a domande complesse. La comprensione e l'uso delle distribuzioni presentate costituiscono le fondamenta per avanzare verso tecniche pi√π sofisticate, come l'inferenza bayesiana avanzata o la modellazione gerarchica. \n\n## Esercizi {.unnumbered}\n\n::: {.callout-important title=\"Problemi\" collapse=\"true\"}\nEsercizi sulla distribuzione normale, risolvibili usando R, sono disponibili sulla seguente [pagina web](https://mathcenter.oxford.emory.edu/site/math117/probSetNormalDistribution/).\n:::\n\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#>  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#>  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#> [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#> [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#> [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#> [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#> [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#> [25] here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#>  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#>  [7] digest_0.6.37         timechange_0.3.0      estimability_1.5.1   \n#> [10] lifecycle_1.0.4       survival_3.8-3        magrittr_2.0.3       \n#> [13] compiler_4.5.1        rlang_1.1.6           tools_4.5.1          \n#> [16] knitr_1.50            labeling_0.4.3        bridgesampling_1.1-2 \n#> [19] htmlwidgets_1.6.4     curl_7.0.0            pkgbuild_1.4.8       \n#> [22] RColorBrewer_1.1-3    abind_1.4-8           multcomp_1.4-28      \n#> [25] withr_3.0.2           purrr_1.1.0           grid_4.5.1           \n#> [28] stats4_4.5.1          colorspace_2.1-1      xtable_1.8-4         \n#> [31] inline_0.3.21         emmeans_1.11.2-8      scales_1.4.0         \n#> [34] MASS_7.3-65           cli_3.6.5             mvtnorm_1.3-3        \n#> [37] rmarkdown_2.29        ragg_1.5.0            generics_0.1.4       \n#> [40] RcppParallel_5.1.11-1 cachem_1.1.0          stringr_1.5.1        \n#> [43] splines_4.5.1         parallel_4.5.1        vctrs_0.6.5          \n#> [46] V8_7.0.0              Matrix_1.7-4          sandwich_3.1-1       \n#> [49] jsonlite_2.0.0        arrayhelpers_1.1-0    systemfonts_1.2.3    \n#> [52] glue_1.8.0            codetools_0.2-20      distributional_0.5.0 \n#> [55] lubridate_1.9.4       stringi_1.8.7         gtable_0.3.6         \n#> [58] QuickJSR_1.8.0        htmltools_0.5.8.1     Brobdingnag_1.2-9    \n#> [61] R6_2.6.1              textshaping_1.0.3     rprojroot_2.1.1      \n#> [64] evaluate_1.0.5        lattice_0.22-7        backports_1.5.0      \n#> [67] memoise_2.0.1         broom_1.0.9           snakecase_0.11.1     \n#> [70] rstantools_2.5.0      coda_0.19-4.1         gridExtra_2.3        \n#> [73] nlme_3.1-168          checkmate_2.3.3       xfun_0.53            \n#> [76] zoo_1.8-14            pkgconfig_2.0.3\n```\n:::\n\n\n## Bibliografia {.unnumbered}\n",
    "supporting": [
      "14_cont_rv_distr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}