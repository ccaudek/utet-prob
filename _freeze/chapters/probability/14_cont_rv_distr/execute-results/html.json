{
  "hash": "5afa1bf6dc5c3cfab8a867664f2c56eb",
  "result": {
    "engine": "knitr",
    "markdown": "# Distribuzioni di v.c. continue {#sec-prob-cont-prob-distr}\n\n\n::: callout-important\n## In questo capitolo imparerai a:\n\n- comprendere le principali distribuzioni di densità di probabilità;\n- utilizzare R per manipolare e analizzare queste distribuzioni.\n::: \n\n::: callout-tip\n## Prerequisiti\n\n- Leggere il capitolo *Continuous random variables* [@blitzstein2019introduction].\n- Leggere il capitolo *Special Distributions* [@schervish2014probability].\n- Leggere il capitolo *Random Variables and Probability Distributions* [@schervish2014probability].\n:::\n\n::: callout-caution\n## Preparazione del Notebook\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n```\n:::\n\n:::\n\n\n## Introduzione\n\nProprio come per le variabili casuali discrete, anche per le variabili casuali continue è possibile rappresentare la variabilità di una popolazione attraverso un modello statistico. Tuttavia, mentre le distribuzioni discrete si applicano a fenomeni con un numero finito o numerabile di esiti, le variabili casuali continue richiedono l'uso di **funzioni di densità di probabilità** (pdf), che descrivono fenomeni in cui i valori possono assumere un continuum di possibilità. Queste funzioni ci permettono di modellare e analizzare situazioni in cui i risultati non sono discreti, ma possono variare in modo continuo.\n\nLa funzione di densità di probabilità $f(x)$ associata a una variabile casuale continua $X$ rappresenta la distribuzione della probabilità all'interno della popolazione. A differenza delle distribuzioni discrete, dove la probabilità è assegnata direttamente a singoli valori, la pdf non fornisce la probabilità di un singolo punto, ma descrive la probabilità che $X$ assuma valori all'interno di un intervallo specifico. Questo approccio consente di costruire un modello matematico della popolazione, utile per fare previsioni e comprendere meglio i fenomeni aleatori continui.\n\n## La Distribuzione Uniforme Continua\n\nLa **distribuzione uniforme continua** rappresenta un pilastro della teoria delle probabilità, caratterizzata da una densità di probabilità costante su un intervallo definito. Questo modello è particolarmente utile per descrivere fenomeni casuali dove ogni esito possibile ha identica probabilità di verificarsi, come nel caso di uno spinner perfettamente bilanciato o di un generatore di numeri casuali ideale.\n\n### Un esempio intuitivo: lo spinner  \nConsideriamo uno spinner circolare con valori angolari compresi tra 0° e 360°. Se il dispositivo è **perfettamente equilibrato**, ogni angolo ha la stessa probabilità di essere selezionato dopo una rotazione. Questo esperimento costituisce un’implementazione concreta della distribuzione uniforme sull’intervallo $[0, 360)$.\n\n### Simulazione della distribuzione: dal campione piccolo alla convergenza teorica  \n\nPer illustrare il comportamento della distribuzione, analizziamo due scenari distinti attraverso simulazioni numeriche.\n\n**Caso 1: Campione piccolo (n = 20)**  \n\nGeneriamo 20 valori casuali e visualizziamoli con un istogramma:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)  # Riproducibilità dei risultati\nspinner_results <- runif(20, min = 0, max = 360)\nprint(spinner_results)  # Output dei dati\n#>  [1] 103.5 283.8 147.2 317.9 338.6  16.4 190.1 321.3 198.5 164.4 344.5 163.2\n#> [13] 243.9 206.1  37.1 323.9  88.6  15.1 118.1 343.6\n\n# Creazione dell'istogramma\nggplot(data.frame(Valori = spinner_results), aes(x = Valori)) +\n  geom_histogram(\n    binwidth = 10, \n    fill = \"skyblue\", \n    color = \"black\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"Angolo (gradi)\", \n    y = \"Frequenza relativa\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nL’istogramma mostra un andamento **irregolare**, riflettendo la variabilità intrinseca dei piccoli campioni. Questa disomogeneità è attesa e diminuisce all’aumentare della dimensione campionaria.\n\n**Caso 2: Campione grande (n = 100,000)**  \n\nRipetiamo la simulazione con un campione esteso:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspinner_results_large <- runif(100000, min = 0, max = 360)\n\nggplot(data.frame(Valori = spinner_results_large), aes(x = Valori)) +\n  geom_histogram(\n    binwidth = 10, \n    fill = \"skyblue\", \n    color = \"black\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"Angolo (gradi)\", \n    y = \"Frequenza relativa\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nL’istogramma ora rivela un profilo **piatto e regolare**, in accordo con la forma teorica della distribuzione. Questo risultato dimostra empiricamente la Legge dei Grandi Numeri, dove all’aumentare delle osservazioni la distribuzione empirica converge a quella teorica.\n\n### La Funzione di Densità di Probabilità (PDF) \n\nPer una variabile casuale $X \\sim \\mathcal{U}(a, b)$, la PDF è definita come:\n\n$$\nf(x) = \n\\begin{cases} \n  \\displaystyle \\frac{1}{b - a} & \\text{se } x \\in [a, b], \\\\\n  0 & \\text{altrimenti}.\n\\end{cases}\n$$\n\n**Proprietà chiave:**  \n\n- l’area totale sotto la curva è unitaria: $\\int_{a}^{b} \\frac{1}{b - a} \\, dx = 1$;  \n- la densità è nulla al di fuori dell’intervallo $[a, b]$.  \n\n**Applicazione allo spinner:**  \n\n$$\nf(x) = \\frac{1}{360} \\quad \\text{per } x \\in [0, 360].\n$$\n\n**Visualizzazione grafica in R:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- seq(-50, 410, length.out = 500)  \ndensity_uniform <- dunif(x, min = 0, max = 360)\n\nggplot(data.frame(x = x, y = density_uniform), aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"blue\") +\n  geom_vline(xintercept = c(0, 360), linetype = \"dashed\", color = \"red\") +\n  labs(\n    x = \"x (gradi)\", \n    y = \"Densità f(x)\"\n  ) +\n  xlim(-50, 410) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIl grafico evidenzia la densità costante nell’intervallo $[0, 360]$ e l’assenza di probabilità al di fuori di esso.\n\n### Calcolo delle Probabilità: Metodo Geometrico e Funzionale  \n\nLa probabilità che $X$ assuma valori in un sottointervallo $[c, d] \\subseteq [a, b]$ è data da:  \n\n$$\nP(c \\leq X \\leq d) = \\frac{d - c}{b - a}.\n$$\n\n**Esempio applicativo:**  \n\nCalcoliamo la probabilità che lo spinner si fermi tra 150° e 250°:  \n\n$$\nP(150 \\leq X \\leq 250) = \\frac{250 - 150}{360} = \\frac{100}{360} = \\frac{5}{18} \\approx 0.2778.\n$$\n\n**Conferma numerica in R:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Approccio manuale\nprob_manuale <- (250 - 150) / 360  \n\n# Utilizzo della funzione cumulativa (CDF)\nprob_cdf <- punif(250, min = 0, max = 360) - punif(150, min = 0, max = 360)  \n```\n:::\n\n\n**Rappresentazione grafica dell’area di probabilità:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data.frame(x = x, fx = density_uniform), aes(x = x, y = fx)) +\n  geom_line(linewidth = 1.2, color = \"blue\") +\n  geom_area(\n    data = subset(data.frame(x, fx = density_uniform), x >= 150 & x <= 250),\n    aes(x = x, y = fx), \n    fill = \"gray\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"x (gradi)\", \n    y = \"Densità f(x)\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nL’area grigia corrisponde esattamente al valore di probabilità calcolato, illustrando visivamente il concetto di integrazione della PDF.\n\n### Proprietà Fondamentali: Media e Varianza  \n\nPer $X \\sim \\mathcal{U}(a, b)$ valgono le seguenti relazioni:  \n\n1. **Valore atteso (centro della distribuzione):**  \n\n   $$\n   E(X) = \\frac{a + b}{2}.\n   $$  \n   \n   *Esempio:* Per lo spinner, $E(X) = (0 + 360)/2 = 180$ gradi.\n\n2. **Varianza (misura di dispersione):**  \n\n   $$\n   \\text{Var}(X) = \\frac{(b - a)^2}{12}.\n   $$  \n   \n   *Esempio:* Per lo spinner, $\\text{Var}(X) = (360 - 0)^2 / 12 = 10,\\!800$ gradi².\n\n### Implementazione in R: Funzioni Principali  \n\nR fornisce quattro funzioni per lavorare con la distribuzione uniforme:  \n\n| Funzione  | Descrizione                           | Esempio d’uso                   |\n|-----------|---------------------------------------|----------------------------------|\n| `runif()` | Genera valori casuali                 | `runif(5, min=0, max=1)`        |\n| `dunif()` | Calcola la densità $f(x)$             | `dunif(180, min=0, max=360)`    |\n| `punif()` | Calcola la CDF $P(X \\leq x)$          | `punif(250, min=0, max=360)`    |\n| `qunif()` | Determina il quantile per una probabilità | `qunif(0.9, min=0, max=360)`    |\n\n**Esempi operativi:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 1. Generazione di 5 numeri casuali in [0, 1]\nrunif(5, min = 0, max = 1)  \n#> [1] 0.0372 0.9278 0.4480 0.9159 0.3558\n\n# 2. Valore della densità in x = 0.5 per U(0,1)\ndunif(0.5, min = 0, max = 1)  \n#> [1] 1\n\n# 3. Probabilità cumulativa fino a x = 0.8 per U(0,1)\npunif(0.8, min = 0, max = 1)  \n#> [1] 0.8\n\n# 4. Calcolo del quantile corrispondente al 50° percentile (mediana)\nqunif(0.5, min = 0, max = 360)  \n#> [1] 180\n```\n:::\n\n\n## La Distribuzione Esponenziale  \n\nLa **distribuzione esponenziale** è una distribuzione continua fondamentale per modellare il **tempo di attesa** fino al verificarsi di un evento casuale. La sua caratteristica distintiva è la **proprietà di assenza di memoria**, che la rende unica nel panorama delle distribuzioni probabilistiche.  \n\n### La proprietà di assenza di memoria: un concetto chiave  \n\nL’**assenza di memoria** implica che la probabilità che un evento si verifichi in un intervallo futuro è indipendente dal tempo già trascorso.  \n\n**Esempio intuitivo:**  \nimmaginiamo una persona che sperimenta **attacchi di ansia improvvisi**, il cui tempo tra un episodio e il successivo segue una distribuzione esponenziale. Se l’individuo non ha avuto un attacco nelle ultime 2 settimane, la probabilità che ne sperimenti uno nei prossimi 3 giorni è **identica** a quella di una persona appena uscita da un episodio, nel medesimo intervallo di 3 giorni.  \n\nQuesta analogia illustra la **proprietà di assenza di memoria**: il \"tempo trascorso dall’ultimo evento\" (in questo caso, un attacco di ansia) non influenza la probabilità futura. Il sistema non \"accumula stress\" né riduce il rischio col passare del tempo senza episodi, riflettendo dinamiche tipiche di processi psicologici **non legati a meccanismi di apprendimento o adattamento**.  \n\n**Parametri chiave nell’esempio:** \n\n- **$\\lambda$ (tasso):** Frequenza media degli attacchi (es. 0.1 episodi/giorno). \n- **$\\mu = 1/\\lambda$ (media):** Tempo medio tra due episodi (es. 10 giorni).  \n\nLa distribuzione esponenziale modellizza così situazioni in cui il comportamento è **puramente stocastico** e non influenzato dalla storia precedente, come certi pattern di ansia, impulsività o reazioni fisiologiche a stimoli neutri.\n\n### Struttura matematica: PDF e parametri  \n\nLa **funzione di densità di probabilità (PDF)** di una variabile $X \\sim \\text{Exp}(\\lambda)$ è:  \n\n$$\nf(x) = \n\\begin{cases} \n\\lambda e^{-\\lambda x} & \\text{se } x \\geq 0, \\\\\n0 & \\text{altrimenti},\n\\end{cases}\n$$  \n\ndove:  \n\n- **$\\lambda$ (tasso):** numero medio di eventi per unità di tempo (es. 0.25 episodi/ora).  \n- **$\\mu = 1/\\lambda$ (media):** tempo medio di attesa per l’evento (es. 4 ore/episodio).  \n\n**Forma alternativa con $\\mu$:**  \n\n$$\nf(x) = \\frac{1}{\\mu} e^{-x/\\mu} \\quad \\text{per } x \\geq 0.\n$$  \n\n### Proprietà fondamentali  \n\nPer $X \\sim \\text{Exp}(\\lambda)$:  \n\n| Proprietà               | Formula                  | Interpretazione                          |  \n|-------------------------|--------------------------|------------------------------------------|  \n| **Valore atteso (μ)**   | $E(X) = \\frac{1}{\\lambda}$ | Tempo medio di attesa per l’evento.      |  \n| **Varianza**            | $\\text{Var}(X) = \\frac{1}{\\lambda^2}$ | Dispersione cresce col quadrato di 1/λ. |  \n| **Deviazione standard** | $\\sigma_X = \\frac{1}{\\lambda}$       | Spread lineare attorno alla media.       |  \n\n**Esempio applicato.**  \nSe il tempo medio di pubblicazione dei voti di un esame universitario è $\\mu = 4$ giorni ($\\lambda = 0.25$), la PDF è:  \n\n$$\nf(x) = \\frac{1}{4} e^{-x/4} \\quad (x \\geq 0).\n$$  \n\n### Visualizzazione della densità in R  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Definizione dei parametri\nmu <- 4\nlambda <- 1 / mu  # 0.25\n\n# Generazione dei punti per il grafico\nx <- seq(0, 20, by = 0.1)\npdf <- dexp(x, rate = lambda)\n\n# Creazione del grafico\nggplot(data.frame(x = x, y = pdf), aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  labs(\n    x = \"Tempo di attesa (giorni)\", \n    y = \"Densità f(x)\",\n    title = paste(\"PDF esponenziale (μ =\", mu, \"giorni)\")\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIl grafico mostra un **decadimento esponenziale**: la probabilità decresce rapidamente all’aumentare del tempo.  \n\n### Calcolo delle Probabilità: Tre Scenari   \n\n**1. Probabilità cumulativa: $P(X \\leq 1.5)$** -- qual è la probabilità che il voto venga pubblicato entro un giorno e mezzo?\n\nUtilizziamo la **funzione di ripartizione (CDF)**:  \n\n$$\nP(X \\leq 1.5) = 1 - e^{-\\lambda \\cdot 1.5} = 1 - e^{-0.25 \\cdot 1.5} \\approx 0.312.\n$$  \n\n**Codice R:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npexp(1.5, rate = lambda)  # Restituisce 0.312\n#> [1] 0.313\n```\n:::\n\n\n**Visualizzazione:** \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Area sotto la curva per X <= 1.5\nggplot(data.frame(x = x, y = pdf), aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_area(\n    data = subset(data.frame(x, y = pdf), x <= 1.5),\n    aes(x = x, y = y), \n    fill = \"gray\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"Tempo (giorni)\", \n    y = \"Densità\",\n    title = \"Probabilità P(X ≤ 1.5)\"\n  )\n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**2. Probabilità intervallo: $P(1 \\leq X \\leq 6)$** -- qual è la probabilità che il voto venga pubblicato in un intervallo compreso tra 1 e 6 giorni dopo l'esame?\n\nCalcoliamo la differenza tra due CDF:  \n\n$$\nP(1 \\leq X \\leq 6) = F(6) - F(1) = e^{-0.25 \\cdot 1} - e^{-0.25 \\cdot 6} \\approx 0.491.\n$$  \n\n**Codice R:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npexp(6, rate = lambda) - pexp(1, rate = lambda)  # 0.491\n#> [1] 0.556\n```\n:::\n\n\n**Visualizzazione:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Area per 1 <= X <= 6\nggplot(data.frame(x = x, y = pdf), aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_area(\n    data = subset(data.frame(x, y = pdf), x >= 1 & x <= 6),\n    aes(x = x, y = y), \n    fill = \"gray\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"Tempo (giorni)\", \n    y = \"Densità\",\n    title = \"Probabilità P(1 ≤ X ≤ 6)\"\n  )\n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**3. Probabilità della coda: $P(X \\geq 5.5)$** -- qual è la probabilità di un ritardo nella pubblicazione del voto superiore a 5.5 giorni dall'esame?\n\nUsiamo il complemento della CDF:  \n\n$$\nP(X \\geq 5.5) = 1 - P(X \\leq 5.5) = e^{-0.25 \\cdot 5.5} \\approx 0.252.\n$$  \n\n**Codice R:** \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n1 - pexp(5.5, rate = lambda)  # 0.252\n#> [1] 0.253\n# Alternativa equivalente:\npexp(5.5, rate = lambda, lower.tail = FALSE)\n#> [1] 0.253\n```\n:::\n\n\n**Visualizzazione:**  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Area per X >= 5.5\nggplot(data.frame(x = x, y = pdf), aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_area(\n    data = subset(data.frame(x, y = pdf), x >= 5.5),\n    aes(x = x, y = y), \n    fill = \"gray\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"Tempo (giorni)\", \n    y = \"Densità\",\n    title = \"Probabilità P(X ≥ 5.5)\"\n  )\n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Simulazione e convergenza alla teoria  \n\nGeneriamo 1,000,000 di osservazioni da $\\text{Exp}(\\lambda = 0.25)$ e confrontiamo l’istogramma con la PDF teorica:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\nsimulated_data <- rexp(1e6, rate = lambda)\n\nggplot(data.frame(x = simulated_data), aes(x = x)) +\n  geom_histogram(\n    aes(y = after_stat(density)), \n    bins = 100, \n    fill = \"skyblue\", \n    color = \"black\",\n    alpha = 0.6\n  ) +\n  geom_line(\n    data = data.frame(x = x, y = pdf),\n    aes(x = x, y = y), \n    color = \"red\", \n    linewidth = 1.2\n  ) +\n  coord_cartesian(xlim = c(0, 20)) +  # Escludiamo code estreme\n  labs(\n    x = \"Tempo di attesa (giorni)\", \n    y = \"Densità\",\n    title = \"Dati simulati e PDF teorica\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nL’istogramma si allinea perfettamente alla curva rossa, dimostrando la **Legge dei Grandi Numeri**.  \n\n### Funzioni R per la distribuzione esponenziale  \n\nR offre quattro funzioni essenziali:  \n\n| Funzione  | Descrizione                           | Esempio d’uso                    | Output Esempio      |  \n|-----------|---------------------------------------|-----------------------------------|---------------------|  \n| `dexp()`  | Calcola la densità $f(x)$             | `dexp(2, rate = 0.25)`           | 0.1516              |  \n| `pexp()`  | Calcola la CDF $P(X \\leq x)$          | `pexp(4, rate = 0.25)`           | 0.632 (≈1 - e⁻¹)    |  \n| `qexp()`  | Trova il quantile $x$ per una probabilità | `qexp(0.5, rate = 0.25)`       | ~2.773 (mediana)    |  \n| `rexp()`  | Genera valori casuali                 | `rexp(5, rate = 0.25)`           | [3.1, 0.8, 5.2, ...] |  \n\n\n## Distribuzione Normale  \n\nLa **distribuzione normale** (o gaussiana) è fondamentale in statistica per modellare fenomeni naturali, sociali e psicologici. La sua importanza deriva dal **Teorema del Limite Centrale**, che garantisce la convergenza alla normalità per somme di variabili casuali indipendenti.  \n\n### La Famiglia delle Distribuzioni Normali  \n\nOgni distribuzione normale è definita da due parametri:  \n\n- **$\\mu$ (media):** centro della distribuzione;  \n- **$\\sigma$ (deviazione standard):** dispersione dei dati attorno alla media.  \n\nLa funzione di densità è:  \n\n$$\nf(y; \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(y - \\mu)^2}{2\\sigma^2}}.\n$$ {#eq-gaussian}\n\n### Distribuzione Normale Standardizzata  \n\nLa **normale standardizzata** è un caso speciale con $\\mu = 0$ e $\\sigma = 1$. Qualsiasi variabile $Y \\sim \\mathcal{N}(\\mu, \\sigma)$ può essere standardizzata tramite:  \n\n$$\nZ = \\frac{Y - \\mu}{\\sigma}.\n$$ {#eq-z-score}\n\nQuesta trasformazione preserva la forma della distribuzione ma riporta i valori in unità di deviazione standard (**Z-score**), permettendo confronti universali.  \n\n#### Relazione tra Deviazione Standard e Distribuzione  \n\n**La regola empirica 68-95-99.7 vale per tutte le distribuzioni normali**, indipendentemente da $\\mu$ e $\\sigma$:  \n\n- **68.3%** dei dati cade entro $\\pm 1\\sigma$ dalla media;  \n- **95.4%** entro $\\pm 2\\sigma$;  \n- **99.7%** entro $\\pm 3\\sigma$.  \n\nPer intervalli specifici legati a test statistici:  \n\n- **$\\pm 1.96\\sigma$** copre il **95%** dei dati (intervallo di confidenza al 95%);\n- **$\\pm 2.576\\sigma$** copre il **99%** (intervallo al 99%).  \n\n\n### Origini storiche e connessione alla binomiale  \n\nAbraham de Moivre osservò che distribuzioni binomiali con $n$ elevato approssimano una normale. Ad esempio:  \n\n- con $n=10$ e $p=0.9$, la distribuzione è asimmetrica; \n- con $n=1000$, la forma diventa simmetrica e campanulare.  \n\n### Simulazione di Passeggiate Casuali  \n\nLa distribuzione normale emerge naturalmente come risultato della **somma di un gran numero di effetti casuali indipendenti**, un principio formalizzato dal **Teorema del Limite Centrale**. Questo la rende ideale per modellare:  \n\n- **errori di misurazione**, dove piccole fluttuazioni casuali (strumentali, ambientali, umane) si combinano;  \n- **fenomeni biologici multifattoriali** come altezza, peso o QI, influenzati da decine di fattori genetici, ambientali e nutrizionali che interagiscono in modo additivo;  \n- **processi sociali** come i punteggi dei test, dove il risultato finale è il prodotto cumulativo di abilità innate, studio, stato emotivo e altro.  \n\n**Simulazione con passeggiate casuali**\n\nPer visualizzare concretamente questo fenomeno, consideriamo una **passeggiata casuale unidimensionale** semplificata:  \n\n1. **Impostazione:**  \n   - 1,000 partecipanti partono dalla posizione 0;  \n   - ogni partecipante compie **16 passi consecutivi**;  \n   - ogni passo è determinato da un generatore casuale che assegna uno spostamento compreso tra -1 e +1 unità (*simulando l'effetto di piccole perturbazioni indipendenti*).  \n\n2. **Dinamica:**  \n   la posizione finale di ciascun partecipante è la **somma algebrica** degli spostamenti casuali. Nonostante ogni passo individuale segua una distribuzione uniforme, la posizione finale aggregata di tutti i partecipanti mostrerà una distribuzione a campana tipica della normale.  \n   \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri\nnumero_passi <- 16\nripetizioni <- 1000\n\n# Generazione di passeggiate casuali\nset.seed(123)\nx <- matrix(0, nrow = numero_passi + 1, ncol = ripetizioni)\n\nfor (i in 1:ripetizioni) {\n  passi <- runif(numero_passi, min = -1, max = 1)\n  x[-1, i] <- cumsum(passi)\n}\n\n# Grafico delle passeggiate casuali\ndf <- data.frame(\n  Passo = rep(0:numero_passi, times = ripetizioni), \n  Distanza = as.vector(x)\n)\n\nggplot(\n  df, \n  aes(\n    x = Passo, \n    y = Distanza, \n    group = rep(1:ripetizioni, each = numero_passi + 1))\n  ) +\n  geom_line(color = \"blue\", alpha = 0.05) +\n  labs(\n    title = \"Passeggiate Casuali\", \n    x = \"Numero di Passi\", y = \"Distanza dall'Origine\"\n  )\n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Codice di simulazione (esempio concettuale)  \nset.seed(123)  \nn_partecipanti <- 1000  \nn_passi <- 16  \n\n# Genera spostamenti casuali (-1 a +1)  \nspostamenti <- matrix(runif(n_partecipanti * n_passi, min = -1, max = 1), ncol = n_passi)  \n\n# Calcola le posizioni finali  \nposizioni_finali <- rowSums(spostamenti)  \n\n# Visualizzazione  \nggplot(data.frame(Posizione = posizioni_finali), aes(x = Posizione)) +  \n  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = \"lightblue\", alpha = 0.7) +  \n  stat_function(fun = dnorm, args = list(mean = mean(posizioni_finali), sd = sd(posizioni_finali)), color = \"red\", linewidth = 1) +  \n  labs(title = \"Distribuzione delle posizioni finali\", x = \"Posizione\", y = \"Densità\")  \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**Risultato atteso:**  \nl'istogramma delle posizioni finali aderirà alla curva rossa (normale teorica), dimostrando come la **combinazione di piccole variazioni casuali** produca una distribuzione gaussiana, anche partendo da passi non-normali. Questo esperimento illustra l’onnipresenza della normale in contesti reali governati da molteplici fattori indipendenti.\n\n**Perché 16 passi?**  \nLa scelta di 16 passi non è arbitraria:  \n\n- un numero ridotto di passi (es. 3-5) produrrebbe una distribuzione ancora vicina all’uniforme; \n- con 16 passi, la **simmetria** e la **curvatura tipica della gaussiana** diventano chiaramente riconoscibili senza richiedere simulazioni massicce.\n\n### Proprietà fondamentali  \n\n- **Media**: $\\mathbb{E}(Y) = \\mu$;  \n- **Varianza**: $\\mathbb{V}(Y) = \\sigma^2$.  \n\n### Funzioni R per la Normale  \n\n| Funzione  | Descrizione                          | Esempio                       |  \n|-----------|--------------------------------------|-------------------------------|  \n| `dnorm()` | Densità a un punto $y$               | `dnorm(115, mean=100, sd=15)` |  \n| `pnorm()` | Probabilità cumulativa $P(Y \\leq y)$ | `pnorm(115, mean=100, sd=15)` |  \n| `qnorm()` | Quantile per una probabilità $p$     | `qnorm(0.975, mean=100, sd=15)` |  \n| `rnorm()` | Genera valori casuali                | `rnorm(10, mean=100, sd=15)`  |  \n\n\n### Visualizzazione delle aree critiche  \n\nLe aree sotto la curva corrispondenti a $\\pm 1\\sigma$, $\\pm 1.96\\sigma$, e $\\pm 3\\sigma$ possono essere visualizzate in R:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Esempio per ±1.96σ (95% di confidenza)  \nmu <- 100  \nsigma <- 15  \nx <- seq(mu - 4*sigma, mu + 4*sigma, length.out=1000)  \ndf <- data.frame(x=x, pdf=dnorm(x, mu, sigma))  \n\nggplot(df, aes(x=x, y=pdf)) +  \n  geom_line(color=\"blue\") +  \n  geom_area(data=subset(df, x >= mu - 1.96*sigma & x <= mu + 1.96*sigma),  \n            fill=\"gray\", alpha=0.5) +  \n  labs(title=\"95% dei dati entro ±1.96σ\", x=\"Valori\", y=\"Densità\")  \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**In sintesi**, la distribuzione normale standardizzata permette di standardizzare qualsiasi fenomeno Gaussiano, rendendo confrontabili dati eterogenei. La relazione tra deviazioni standard e aree sottese è universale: **indipendentemente dalla media e varianza originale, il 68-95-99.7% dei dati cadrà sempre entro 1-2-3σ**. \n\n\n\n::: {.callout-important title=\"Esercizio\" collapse=\"true\"}\nUna psicologa vuole studiare i **livelli di ansia** tra gli studenti universitari durante la settimana degli esami. Dalle ricerche precedenti si sa che nella **popolazione universitaria**:\n\n- il **punteggio medio di ansia** è di **50 punti** su una scala da 0 a 100;\n- la **deviazione standard** dei punteggi di ansia è **10 punti**.\n\nLa psicologa decide di **estrarre un campione casuale di 25 studenti**.\n\nVogliamo usare la **distribuzione campionaria della media** per rispondere a due domande:\n\n1. Qual è la probabilità di ottenere una media campionaria maggiore di 54 punti?\n2. Quale media campionaria rappresenta il **95° percentile** della distribuzione campionaria?\n\n📘 **Concetti chiave.**\n\nLa **distribuzione campionaria della media** ha:\n\n- la **stessa media** della popolazione ($\\mu$),\n- una **deviazione standard più piccola**, detta *errore standard della media* (SE):\n\n$$\nSE = \\frac{\\sigma}{\\sqrt{n}} = \\frac{10}{\\sqrt{25}} = 2 .\n$$\n\nUseremo due funzioni importanti in R:\n\n- `dnorm(x, mean, sd)`: calcola la **densità** della normale in un punto $x$.\n- `qnorm(p, mean, sd)`: calcola il valore di $x$ corrispondente a una certa **probabilità cumulativa** $p$.\n\n\n✅ **Codice base.**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri della popolazione e del campione\nmu <- 50       # media della popolazione\nsigma <- 10    # deviazione standard\nn <- 25        # dimensione campione\n\n# Errore standard della media\nSE <- sigma / sqrt(n)\nSE\n#> [1] 2\n```\n:::\n\n\n🔍 **Domanda 1: Probabilità di ottenere una media > 54.**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Probabilità che la media campionaria sia maggiore di 54\np_oltre_54 <- pnorm(54, mean = mu, sd = SE, lower.tail = FALSE)\np_oltre_54\n#> [1] 0.0228\n```\n:::\n\n\nLa probabilità è molto bassa. Questo vuol dire che, se la vera media della popolazione fosse 50, ottenere una media campionaria superiore a 54 sarebbe raro.\n\n🔍 **Domanda 2: Media al 95° percentile.**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo del valore soglia al 95° percentile\nq_95 <- qnorm(0.95, mean = mu, sd = SE)\nq_95\n#> [1] 53.3\n```\n:::\n\n\nAll'interno della distribuzione campionaria, solo il 5% dei campioni ha una media superiore a questo valore.\n\n📊 Grafico 1: Probabilità di media > 54\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Dati per la distribuzione normale\nx_vals <- seq(44, 56, length.out = 300)\ndens_vals <- dnorm(x_vals, mean = mu, sd = SE)\ndf <- data.frame(x = x_vals, y = dens_vals)\n\n# Grafico\nggplot(df, aes(x, y)) +\n  geom_line(color = \"black\") +\n  geom_area(data = subset(df, x >= 54), aes(x, y), fill = \"red\", alpha = 0.4) +\n  geom_vline(xintercept = 54, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Distribuzione campionaria della media (n = 25)\",\n    subtitle = \"Area rossa = P(media > 54)\",\n    x = \"Media campionaria\",\n    y = \"Densità\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n📊 **Grafico 2: Valore al 95° percentile**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Grafico con il 95° percentile evidenziato\nggplot(df, aes(x, y)) +\n  geom_line(color = \"black\") +\n  geom_area(data = subset(df, x <= q_95), aes(x, y), fill = \"blue\", alpha = 0.4) +\n  geom_vline(xintercept = q_95, color = \"blue\", linetype = \"dashed\") +\n  labs(\n    title = \"Distribuzione campionaria della media (n = 25)\",\n    subtitle = \"Area blu = 95% dei campioni (valore critico ≈ 53.29)\",\n    x = \"Media campionaria\",\n    y = \"Densità\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-23-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**Domande di approfondimento.**\n\n1. Perché l'errore standard della media è più piccolo della deviazione standard della popolazione?\n2. Se la dimensione del campione aumentasse a 100, come cambierebbe l'errore standard?\n3. Che cosa rappresenta `pnorm(54, ...)` nel nostro contesto?\n4. In quali casi, in psicologia, potresti voler calcolare il 95° percentile di una distribuzione campionaria?\n\n**Simulazione Monte Carlo.**\n\nSimuliamo 10.000 campioni casuali, ciascuno di **25 studenti**, estratti da una popolazione normale con media = 50 e deviazione standard = 10. Per ogni campione calcoliamo la media. Alla fine, visualizziamo la distribuzione di queste medie.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)  # per rendere la simulazione replicabile\n\n# Parametri\nmu <- 50\nsigma <- 10\nn <- 25\nn_sim <- 10000  # numero di campioni\n\n# Simulazione: 10.000 medie campionarie\ncampioni <- replicate(n_sim, mean(rnorm(n, mean = mu, sd = sigma)))\n\n# Visualizza le prime 5 medie\nhead(campioni)\n#> [1] 49.7 51.0 50.1 52.8 47.2 47.7\n```\n:::\n\n\n📊 **Istogramma delle medie campionarie.**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n\ndf_sim <- data.frame(media_campionaria = campioni)\n\nggplot(df_sim, aes(x = media_campionaria)) +\n  geom_histogram(aes(y = ..density..), bins = 40, fill = \"lightblue\", color = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma / sqrt(n)),\n                color = \"red\", size = 1) +\n  labs(\n    title = \"Distribuzione delle medie campionarie\",\n    subtitle = \"Istogramma di 10.000 medie di campioni di 25 studenti\",\n    x = \"Media campionaria\",\n    y = \"Densità\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nCosa si osserva?\n\n- Le medie **non sono tutte uguali**, ma si distribuiscono **intorno alla media vera** (50).\n- La forma della distribuzione delle medie è **normale**, anche se i dati originali **non devono necessariamente esserlo** (grazie al Teorema del Limite Centrale).\n- La **deviazione standard** della distribuzione simulata è vicina all'**errore standard teorico**:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Confronto tra errore standard teorico e osservato\nSE_teorico <- sigma / sqrt(n)\nSE_osservato <- sd(campioni)\n\nc(SE_teorico = SE_teorico, SE_osservato = SE_osservato)\n#>   SE_teorico SE_osservato \n#>         2.00         1.97\n```\n:::\n\n\n**Domande di approfondimento.**\n\n1. Perché la forma dell’istogramma è simile a una curva normale?\n2. Cosa succederebbe alla **larghezza della distribuzione** se aumentassimo la dimensione del campione?\n3. Se la media osservata in un esperimento reale fosse **fuori dalla zona centrale**, come potremmo interpretarla?\n:::\n\n::: {.callout-important title=\"Esercizio\" collapse=\"true\"}\nConsideriamo un esercizio in cui si utilizza la **distribuzione normale** e si consultano le **tavole della normale standard** (z) per risolvere il problema dopo la **standardizzazione**.\n\nIn uno studio su un campione di 600 studenti universitari, i punteggi ottenuti a un test di ansia da esame seguono una distribuzione normale con **media** $\\mu = 50$ e **deviazione standard** $\\sigma = 10$.\n\n1. Qual è la **probabilità** che uno studente scelto a caso ottenga un punteggio **inferiore a 65**?\n\n2. Qual è la **percentuale di studenti** che ottengono un punteggio **compreso tra 45 e 60**?\n\n3. Qual è il punteggio minimo che uno studente deve ottenere per rientrare nel **10% superiore** della distribuzione?\n\n**1. Probabilità che $X < 65$.**\n\nStandardizziamo:\n\n$$\nZ = \\frac{X - \\mu}{\\sigma} = \\frac{65 - 50}{10} = \\frac{15}{10} = 1.5 .\n$$\n\nCerchiamo $P(Z < 1.5)$ nella **tavola della normale standard**:\n\n$$\nP(Z < 1.5) \\approx 0.9332 .\n$$\n\n**Risposta**: La probabilità che uno studente ottenga meno di 65 è circa **93.32%**.\n\n**2. Probabilità che $45 < X < 60$.**\n\nCalcoliamo gli z-score:\n\n$$\nZ_1 = \\frac{45 - 50}{10} = -0.5 \\quad ; \\quad Z_2 = \\frac{60 - 50}{10} = 1.0 .\n$$\n\nCerchiamo nelle tavole:\n\n- $P(Z < 1.0) \\approx 0.8413$\n- $P(Z < -0.5) \\approx 0.3085$\n\nQuindi:\n\n$$\nP(45 < X < 60) = P(Z < 1.0) - P(Z < -0.5) = 0.8413 - 0.3085 = 0.5328\n$$\n\n**Risposta**: Circa il **53.28%** degli studenti ha un punteggio tra 45 e 60.\n\n**3. Punteggio minimo per rientrare nel 10% superiore.**\n\nIl 10% superiore corrisponde a:\n\n$$\nP(Z > z) = 0.10 \\Rightarrow P(Z < z) = 0.90 .\n$$\n\nDalla tavola:  \n$P(Z < 1.28) \\approx 0.8997$,  \n$P(Z < 1.29) \\approx 0.9015$\n\nPrendiamo $z = 1.28$\n\nOra risolviamo per $X$:\n\n$$\nX = z \\cdot \\sigma + \\mu = 1.28 \\cdot 10 + 50 = 62.8\n$$\n\n**Risposta**: Il punteggio minimo per rientrare nel 10% superiore è circa **62.8**.\n:::\n\n## Distribuzione Chi-Quadrato\n\nLa distribuzione **$\\chi^2$** deriva dalla distribuzione normale e descrive la somma dei quadrati di $k$ variabili casuali indipendenti e identicamente distribuite (i.i.d.) che seguono la distribuzione normale standard $\\mathcal{N}(0, 1)$. Una variabile casuale $\\chi^2_{~k}$ con $k$ gradi di libertà è definita come:\n\n$$\nZ_1^2 + Z_2^2 + \\dots + Z_k^2,\n$$ {#eq-chisq-def}\n\ndove $Z_1, Z_2, \\dots, Z_k \\sim \\mathcal{N}(0, 1)$. Il parametro $k$, detto **gradi di libertà** ($\\nu$), determina la forma della distribuzione.\n\n### Funzione di densità\n\nLa densità di probabilità della distribuzione $\\chi^2_{~\\nu}$ è data da:\n\n$$\nf(x) = C_{\\nu} x^{\\nu/2 - 1} \\exp(-x/2), \\quad \\text{per } x > 0,\n$$ {#eq-chisq-def2}\n\ndove $C_{\\nu}$ è una costante di normalizzazione. \n\n### Simulazione della Distribuzione Chi-Quadrato\n\nUtilizziamo la definizione per simulare la distribuzione $\\chi^2$ con 3 gradi di libertà.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Impostare il seed per la riproducibilità\nset.seed(1234)\n\n# Generare 1000 valori casuali per 3 variabili gaussiane standard\nn <- 1000\nvar1 <- rnorm(n, mean = 0, sd = 1)\nvar2 <- rnorm(n, mean = 0, sd = 1)\nvar3 <- rnorm(n, mean = 0, sd = 1)\n\n# Calcolare la somma dei quadrati\nchi_sq_values <- var1^2 + var2^2 + var3^2\n\n# Creare un dataframe per il grafico\ndata <- data.frame(chi_sq_values = chi_sq_values)\n\n# Istogramma e densità teorica\nggplot(data, aes(x = chi_sq_values)) +\n  geom_histogram(\n    aes(y = after_stat(density)), \n    bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7\n    ) +\n  stat_function(fun = dchisq, args = list(df = 3), color = \"red\", size = 1) +\n  labs(\n    title = \"Distribuzione Chi-Quadrato (df = 3)\",\n    x = \"Valore\",\n    y = \"Densità\"\n  )\n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n- L'istogramma rappresenta i valori empirici simulati;\n- la curva rossa rappresenta la densità teorica della distribuzione $\\chi^2_{~3}$.\n\n### Media e Varianza Empiriche\n\nCalcoliamo la media e la varianza dei valori simulati:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Media empirica\nmean(chi_sq_values)\n#> [1] 2.98\n\n# Varianza empirica\nvar(chi_sq_values)\n#> [1] 5.97\n```\n:::\n\n\nQuesti valori possono essere confrontati con le proprietà teoriche della distribuzione $\\chi^2$:\n\n- **media**: $\\nu = 3$;\n- **varianza**: $2\\nu = 6$.\n\n### Grafico per Diversi Gradi di Libertà\n\nConfrontiamo le distribuzioni $\\chi^2$ per diversi valori di $\\nu$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Intervallo di x\nx <- seq(0, 40, by = 0.1)\n\n# Gradi di libertà\nnus <- c(2, 4, 8, 16)\n\n# Creare un dataframe\ndata <- do.call(rbind, lapply(nus, function(nu) {\n  data.frame(x = x, f_x = dchisq(x, df = nu), nu = as.factor(nu))\n}))\n\n# Grafico\nggplot(data, aes(x = x, y = f_x, color = nu)) +\n  geom_line(size = 1) +\n  labs(\n    x = \"x\",\n    y = \"f(x)\",\n    color = expression(nu)\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Proprietà della Distribuzione Chi-Quadrato\n\n1. **Asimmetria**: La distribuzione $\\chi^2_{\\nu}$ è asimmetrica, ma diventa più simmetrica al crescere di $\\nu$.\n2. **Media**: $\\mathbb{E}[\\chi^2_{\\nu}] = \\nu$.\n3. **Varianza**: $\\mathbb{V}[\\chi^2_{\\nu}] = 2\\nu$.\n4. **Convergenza**: Per $\\nu \\to \\infty$, $\\chi^2_{\\nu} \\to \\mathcal{N}(\\nu, 2\\nu)$.\n5. **Somma**: La somma di variabili $\\chi^2$ indipendenti con gradi di libertà $\\nu_1, \\nu_2, \\dots, \\nu_k$ segue una distribuzione $\\chi^2$ con $\\nu = \\sum_{i=1}^k \\nu_i$.\n\n### Applicazioni\n\nLa distribuzione $\\chi^2$ è utilizzata in molteplici ambiti statistici, tra cui:\n\n- **test di indipendenza**: per verificare se due variabili categoriche sono indipendenti;\n- **test di adattamento**: per confrontare una distribuzione empirica con una teorica.\n\n## Distribuzione $t$ di Student\n\nLa **distribuzione $t$ di Student** è una delle distribuzioni fondamentali della statistica inferenziale. Deriva dalle distribuzioni Normale e Chi-quadrato ed è particolarmente utile per analizzare campioni di piccole dimensioni o situazioni in cui la varianza della popolazione è sconosciuta.\n\n### Definizione Formale\n\nSe:\n\n- $Z \\sim \\mathcal{N}(0, 1)$ (distribuzione Normale standard),\n- $W \\sim \\chi^2_{\\nu}$ (distribuzione Chi-quadrato con $\\nu$ gradi di libertà),\n\ne $Z$ e $W$ sono indipendenti, allora la variabile casuale\n\n$$\nT = \\frac{Z}{\\sqrt{\\frac{W}{\\nu}}}\n$$ {#eq-t-def}\n\nsegue una **distribuzione $t$ di Student** con $\\nu$ gradi di libertà. Si indica come $T \\sim t_{\\nu}$.\n\n### Proprietà della Distribuzione $t$ di Student\n\n1. **Forma della distribuzione**:\n\n   - la distribuzione $t$ è simmetrica rispetto a zero, come la Normale standard ($\\mathcal{N}(0, 1)$);\n   - presenta **code più pesanti** rispetto alla Normale, riflettendo una maggiore probabilità di osservare valori estremi.\n\n2. **Code pesanti e gradi di libertà**:\n\n   - la pesantezza delle code diminuisce con l'aumentare dei gradi di libertà ($\\nu$);\n   - per $\\nu \\to \\infty$, la distribuzione $t$ converge alla distribuzione Normale standard.\n\n3. **Media e varianza**:\n\n   - la **media** è $0$ per $\\nu > 1$;\n   - la **varianza** è:\n   \n     $$\n     \\text{Var}(T) = \\frac{\\nu}{\\nu - 2}, \\quad \\text{per } \\nu > 2.\n     $$\n     \n     Per $\\nu \\leq 2$, la varianza non è definita.\n\n4. **Applicazioni principali**:\n\n   - **test t di Student**: Confronto delle medie di due gruppi o test per una singola media;\n   - **intervalli di confidenza**: Stima dell'intervallo per la media quando la varianza è sconosciuta.\n\n### Differenze tra la Distribuzione $t$ e la Normale\n\n| **Caratteristica**            | **Distribuzione Normale**     | **Distribuzione $t$ di Student**  |\n|-------------------------------|-------------------------------|--------------------------------------|\n| Forma                         | Simmetrica, a campana         | Simmetrica, a campana               |\n| Code                          | Sottili                       | Pesanti                             |\n| Dipendenza dai gradi di libertà | No                            | Sì                                  |\n| Convergenza                   | Non varia                     | Con $\\nu \\to \\infty$, converge alla Normale |\n\n### Visualizzazione della Distribuzione $t$\n\nConfrontiamo graficamente la distribuzione $t$ con diversi gradi di libertà e la distribuzione Normale standard:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Creazione dei dati\nx <- seq(-4, 4, length.out = 1000)\ndf <- c(1, 2, 5, 10)  # Gradi di libertà\n\n# Dataframe con curve di densità\ndata <- data.frame(\n  x = rep(x, length(df) + 1),\n  density = c(\n    dnorm(x),\n    dt(x, df[1]),\n    dt(x, df[2]),\n    dt(x, df[3]),\n    dt(x, df[4])\n  ),\n  distribution = rep(c(\"Normale\", paste(\"t (df =\", df, \")\")), each = length(x))\n)\n\n# Plot\nggplot(data, aes(x = x, y = density, color = distribution)) +\n  geom_line(size = 1) +\n  labs(\n    x = \"Valore\",\n    y = \"Densità\",\n    color = \"Distribuzione\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-30-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Simulazione della Distribuzione $t$\n\nSimuliamo una distribuzione $t$ con 10 gradi di libertà e confrontiamola con la densità teorica.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Impostare il seed per la riproducibilità\nset.seed(123)\n\n# Simulare 1000 valori da una distribuzione t\nn <- 1000\ndf <- 10  # Gradi di libertà\nt_values <- rt(n, df = df)\n\n# Creare un dataframe per il grafico\ndata <- data.frame(t_values = t_values)\n\n# Istogramma con densità teorica\nggplot(data, aes(x = t_values)) +\n  geom_histogram(\n    aes(y = after_stat(density)), \n    bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7\n  ) +\n  stat_function(fun = dt, args = list(df = df), color = \"red\", size = 1) +\n  labs(\n    x = \"Valore\",\n    y = \"Densità\"\n  ) \n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-31-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Proprietà Teoriche della Distribuzione $t$\n\n1. **Media**:\n   $$\n   \\mathbb{E}[T] = 0, \\quad \\text{per } \\nu > 1.\n   $$\n\n2. **Varianza**:\n   $$\n   \\mathbb{V}[T] = \\frac{\\nu}{\\nu - 2}, \\quad \\text{per } \\nu > 2.\n   $$\n\n3. **Simmetria**:\n   - la distribuzione è simmetrica rispetto a zero, come la Normale.\n\n4. **Code**:\n   - le code sono più pesanti rispetto alla Normale, riflettendo una maggiore incertezza per piccoli campioni.\n\nIn conclusione, la distribuzione $t$ di Student è uno strumento versatile nell'inferenza statistica, trovando applicazione in contesti sia frequentisti che bayesiani. È particolarmente utile in situazioni in cui la conoscenza della varianza è limitata o i campioni sono di dimensioni ridotte. Grazie alla sua forma simmetrica e alle code più pesanti rispetto alla distribuzione Normale, la distribuzione $t$ può modellare meglio l'incertezza, includendo una maggiore probabilità per valori estremi.\n\nNel contesto bayesiano, la distribuzione $t$ viene utilizzata come:\n\n- **prior informativo robusto**, per modellare parametri con valori plausibili lontani dalla media ma con una penalizzazione graduale per valori estremi.\n- **distribuzione predittiva** per sintetizzare l'incertezza derivante da campioni piccoli o con variabilità elevata.\n\nIn entrambi i paradigmi, la distribuzione $t$ rappresenta una scelta robusta, capace di riflettere in modo flessibile la natura dei dati. Inoltre, per valori elevati dei gradi di libertà, la distribuzione $t$ converge alla distribuzione Normale, un caso limite che ne estende ulteriormente l’utilità in vari contesti analitici.\n\n## Funzione Beta di Eulero\n\nLa **funzione Beta di Eulero** è una funzione matematica, non una densità di probabilità, ma è strettamente collegata alla distribuzione Beta, poiché appare nella sua definizione. Indicata comunemente con il simbolo $\\mathcal{B}(\\alpha, \\beta)$, la funzione Beta può essere espressa in vari modi. Per i nostri scopi, utilizziamo la seguente definizione:\n\n$$\n\\mathcal{B}(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\,,\n$$ {#eq-eulero-beta-def}\n\ndove $\\Gamma(x)$ rappresenta la funzione Gamma, una generalizzazione del fattoriale definita per numeri reali positivi. Quando $x$ è un numero intero, la funzione Gamma si riduce al fattoriale traslato:\n\n$$\n\\Gamma(x) = (x-1)!.\n$$\n\n::: {#exm-}\nSupponiamo di voler calcolare $\\mathcal{B}(3, 9)$. Utilizzando la definizione, abbiamo:\n\n$$\n\\mathcal{B}(3, 9) = \\frac{\\Gamma(3) \\cdot \\Gamma(9)}{\\Gamma(3 + 9)}.\n$$\n\nIn R, possiamo calcolarla in tre modi diversi.\n\n1. Utilizzando la definizione con la funzione `gamma()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nalpha <- 3\nbeta <- 9\n\nbeta_function <- gamma(alpha) * gamma(beta) / gamma(alpha + beta)\nbeta_function\n#> [1] 0.00202\n```\n:::\n\n\n2. Utilizzando direttamente la funzione `beta()` di R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbeta(alpha, beta)\n#> [1] 0.00202\n```\n:::\n\n\n3. Calcolo manuale con fattoriali:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(factorial(alpha - 1) * factorial(beta - 1)) / factorial(alpha + beta - 1)\n#> [1] 0.00202\n```\n:::\n\n\nTutti e tre i metodi restituiscono lo stesso risultato, confermando la correttezza della definizione.\n:::\n\nLa funzione Beta è utilizzata nella definizione della **densità di probabilità Beta**. Essa serve a normalizzare la densità, garantendo che l'area sotto la curva sia pari a $1$. \n\n## Distribuzione Beta\n\nLa **distribuzione Beta**, indicata come $\\mathcal{Beta}(\\alpha, \\beta)$, è una distribuzione di probabilità continua definita sull’intervallo $(0, 1)$. È particolarmente utile per modellare proporzioni, probabilità, o in generale qualsiasi fenomeno che assume valori compresi tra 0 e 1.\n\nQuesta distribuzione è molto flessibile: a seconda dei valori dei parametri $\\alpha$ e $\\beta$, può assumere forme simmetriche, asimmetriche, concave, convesse, ecc. È frequentemente utilizzata come distribuzione a priori nei modelli bayesiani per parametri che rappresentano probabilità.\n\n### Definizione\n\n:::: {.definition #def-beta}\nSia $\\theta$ una variabile casuale continua. Se $\\theta$ segue una distribuzione Beta con parametri $\\alpha > 0$ e $\\beta > 0$, scriviamo:\n\n$$\n\\theta \\sim \\mathcal{Beta}(\\alpha, \\beta),\n$$\n\ne la sua **funzione di densità di probabilità** (pdf) è data da:\n\n$$\n\\mathcal{Beta}(\\theta \\mid \\alpha, \\beta) = \\frac{1}{\\mathcal{B}(\\alpha, \\beta)} \\, \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}, \\quad \\text{per } \\theta \\in (0, 1),\n$$ {#eq-beta-distr-def}\n\ndove $\\mathcal{B}(\\alpha, \\beta)$ è la funzione Beta (o funzione beta di Eulero).\n::::\n\n### Rappresentazione alternativa\n\nUn’espressione equivalente della densità, che mette in evidenza il legame con la funzione Gamma, è:\n\n$$\n\\mathcal{Beta}(\\theta \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\, \\Gamma(\\beta)} \\, \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}, \\quad \\text{per } \\theta \\in (0, 1),\n$$ {#eq-beta-distr-def2}\n\ndove $\\Gamma(\\cdot)$ è la funzione Gamma, che generalizza il fattoriale: $\\Gamma(n) = (n - 1)!$ per ogni intero positivo $n$.\n\n\n### Ruolo dei Parametri $\\alpha$ e $\\beta$\n\nI parametri $\\alpha$ e $\\beta$ determinano la forma della distribuzione:\n\n- **$\\alpha > 1$**: favorisce valori di $\\theta$ vicini a 1.\n- **$\\beta > 1$**: favorisce valori di $\\theta$ vicini a 0.\n- **$\\alpha = \\beta = 1$**: corrisponde alla distribuzione uniforme sull'intervallo $[0, 1]$.\n- **$\\alpha, \\beta < 1$**: la distribuzione è bimodale, concentrandosi agli estremi (vicino a 0 e 1).\n\n### Proprietà della Distribuzione Beta\n\n1. **Valore atteso**:\n   $$\n   \\mathbb{E}(\\theta) = \\frac{\\alpha}{\\alpha + \\beta}.\n   $$\n\n2. **Varianza**:\n   $$\n   \\mathbb{V}(\\theta) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}.\n   $$\n\n3. **Moda** (se $\\alpha, \\beta > 1$):\n   $$\n   \\text{Moda}(\\theta) = \\frac{\\alpha - 1}{\\alpha + \\beta - 2}.\n   $$\n\nQueste proprietà evidenziano come $\\alpha$ e $\\beta$ possano essere interpretati come \"successi\" e \"fallimenti\" in una serie di prove, fornendo un collegamento intuitivo con la distribuzione binomiale.\n\n### Relazione con la Distribuzione Binomiale\n\nLa **distribuzione Beta** può essere interpretata come una generalizzazione continua della distribuzione binomiale. Mentre la distribuzione binomiale descrive la **probabilità di osservare un certo numero di successi** in un numero fissato di prove ($n$), la distribuzione Beta descrive l’**incertezza sulla probabilità di successo** $\\theta$ stessa, trattandola come una variabile casuale.\n\n#### Contesto bayesiano\n\nIn un contesto di **inferenza bayesiana**, la distribuzione Beta viene comunemente utilizzata come **distribuzione a priori coniugata** per il modello binomiale. Ciò significa che, se si assume una distribuzione Beta come prior per $\\theta$, anche la distribuzione a posteriori (dopo aver osservato i dati) sarà una Beta, ma con parametri aggiornati.\n\nSupponiamo:\n\n- che $\\theta$ sia la probabilità di successo in un compito con esiti binari (es. risposta corretta o errata),\n- di assumere una distribuzione a priori:  \n  $$\n  \\theta \\sim \\mathcal{Beta}(\\alpha, \\beta),\n  $$\n- e di osservare $y$ successi su $n$ prove, con modello di verosimiglianza:\n  $$\n  y \\sim \\mathcal{Binom}(n, \\theta).\n  $$\n\nAllora, per il teorema di Bayes, la distribuzione a posteriori di $\\theta$ sarà:\n\n$$\n\\theta \\mid \\text{dati} \\sim \\mathcal{Beta}(\\alpha + y, \\beta + n - y).\n$$\n\n#### Vantaggi della coniugatezza\n\nQuesto aggiornamento è particolarmente comodo perché:\n\n- si ottiene in forma chiusa (senza dover ricorrere a metodi numerici),\n- i parametri $\\alpha$ e $\\beta$ possono essere interpretati come **conteggi fittizi di successi e insuccessi** prima dell’osservazione dei dati,\n- l’informazione a priori e quella empirica si combinano sommando i rispettivi “conteggi.”\n\nQuesta proprietà rende la distribuzione Beta una scelta naturale nei modelli bayesiani con dati binomiali, come in contesti psicologici in cui si vogliono modellare incertezze sulla probabilità di una risposta corretta, sull’esito di una scelta, o sul successo di un comportamento.\n\n### Visualizzazione della Distribuzione Beta\n\nDi seguito mostriamo come la forma della distribuzione Beta varia al variare dei parametri $\\alpha$ e $\\beta$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri\nx <- seq(0, 1, length.out = 200)\nalphas <- c(0.5, 5.0, 1.0, 2.0, 2.0)\nbetas <- c(0.5, 1.0, 3.0, 2.0, 5.0)\n\n# Creare un dataframe\ndf <- do.call(rbind, lapply(1:length(alphas), function(i) {\n  data.frame(\n    x = x,\n    density = dbeta(x, alphas[i], betas[i]),\n    label = paste0(\"α = \", alphas[i], \", β = \", betas[i])\n  )\n}))\n\n# Plot\nggplot(df, aes(x = x, y = density, color = label)) +\n  geom_line(size = 1) +\n  labs(\n    x = \"x\",\n    y = \"f(x)\"\n  ) +\n  theme(legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-35-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Costante di Normalizzazione\n\nLa costante di normalizzazione della distribuzione Beta è il reciproco della funzione Beta di Eulero, $B(\\alpha, \\beta)$. Questa garantisce che:\n\n$$\n\\int_0^1 \\mathcal{Beta}(\\theta \\mid \\alpha, \\beta) \\, d\\theta = 1.\n$$\n\n::: {#exm-}\nDi seguito viene proposto un esempio in R per calcolare l'area sottesa alla distribuzione Beta non normalizzata e, con gli stessi parametri, ottenere il valore della funzione Beta di Eulero. L'obiettivo è mostrare come la costante di normalizzazione, pari al reciproco di $B(\\alpha, \\beta)$, garantisca che l'integrale della densità normalizzata su $[0,1]$ sia pari a 1.\n\nSupponiamo di voler utilizzare i parametri:\n\n- $\\alpha = 2$\n- $\\beta = 5$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri della distribuzione Beta\nalpha <- 2\nbeta  <- 5\n\n# Definiamo la funzione non normalizzata della distribuzione Beta\nunnormalized_beta <- function(theta) {\n  theta^(alpha - 1) * (1 - theta)^(beta - 1)\n}\n\n# Calcoliamo l'integrale della funzione non normalizzata su [0, 1]\nintegrale <- integrate(unnormalized_beta, lower = 0, upper = 1)$value\ncat(\"Integrale della funzione non normalizzata:\", integrale, \"\\n\")\n#> Integrale della funzione non normalizzata: 0.0333\n\n# Calcoliamo il valore della funzione Beta usando la funzione beta() di R\nvalore_beta <- beta(alpha, beta)\ncat(\"Valore della funzione Beta B(alpha, beta):\", valore_beta, \"\\n\")\n#> Valore della funzione Beta B(alpha, beta): 0.0333\n```\n:::\n\n\nSpiegazione del Codice\n\n1. **Definizione dei Parametri e della Funzione**  \n   Impostiamo $\\alpha = 2$ e $\\beta = 5$ e definiamo la funzione non normalizzata:\n   $$\n   f(\\theta) = \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}.\n   $$\n\n2. **Calcolo dell'Integrale**  \n   Utilizzando la funzione `integrate()`, calcoliamo l'area sottesa a $f(\\theta)$ nell'intervallo $[0,1]$, che corrisponde a $\\mathcal{B}(\\alpha, \\beta)$.\n\n3. **Verifica con la Funzione Beta**  \n   La funzione `beta(alpha, beta)` di R restituisce direttamente il valore di $\\mathcal{B}(\\alpha, \\beta)$. La stampa dei due valori conferma che l'integrale calcolato e il valore della funzione Beta coincidono.\n\n4. **Costante di Normalizzazione**  \n   Il reciproco di $\\mathcal{B}(\\alpha, \\beta)$ è calcolato e utilizzato per definire la densità normalizzata della distribuzione Beta. L'integrazione della densità normalizzata su $[0,1]$ restituisce 1, confermando la corretta normalizzazione.\n\nQuesto esempio in R mostra in modo pratico come la costante di normalizzazione derivi dalla funzione Beta di Eulero e come essa venga applicata per ottenere una densità di probabilità correttamente normalizzata.\n:::\n\nIn conclusione, la distribuzione Beta si rivela particolarmente utile per modellare variabili continue comprese nell'intervallo [0, 1]. Grazie alla sua parametrizzazione tramite $\\alpha$ e $\\beta$, consente di adattare la forma della densità in modo specifico alle caratteristiche osservate dei dati, facilitando la stima di proporzioni. Inoltre, essendo il coniugato della distribuzione binomiale, permette un aggiornamento analitico nei modelli bayesiani, semplificando l'inferenza quando si raccolgono dati incrementali, come nella stima della probabilità di successo in esperimenti o studi psicologici.\n\n## Distribuzione di Cauchy\n\nLa **distribuzione di Cauchy** è un caso speciale della distribuzione $t$ di Student con un solo grado di libertà ($t_1$). Questa distribuzione è caratterizzata da code molto pesanti e da una media e varianza non definite, rendendola particolarmente utile in contesti dove valori estremi possono avere un'influenza importante.\n\n::: {#def-}\nLa funzione di densità di probabilità della distribuzione di Cauchy è definita da due parametri:\n\n- **$\\alpha$**: posizione (location), che determina il centro della distribuzione.\n- **$\\beta > 0$**: scala (scale), che controlla la larghezza della distribuzione.\n\nLa densità è data da:\n\n$$\nf(x \\mid \\alpha, \\beta) = \\frac{1}{\\pi \\beta \\left[1 + \\left( \\frac{x - \\alpha}{\\beta} \\right)^2\\right]} ,\n$$\n\ndove:\n\n- $x \\in \\mathbb{R}$,\n- $\\alpha \\in \\mathbb{R}$,\n- $\\beta > 0$.\n\nQuesta funzione descrive una distribuzione simmetrica attorno a $\\alpha$, con code più pesanti rispetto alla distribuzione Normale.\n:::\n\n### Proprietà della Distribuzione di Cauchy\n\n1. **Simmetria**: La distribuzione è simmetrica rispetto a $\\alpha$.\n2. **Code Pesanti**: Le code sono significativamente più pesanti rispetto alla distribuzione Normale, con una decrescita più lenta ($\\propto x^{-2}$).\n3. **Media e Varianza**: La distribuzione non ha una media né una varianza definita.\n4. **Relazione con $t_1$**: La distribuzione di Cauchy è equivalente a una distribuzione $t$ di Student con 1 grado di libertà.\n5. **Caratteristiche Estreme**: I valori estremi hanno una probabilità più alta rispetto ad altre distribuzioni comuni, rendendola utile per modellare fenomeni con outlier significativi.\n\n### Visualizzazione della Distribuzione di Cauchy\n\nPer comprendere l'effetto dei parametri $\\alpha$ e $\\beta$ sulla forma della distribuzione, consideriamo alcuni esempi con:\n\n- $\\alpha = 0.0, 0.0, 0.0, -2.0$,\n- $\\beta = 0.5, 1.0, 2.0, 1.0$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Definire i parametri\nx <- seq(-5, 5, length.out = 500)\nalphas <- c(0.0, 0.0, 0.0, -2.0)\nbetas <- c(0.5, 1.0, 2.0, 1.0)\n\n# Creare un data frame per i risultati\ndf <- do.call(rbind, lapply(1:length(alphas), function(i) {\n  data.frame(\n    x = x,\n    density = dcauchy(x, location = alphas[i], scale = betas[i]),\n    label = paste0(\"α = \", alphas[i], \", β = \", betas[i])\n  )\n}))\n\n# Grafico\nggplot(df, aes(x = x, y = density, color = label)) +\n  geom_line(size = 1) +\n  labs(\n    x = \"x\",\n    y = \"f(x)\"\n  ) +\n  theme(\n    legend.title = element_blank()\n  )\n```\n\n::: {.cell-output-display}\n![](14_cont_rv_distr_files/figure-html/unnamed-chunk-37-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Applicazioni della Distribuzione di Cauchy\n\n1. **Inferenza Bayesiana**:\n\n   - Utilizzata come prior **robusto** in modelli bayesiani, particolarmente quando si vuole attribuire una probabilità maggiore a valori estremi rispetto a una distribuzione Normale.\n\n2. **Modellazione di Fenomeni con Outlier**:\n\n   - La distribuzione di Cauchy è adatta per descrivere dati con valori estremi significativi che possono influenzare fortemente altre distribuzioni.\n\nIn conclusione, la distribuzione di Cauchy, con le sue proprietà uniche come code pesanti e l'assenza di media e varianza definite, è uno strumento fondamentale per modellare fenomeni in cui i valori estremi giocano un ruolo importante. La sua relazione con la distribuzione $t$ di Student e la sua utilità nei modelli bayesiani ne ampliano ulteriormente le applicazioni in contesti statistici e probabilistici avanzati.\n\n## Distribuzione Gamma\n\nLa **distribuzione Gamma** è una distribuzione di probabilità continua utilizzata principalmente per modellare variabili strettamente positive, come tassi, varianze, o tempi di attesa. È usata nella statistica bayesiana come distribuzione a priori per parametri positivi e trova applicazione in ambiti come la modellazione di eventi rari.\n\n::: {#def-}\nLa distribuzione Gamma è caratterizzata da due parametri principali:\n\n- **Parametro di forma** ($\\alpha$): determina la forma generale della distribuzione.\n- **Parametro di scala** ($\\theta$) o, alternativamente, il **parametro di tasso** ($\\beta = 1/\\theta$): regola la larghezza o la dispersione della distribuzione.\n\nLa funzione di densità di probabilità (PDF) è data da:\n\n$$\nf(x \\mid \\alpha, \\theta) = \\frac{x^{\\alpha-1} e^{-x/\\theta}}{\\theta^\\alpha \\Gamma(\\alpha)}, \\quad x > 0,\n$$\n\ndove:\n\n- $x$ è la variabile casuale continua,\n- $\\Gamma(\\alpha)$ è la funzione Gamma di Eulero, definita come:\n\n$$\n\\Gamma(\\alpha) = \\int_0^\\infty t^{\\alpha-1} e^{-t} dt.\n$$\n\nSe utilizziamo il parametro di tasso $\\beta = 1/\\theta$, la PDF può essere scritta come:\n\n$$\nf(x \\mid \\alpha, \\beta) = \\frac{\\beta^\\alpha x^{\\alpha-1} e^{-\\beta x}}{\\Gamma(\\alpha)}, \\quad x > 0.\n$$\n:::\n\n### Proprietà della Distribuzione Gamma\n\n1. **Media**:\n   $$\n   \\mathbb{E}[X] = \\alpha \\cdot \\theta = \\frac{\\alpha}{\\beta}.\n   $$\n\n2. **Varianza**:\n   $$\n   \\text{Var}(X) = \\alpha \\cdot \\theta^2 = \\frac{\\alpha}{\\beta^2}.\n   $$\n\n3. **Moda** (per $\\alpha > 1$):\n   $$\n   \\text{Moda}(X) = (\\alpha - 1) \\cdot \\theta.\n   $$\n\n\nDi seguito, mostriamo un esempio per $\\alpha = 3$ e $\\beta = 5/3$, calcolando e rappresentando graficamente la distribuzione.\n\n1. **Calcolo della Media e della Deviazione Standard**:\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   # Parametri\n   alpha <- 3\n   beta <- 5 / 3\n   \n   # Calcolo\n   mean <- alpha / beta\n   sigma <- sqrt(alpha / beta^2)\n   \n   cat(\"Media:\", mean, \"\\n\")\n   #> Media: 1.8\n   cat(\"Deviazione Standard:\", sigma, \"\\n\")\n   #> Deviazione Standard: 1.04\n   ```\n   :::\n\n\n2. **Generazione e Plot dei Dati**:\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   # Generazione di dati\n   set.seed(123)\n   data <- rgamma(100000, shape = alpha, rate = beta)\n   \n   # Data frame per ggplot\n   df <- data.frame(values = data)\n   \n   # Plot\n   ggplot(df, aes(x = values)) +\n     geom_histogram(aes(y = ..density..), bins = 30, fill = \"green\", alpha = 0.6) +\n     stat_function(fun = function(x) dgamma(x, shape = alpha, rate = beta),\n                   color = \"red\", size = 1) +\n     labs(\n       x = \"Valore\",\n       y = \"Densità di probabilità\"\n     ) \n   ```\n   \n   ::: {.cell-output-display}\n   ![](14_cont_rv_distr_files/figure-html/unnamed-chunk-39-1.png){fig-align='center' width=85%}\n   :::\n   :::\n\n\n### Applicazioni della Distribuzione Gamma\n\n1. **Modellazione del Tempo di Attesa**:  \n   La distribuzione Gamma è ideale per modellare tempi di attesa, ad esempio, il tempo necessario affinché si verifichino $n$ eventi in un processo di Poisson.\n\n2. **Inferenza Bayesiana**:  \n   - Utilizzata come prior per parametri positivi, come tassi ($\\lambda$) o varianze ($\\sigma^2$).\n   - Ad esempio, nella modellazione bayesiana dei processi di Poisson, una distribuzione Gamma è una scelta naturale per il prior su $\\lambda$.\n\n\n## Riflessioni conclusive\n\nLe distribuzioni di probabilità costituiscono il cuore dell'inferenza statistica, sia bayesiana che frequentista. In questo capitolo, abbiamo esplorato come R offre un insieme completo di strumenti per lavorare con diverse distribuzioni, permettendo di modellare e analizzare una vasta gamma di fenomeni.\n\n### Principali Applicazioni\n\n1. **Inferenza Bayesiana**:  \n   Le distribuzioni di probabilità, come la Beta, la Gamma e la Normale, sono essenziali per definire priors, calcolare posteriori e quantificare l'incertezza nei modelli bayesiani. Ad esempio:\n   - La distribuzione Beta è ideale per modellare credenze a priori su proporzioni o probabilità.\n   - La distribuzione Gamma è ampiamente usata per modellare parametri positivi come tassi o varianze.\n\n2. **Analisi Statistica e Modellazione**:  \n   Le distribuzioni, come la $t$ di Student, sono fondamentali per il confronto tra campioni, mentre la Normale è indispensabile per modellare fenomeni che seguono la legge del limite centrale.\n\n3. **Generazione e Simulazione di Dati**:  \n   R permette di generare campioni casuali da distribuzioni comuni, utili per simulazioni, bootstrap e validazione di modelli.\n\n### Funzionalità di R\n\nCon poche funzioni, R consente di:\n\n- **Generare campioni casuali**: con funzioni come `rnorm`, `rgamma`, `rbeta`, possiamo simulare dati da distribuzioni specifiche.\n- **Calcolare densità**: ad esempio, con `dnorm`, `dgamma`, `dbeta`, possiamo visualizzare le funzioni di densità.\n- **Calcolare probabilità cumulate**: con funzioni come `pnorm`, `pbeta`, possiamo determinare probabilità su intervalli specifici.\n- **Determinare quantili**: con funzioni come `qnorm`, `qgamma`, possiamo calcolare i punti corrispondenti a specifici livelli di probabilità.\n\n### Versatilità delle Distribuzioni\n\nLe distribuzioni esplorate non solo descrivono fenomeni naturali, ma sono anche i \"mattoncini\" per costruire modelli statistici complessi. Le loro proprietà, come la media, la varianza, la simmetria o le code pesanti, consentono di adattare il modello al fenomeno studiato.\n\nIn conclusione, il linguaggio R, con la sua flessibilità e ricchezza di strumenti, permette di padroneggiare le distribuzioni di probabilità, non solo come oggetti matematici, ma anche come strumenti pratici per rispondere a domande complesse. La comprensione e l'uso delle distribuzioni presentate costituiscono le fondamenta per avanzare verso tecniche più sofisticate, come l'inferenza bayesiana avanzata o la modellazione gerarchica. \n\n## Esercizi {.unnumbered}\n\n::: {.callout-important title=\"Problemi\" collapse=\"true\"}\nEsercizi sulla distribuzione normale, risolvibili usando R, sono disponibili sulla seguente [pagina web](https://mathcenter.oxford.emory.edu/site/math117/probSetNormalDistribution/).\n:::\n\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#>  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#>  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#> [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#> [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#> [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#> [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#> [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#> [25] here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#>  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#>  [7] digest_0.6.37         timechange_0.3.0      estimability_1.5.1   \n#> [10] lifecycle_1.0.4       survival_3.8-3        magrittr_2.0.3       \n#> [13] compiler_4.5.1        rlang_1.1.6           tools_4.5.1          \n#> [16] knitr_1.50            labeling_0.4.3        bridgesampling_1.1-2 \n#> [19] htmlwidgets_1.6.4     curl_7.0.0            pkgbuild_1.4.8       \n#> [22] RColorBrewer_1.1-3    abind_1.4-8           multcomp_1.4-28      \n#> [25] withr_3.0.2           purrr_1.1.0           grid_4.5.1           \n#> [28] stats4_4.5.1          colorspace_2.1-1      xtable_1.8-4         \n#> [31] inline_0.3.21         emmeans_1.11.2-8      scales_1.4.0         \n#> [34] MASS_7.3-65           cli_3.6.5             mvtnorm_1.3-3        \n#> [37] rmarkdown_2.29        ragg_1.5.0            generics_0.1.4       \n#> [40] RcppParallel_5.1.11-1 cachem_1.1.0          stringr_1.5.1        \n#> [43] splines_4.5.1         parallel_4.5.1        vctrs_0.6.5          \n#> [46] V8_7.0.0              Matrix_1.7-4          sandwich_3.1-1       \n#> [49] jsonlite_2.0.0        arrayhelpers_1.1-0    systemfonts_1.2.3    \n#> [52] glue_1.8.0            codetools_0.2-20      distributional_0.5.0 \n#> [55] lubridate_1.9.4       stringi_1.8.7         gtable_0.3.6         \n#> [58] QuickJSR_1.8.0        htmltools_0.5.8.1     Brobdingnag_1.2-9    \n#> [61] R6_2.6.1              textshaping_1.0.3     rprojroot_2.1.1      \n#> [64] evaluate_1.0.5        lattice_0.22-7        backports_1.5.0      \n#> [67] memoise_2.0.1         broom_1.0.9           snakecase_0.11.1     \n#> [70] rstantools_2.5.0      coda_0.19-4.1         gridExtra_2.3        \n#> [73] nlme_3.1-168          checkmate_2.3.3       xfun_0.53            \n#> [76] zoo_1.8-14            pkgconfig_2.0.3\n```\n:::\n\n\n## Bibliografia {.unnumbered}\n",
    "supporting": [
      "14_cont_rv_distr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}