{
  "hash": "65ccbd6e66c2ab107c11d18b7252036c",
  "result": {
    "engine": "knitr",
    "markdown": "# Covarianza e correlazione {#sec-prob-cov-cor}\n\n\n## Introduzione {.unnumbered .unlisted}\n\nQuando due variabili casuali non sono indipendenti, diciamo che esse sono *associate* o *dipendenti*. È importante non solo stabilire se tale relazione esista, ma anche quantificare la sua intensità e la sua direzione. A tal fine, utilizziamo due misure chiave: la *covarianza* e la *correlazione*.\n\n### Panoramica del capitolo {.unnumbered .unlisted}\n\n- Definire e calcolare la covarianza per quantificare la relazione lineare tra due variabili casuali.\n- Utilizzare la correlazione per misurare l'intensità della relazione lineare tra variabili casuali, indipendentemente dalle loro unità di misura.\n- Comprendere le proprietà chiave della covarianza e della correlazione, inclusa l'incorrelazione.\n- Estendere i concetti di probabilità congiunta, marginale e condizionale alle variabili continue, utilizzando gli integrali.\n\n::: {.callout-tip collapse=true}\n## Prerequisiti\n- Leggere il capitolo *Joint Distributions* [@kroese2025statistical].\n- Leggere il capitolo *Joint Distributions* [@blitzstein2019introduction].\n:::\n\n::: {.callout-caution collapse=true title=\"Preparazione del Notebook\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |>\n  source()\n```\n:::\n\n:::\n\n\n## Covarianza\n\nLa covarianza misura il grado e la direzione della relazione lineare tra due variabili casuali. Una covarianza positiva indica che le due variabili tendono ad aumentare o diminuire insieme, mentre una covarianza negativa indica che una variabile tende ad aumentare quando l'altra diminuisce.\n\n### Definizione di Covarianza\n\nLa covarianza tra due variabili casuali discrete $X$ e $Y$ è definita come:\n\n$$\n\\text{Cov}(X, Y) = \\mathbb{E}\\left[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])\\right] .\n$$\n\nEsplicitamente, questa definizione può essere riscritta come:\n\n$$\n\\text{Cov}(X, Y) = \\sum_{x}\\sum_{y}(x - \\mu_X)(y - \\mu_Y)p(x, y) .\n$$\n\ndove $\\mu_X$ e $\\mu_Y$ sono le medie delle variabili $X$ e $Y$ e $p(x,y)$ è la funzione di massa di probabilità congiunta.\n\n\n\nQuesta definizione mostra una stretta analogia con la varianza, che è la covarianza di una variabile con se stessa:\n\n$$\n\\mathbb{V}(X) = Cov(X, X).\n$$\n\nInoltre, la covarianza può essere calcolata attraverso la relazione:\n\n$$\nCov(X, Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y).\n$$\n\n### Dimostrazione\n\nLa formula alternativa per la covarianza si dimostra come segue.\n\nPer definizione, la **covarianza** tra due variabili casuali $X$ e $Y$ è:\n\n$$\n\\mathrm{Cov}(X, Y) \\;=\\; \\mathbb{E}\\Bigl[\\bigl(X - \\mathbb{E}[X]\\bigr)\\,\\bigl(Y - \\mathbb{E}[Y]\\bigr)\\Bigr].\n$$\n\nQuesta è semplicemente la definizione formale, in cui consideriamo la \"deviazione\" di $X$ dal proprio valor medio ($\\mathbb{E}[X]$) e la \"deviazione\" di $Y$ dal proprio valor medio ($\\mathbb{E}[Y]$), e ne calcoliamo l'aspettativa del prodotto.\n\nConsideriamo l’argomento dell’aspettativa: $\\bigl(X - \\mathbb{E}[X]\\bigr)\\,\\bigl(Y - \\mathbb{E}[Y]\\bigr)$.\n\nPer prima cosa **espandiamo** il prodotto come faremmo con normali variabili algebriche:\n\n$$\n\\bigl(X - \\mathbb{E}[X]\\bigr)\\,\\bigl(Y - \\mathbb{E}[Y]\\bigr) \n= X\\,Y \\;-\\; X\\,\\mathbb{E}[Y] \\;-\\; \\mathbb{E}[X]\\,Y \\;+\\; \\mathbb{E}[X]\\mathbb{E}[Y].\n$$\n\nAdesso prendiamo l’**aspettativa** (o valore atteso) di ciascun termine che abbiamo ottenuto. Indichiamo con $\\mathbb{E}$ l’operatore di aspettativa:\n\n$$\n\\mathbb{E}\\Bigl[\\bigl(X - \\mathbb{E}[X]\\bigr)\\,\\bigl(Y - \\mathbb{E}[Y]\\bigr)\\Bigr]\n= \\mathbb{E}[\\,X\\,Y \\;-\\; X\\,\\mathbb{E}[Y] \\;-\\; \\mathbb{E}[X]\\,Y \\;+\\; \\mathbb{E}[X]\\mathbb{E}[Y]\\,].\n$$\n\nGrazie alla **linearità dell’aspettativa**, possiamo scindere questa grande aspettativa in una somma (e differenza) di aspettative di singoli termini:\n\n$$\n= \\mathbb{E}[XY] \n\\;-\\; \\mathbb{E}[X\\,\\mathbb{E}[Y]]\n\\;-\\; \\mathbb{E}[\\mathbb{E}[X]\\,Y]\n\\;+\\; \\mathbb{E}[\\mathbb{E}[X]\\mathbb{E}[Y]].\n$$\n\nRicordiamo che $\\mathbb{E}[X]$ e $\\mathbb{E}[Y]$ sono numeri (costanti) e non variabili casuali. Dunque, quando nell’aspettativa compare un fattore costante, possiamo estrarlo fuori dall’operatore $\\mathbb{E}[\\cdot]$.\n\n1. $\\mathbb{E}[X\\,\\mathbb{E}[Y]]$ si semplifica in $\\mathbb{E}[Y]\\cdot \\mathbb{E}[X]$ perché $\\mathbb{E}[Y]$ è una costante. In formula:\n   $$\n   \\mathbb{E}[X\\,\\mathbb{E}[Y]] \n   = \\mathbb{E}[Y] \\,\\mathbb{E}[X].\n   $$\n\n2. Allo stesso modo, $\\mathbb{E}[\\mathbb{E}[X]\\,Y]$ si semplifica in $\\mathbb{E}[X]\\cdot \\mathbb{E}[Y]$.\n\n3. Infine, $\\mathbb{E}[\\mathbb{E}[X]\\mathbb{E}[Y]]$ è $\\mathbb{E}[X]\\mathbb{E}[Y]$ in quanto $\\mathbb{E}[X]\\mathbb{E}[Y]$ è già una costante.\n\nUsando queste regole, riscriviamo i termini:\n\n$$\n\\mathbb{E}[XY] \n\\;-\\; \\mathbb{E}[X]\\mathbb{E}[Y]\n\\;-\\; \\mathbb{E}[X]\\mathbb{E}[Y]\n\\;+\\; \\mathbb{E}[X]\\mathbb{E}[Y].\n$$\n\nOsserviamo i termini rimanenti:\n\n$$\n\\mathbb{E}[XY] \\;-\\; \\mathbb{E}[X]\\mathbb{E}[Y]\n\\;-\\; \\mathbb{E}[X]\\mathbb{E}[Y]\n\\;+\\; \\mathbb{E}[X]\\mathbb{E}[Y].\n$$\n\n- Il termine $\\mathbb{E}[X]\\mathbb{E}[Y]$ compare **due volte** in negativo ($-\\,\\mathbb{E}[X]\\mathbb{E}[Y]$) e **una volta** in positivo ($+\\,\\mathbb{E}[X]\\mathbb{E}[Y]$).  \n- Facendo la somma algebrica, ne rimane solo $-\\,\\mathbb{E}[X]\\mathbb{E}[Y]$ (perché $-\\,1 -\\,1 +\\,1 = -\\,1$).\n\nQuindi il risultato è:\n\n$$\n\\mathbb{E}[XY]\n\\;-\\; \\mathbb{E}[X]\\mathbb{E}[Y].\n$$\n\nAbbiamo quindi dimostrato in maniera esplicita che:\n\n$$\n\\mathrm{Cov}(X, Y)\n= \\mathbb{E}\\bigl[(X - \\mathbb{E}[X]) (Y - \\mathbb{E}[Y])\\bigr]\n= \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y].\n$$\n\n### Esempio Psicologico: Covarianza tra Ansia e Prestazione Cognitiva\n\nRiprendendo i dati del capitolo precedente sulla relazione tra ansia (Y) e prestazione cognitiva (X), calcoliamo ora la covarianza.\n\nMedie marginali:\n\n- Prestazione cognitiva $X$:  \n$$\\mathbb{E}(X)=0\\times0.30 + 1\\times0.45 + 2\\times0.25=0.95$$\n- Ansia $Y$:  \n$$\\mathbb{E}(Y)=0\\times0.30 + 1\\times0.40 + 2\\times0.30=1.00$$\n\nCalcoliamo $\\mathbb{E}(XY)$:\n\n$$\n\\begin{aligned}\n\\mathbb{E}(XY) &= (0\\times0\\times0.05)+(0\\times1\\times0.10)+(0\\times2\\times0.15)+\n\\notag\\\\\n& \\quad (1\\times0\\times0.15)+(1\\times1\\times0.20)+(1\\times2\\times0.10)+\n\\notag\\\\\n& \\quad(2\\times0\\times0.10)+(2\\times1\\times0.10)+(2\\times2\\times0.05)\n\\end{aligned}\n$$\n\nSimplificando:\n\n$$\\mathbb{E}(XY)=0.00+0.00+0.00+0.00+0.20+0.20+0.00+0.20+0.20=0.80$$\n\nQuindi, la covarianza sarà:\n\n$$\\text{Cov}(X,Y)=\\mathbb{E}(XY)-\\mathbb{E}(X)\\mathbb{E}(Y)=0.80-(0.95\\times1.00)=-0.15$$\n\nLa covarianza negativa indica che all'aumentare del livello di ansia tende a corrispondere una diminuzione della prestazione cognitiva, coerentemente con quanto spesso riscontrato nella letteratura psicologica.\n\n\n## Correlazione\n\nLa correlazione standardizza la covarianza, rendendola indipendente dalle unità di misura delle variabili. Essa varia tra -1 e 1 ed è definita come:\n\n$$\n\\rho(X,Y)=\\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}} .\n$$\n\ndove $\\mathbb{V}(X)$ e $\\mathbb{V}(Y)$ rappresentano le varianze di $X$ e $Y$, rispettivamente.\n\nIl coefficiente di correlazione $\\rho_{xy}$ è un valore adimensionale, ovvero non dipende dalle unità di misura delle variabili, e varia nell'intervallo $-1 \\leq \\rho \\leq 1$.\n\n### Calcolo della Correlazione\n\nPer calcolare la correlazione tra ansia e prestazione cognitiva, dobbiamo prima ottenere le varianze di ciascuna variabile.\n\n- Varianza di X (prestazione cognitiva):\n\n$$\n\\begin{aligned}\n\\text{Var}(X) &=\\sum_{x}(x-\\mu_X)^2p(x)\n\\notag\\\\\n&= (0-0.95)^2\\times0.30+(1-0.95)^2\\times0.45+(2-0.95)^2\\times0.25=0.5475 \\notag\n\\end{aligned}\n$$\n\n- Varianza di Y (ansia):\n\n$$\n\\begin{aligned}\n\\text{Var}(Y) &=\\sum_{y}(y-\\mu_Y)^2p(y) \\notag\\\\\n&= (0-1.00)^2\\times0.30+(1-1.00)^2\\times0.40+(2-1.00)^2\\times0.30=0.60 \\notag\n\\end{aligned}\n$$\n\nQuindi, il coefficiente di correlazione è:\n\n$$\n\\rho(X,Y)=\\frac{-0.15}{\\sqrt{0.5475\\times0.60}}=-0.261\n$$\n\nIl valore negativo della correlazione conferma che ansia e prestazione cognitiva presentano una relazione inversa: all'aumentare dell'ansia, la prestazione tende a diminuire.\n\n\n## Interpretazione della Correlazione\n\nIl coefficiente di correlazione è una misura standardizzata e facile da interpretare:\n\n- $\\rho = 1$: perfetta relazione lineare positiva\n- $\\rho = -1$: perfetta relazione lineare negativa\n- $\\rho = 0$: assenza di relazione lineare\n\nNel nostro esempio, il valore $-0.261$ indica una relazione lineare negativa moderata tra ansia e prestazione.\n\n## Proprietà\n\nLa covarianza e la correlazione possiedono una serie di proprietà formali che ne regolano il comportamento al variare delle variabili e delle costanti coinvolte. Di seguito vengono enunciate e discusse le principali.\n\nLa covarianza tra una variabile aleatoria $X$ e una costante $c$ è sempre nulla:\n$$\n\\text{Cov}(c, X) = 0.\n$$\nInoltre, la covarianza gode della proprietà di simmetria:\n$$\n\\text{Cov}(X, Y) = \\text{Cov}(Y, X).\n$$\n\nIl coefficiente di correlazione $\\rho_{X,Y}$ è limitato nell'intervallo chiuso $[-1, 1]$:\n$$\n-1 \\leq \\rho(X, Y) \\leq 1.\n$$\nTale coefficiente è invariante rispetto a cambiamenti di scala delle variabili: per ogni $a > 0$ e $b > 0$ si ha\n$$\n\\rho(aX, bY) = \\rho(X, Y).\n$$\nNel caso di una relazione lineare perfetta della forma $Y = a + bX$, il coefficiente di correlazione assume valore estremo:\n$$\n\\rho(X, Y) = \\begin{cases}\n+1 & \\text{se } b > 0, \\\\\n-1 & \\text{se } b < 0.\n\\end{cases}\n$$\n\nRiguardo alle trasformazioni lineari, la covarianza soddisfa:\n$$\n\\text{Cov}(aX, bY) = ab \\cdot \\text{Cov}(X, Y),\n$$\ne, più in generale, per due combinazioni lineari di variabili aleatorie:\n$$\n\\text{Cov}\\left( \\sum_{i=1}^n a_i X_i, \\sum_{j=1}^m b_j Y_j \\right) = \\sum_{i=1}^n \\sum_{j=1}^m a_i b_j \\, \\text{Cov}(X_i, Y_j).\n$$\n\nLa varianza della somma (o differenza) di due variabili è data da:\n$$\n\\mathbb{V}(X \\pm Y) = \\mathbb{V}(X) + \\mathbb{V}(Y) \\pm 2 \\, \\text{Cov}(X, Y).\n$$\nPiù in generale, per una somma di $n$ variabili aleatorie:\n$$\n\\mathbb{V}\\left( \\sum_{i=1}^n X_i \\right) = \\sum_{i=1}^n \\mathbb{V}(X_i) + 2 \\sum_{i < j} \\text{Cov}(X_i, X_j).\n$$\n\nLa covarianza è additiva in ciascun argomento: per ogni variabile aleatoria $Z$,\n$$\n\\text{Cov}(X + Y, Z) = \\text{Cov}(X, Z) + \\text{Cov}(Y, Z).\n$$\n\nNel caso particolare in cui le variabili $X_1, X_2, \\dots, X_n$ siano mutualmente indipendenti, la covarianza tra due loro combinazioni lineari si semplifica in:\n$$\n\\text{Cov}\\left( \\sum_{i=1}^n a_i X_i, \\sum_{j=1}^n b_j X_j \\right) = \\sum_{i=1}^n a_i b_i \\, \\mathbb{V}(X_i),\n$$\npoiché i termini di covarianza incrociata sono nulli.\n\n\n### Incorrelazione\n\nDue variabili casuali $X$ e $Y$ si dicono *incorrelate* (o *linearmente indipendenti*) se la loro covarianza è nulla:\n\n$$\n\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)] = 0.\n$$\n\nTale condizione è equivalente a ciascuna delle seguenti:\n\n- il coefficiente di correlazione $\\rho_{XY}$ è nullo;\n- il valore atteso del prodotto è uguale al prodotto dei valori attesi: $\\mathbb{E}(XY) = \\mathbb{E}(X)\\mathbb{E}(Y)$.\n\nL’incorrelazione rappresenta una forma di indipendenza statistica più debole rispetto all’*indipendenza stocastica*. Mentre due variabili indipendenti sono sempre incorrelate, il viceversa non è vero: è possibile che $X$ e $Y$ abbiano covarianza nulla pur non essendo stocasticamente indipendenti. In altri termini, l’assenza di correlazione lineare non esclude l’esistenza di altre forme di dipendenza (ad esempio, di tipo non lineare) tra le due variabili.\n\n::: {#exm-}\nConsideriamo una distribuzione di probabilità congiunta di due variabili aleatorie, $X$ e $Y$, definita come:\n\n$$\nf_{XY}(x,y) = \\left\\{\n\\begin{array}{ll}\n\\frac{1}{4} & \\text{per } (x,y) \\in \\{(0,0), (1,1), (1, -1), (2,0) \\}, \\\\\n0 & \\text{altrimenti.}\n\\end{array}\n\\right.\n$$\n\nQuesto implica che le variabili aleatorie $X$ e $Y$ assumono valori specifici con probabilità uniforme solo per determinate coppie $(x, y)$ e zero in tutti gli altri casi.\n:::\n\n## Riflessioni conclusive {.unnumbered .unlisted}\n\nLa covarianza e la correlazione forniscono strumenti essenziali per quantificare le relazioni tra variabili casuali. Utilizzare queste misure permette di approfondire la comprensione delle relazioni psicologiche, come quella tra ansia e prestazione, facilitando ulteriori analisi statistiche e interpretazioni teoriche.\n\n\n::: {.callout-important title=\"Problemi 1\" collapse=\"true\"}\n\n**Esercizio 1: Distribuzione congiunta di due lanci di dado**\n\nSi lancia due volte un dado a sei facce equilibrato. Siano:\n\n- $X$ il risultato del primo lancio.\n- $Y$ il risultato del secondo lancio.\n\n1. Costruisci la tabella della distribuzione congiunta $P(X, Y)$, considerando che tutti i risultati possibili hanno la stessa probabilità.\n2. Verifica che la somma delle probabilità sia 1.\n3. Determina la distribuzione marginale di $X$ e di $Y$.\n4. Le variabili $X$ e $Y$ sono indipendenti? Giustifica la risposta.\n\n**Esercizio 2: Somma di due dadi**\n\nSi lancia due volte un dado a sei facce. Definiamo:\n\n- $S = X + Y$, la somma dei due risultati.\n\n1. Costruisci la tabella di probabilità congiunta $P(X, Y)$.\n2. Calcola la distribuzione di probabilità della variabile aleatoria $S$.\n3. Determina $P(S = 7)$ e $P(S \\leq 5)$.\n4. Qual è il valore più probabile di $S$? E il meno probabile?\n\n**Esercizio 3: Lancio di tre monete**\n\nSi lanciano tre monete equilibrate. Definiamo:\n\n- $X$ il numero di teste ottenute.\n- $Y$ il risultato del primo lancio (1 se testa, 0 se croce).\n\n1. Determina lo spazio campionario e associa i valori delle variabili aleatorie $X$ e $Y$.\n2. Costruisci la distribuzione congiunta $P(X, Y)$.\n3. Calcola $P(X = 2 \\mid Y = 1)$ e $P(Y = 1 \\mid X = 2)$.\n4. Le variabili $X$ e $Y$ sono indipendenti?\n\n**Esercizio 4: Minimo e massimo tra due dadi**\n\nSi lancia due volte un dado a sei facce. Definiamo:\n\n- $X = \\min \\{X_1, X_2\\}$, il valore minimo tra i due lanci.\n- $Y = \\max \\{X_1, X_2\\}$, il valore massimo tra i due lanci.\n\n1. Costruisci la tabella della distribuzione congiunta $P(X, Y)$.\n2. Calcola $P(X = 3, Y = 5)$ e $P(X \\geq 3, Y \\leq 4)$.\n3. Determina la distribuzione marginale di $X$ e di $Y$.\n4. Calcola la covarianza tra $X$ e $Y$.\n\n**Esercizio 5: Differenza tra due dadi**\n\nSi lanciano due dadi a sei facce. Definiamo:\n\n- $X$ il risultato del primo lancio.\n- $Y$ la differenza assoluta tra i due risultati, ovvero $Y = |X - X_2|$.\n\n1. Determina la tabella della distribuzione congiunta $P(X, Y)$.\n2. Calcola la distribuzione marginale di $Y$.\n3. Determina $P(Y = 0)$ e $P(Y = 3)$.\n4. Le variabili $X$ e $Y$ sono indipendenti? Giustifica la risposta.\n:::\n\n::: {.callout-tip title=\"Soluzioni 1\" collapse=\"true\"}\n**Esercizio 1: Distribuzione congiunta di due lanci di dado**\n\nAbbiamo due variabili aleatorie discrete:\n- $X$, risultato del primo lancio di un dado a sei facce.\n- $Y$, risultato del secondo lancio.\n\n**1. Tabella della distribuzione congiunta $P(X, Y)$**\nPoiché il dado è equo, ogni coppia di risultati $(x, y)$ ha la stessa probabilità. Esistono $6 \\times 6 = 36$ combinazioni possibili, e ognuna ha probabilità:\n\n$$\nP(X = x, Y = y) = \\frac{1}{36}, \\quad \\text{per ogni } x, y \\in \\{1, 2, 3, 4, 5, 6\\}\n$$\n\nLa tabella della distribuzione congiunta è:\n\n| $X$ \\ $Y$ | 1      | 2      | 3      | 4      | 5      | 6      |\n|--------|--------|--------|--------|--------|--------|--------|\n| **1**  | 1/36 | 1/36 | 1/36 | 1/36 | 1/36 | 1/36 |\n| **2**  | 1/36 | 1/36 | 1/36 | 1/36 | 1/36 | 1/36 |\n| **3**  | 1/36 | 1/36 | 1/36 | 1/36 | 1/36 | 1/36 |\n| **4**  | 1/36 | 1/36 | 1/36 | 1/36 | 1/36 | 1/36 |\n| **5**  | 1/36 | 1/36 | 1/36 | 1/36 | 1/36 | 1/36 |\n| **6**  | 1/36 | 1/36 | 1/36 | 1/36 | 1/36 | 1/36 |\n\n**2. Verifica che la somma delle probabilità sia 1**\nLa somma di tutte le probabilità è:\n\n$$\n\\sum_{x=1}^{6} \\sum_{y=1}^{6} P(X = x, Y = y) = 36 \\times \\frac{1}{36} = 1.\n$$\n\n**3. Distribuzione marginale di $X$ e $Y$**\nPer ottenere la distribuzione marginale di $X$:\n\n$$\nP(X = x) = \\sum_{y=1}^{6} P(X = x, Y = y) = 6 \\times \\frac{1}{36} = \\frac{1}{6}, \\quad \\forall x.\n$$\n\nAnalogamente, per $Y$:\n\n$$\nP(Y = y) = \\sum_{x=1}^{6} P(X = x, Y = y) = \\frac{1}{6}, \\quad \\forall y.\n$$\n\nEntrambe seguono una distribuzione uniforme su $\\{1, 2, 3, 4, 5, 6\\}$.\n\n**4. Indipendenza di $X$ e $Y$**\nDue variabili sono indipendenti se $P(X = x, Y = y) = P(X = x) P(Y = y)$. \n\n$$\n\\frac{1}{36} = \\frac{1}{6} \\times \\frac{1}{6} = \\frac{1}{36}, \\quad \\forall x, y.\n$$\n\nPoiché questa relazione vale per tutti i valori, $X$ e $Y$ sono indipendenti.\n\n**Esercizio 2: Somma di due dadi**\n\nAbbiamo:\n\n$$\nS = X + Y\n$$\n\n**1. Tabella di probabilità congiunta $P(X, Y)$**\nÈ la stessa tabella costruita nel primo esercizio.\n\n**2. Distribuzione di probabilità di $S$**\nLa somma $S$ assume valori da $2$ (1+1) a $12$ (6+6). La probabilità di ogni valore di $S$ si ottiene contando le coppie $(x, y)$ che lo producono:\n\n| $S$ | $P(S)$ |\n|----|----|\n| 2  | 1/36 |\n| 3  | 2/36 |\n| 4  | 3/36 |\n| 5  | 4/36 |\n| 6  | 5/36 |\n| 7  | 6/36 |\n| 8  | 5/36 |\n| 9  | 4/36 |\n| 10 | 3/36 |\n| 11 | 2/36 |\n| 12 | 1/36 |\n\n**3. Calcolo di $P(S = 7)$ e $P(S \\leq 5)$**\n- $P(S = 7) = 6/36 = 1/6$.\n- $P(S \\leq 5) = P(S = 2) + P(S = 3) + P(S = 4) + P(S = 5)$\n\n$$\n\\frac{1}{36} + \\frac{2}{36} + \\frac{3}{36} + \\frac{4}{36} = \\frac{10}{36} = \\frac{5}{18}.\n$$\n\n**4. Valori più probabili e meno probabili**\n- Il valore più probabile è $S = 7$ ($P(S=7) = 1/6$).\n- I valori meno probabili sono $S = 2$ e $S = 12$ ($P(S) = 1/36$).\n\n**Esercizio 3: Lancio di tre monete**\n\nAbbiamo:\n\n- Tre monete equilibrare.\n- Variabili:\n  - $X$: numero di teste ottenute.\n  - $Y$: risultato del primo lancio (1 se testa, 0 se croce).\n\n**1. Spazio campionario e valori di $X$ e $Y$**\n\nLo spazio campionario dei lanci è:\n\n$$\n\\{ (C, C, C), (C, C, T), (C, T, C), (C, T, T), (T, C, C), (T, C, T), (T, T, C), (T, T, T) \\}\n$$\n\nOra assegniamo $X$ e $Y$:\n\n| Lancio | $X$ (num. teste) | $Y$ (primo lancio) |\n|--------|-----------------|-----------------|\n| C, C, C | 0               | 0               |\n| C, C, T | 1               | 0               |\n| C, T, C | 1               | 0               |\n| C, T, T | 2               | 0               |\n| T, C, C | 1               | 1               |\n| T, C, T | 2               | 1               |\n| T, T, C | 2               | 1               |\n| T, T, T | 3               | 1               |\n\n**2. Distribuzione congiunta $P(X, Y)$**\n\nPoiché ogni lancio ha probabilità $\\frac{1}{8}$, la tabella di probabilità congiunta è:\n\n| $X$ \\ $Y$ | 0    | 1    |\n|--------------|------|------|\n| **0**        | 1/8  | 0    |\n| **1**        | 2/8  | 1/8  |\n| **2**        | 1/8  | 3/8  |\n| **3**        | 0    | 1/8  |\n\n**3. Probabilità condizionate**\n**$P(X = 2 \\mid Y = 1)$**\n$$\nP(X = 2 \\mid Y = 1) = \\frac{P(X = 2, Y = 1)}{P(Y = 1)} = \\frac{3/8}{5/8} = \\frac{3}{5}.\n$$\n\n**$P(Y = 1 \\mid X = 2)$**\n$$\nP(Y = 1 \\mid X = 2) = \\frac{P(X = 2, Y = 1)}{P(X = 2)} = \\frac{3/8}{4/8} = \\frac{3}{4}.\n$$\n\n**4. Indipendenza di $X$ e $Y$**\n\nVerifichiamo se $P(X = x, Y = y) = P(X = x) P(Y = y)$ per ogni coppia.\n\nEsempio: $P(X = 2, Y = 1) = 3/8$ ma $P(X=2) P(Y=1) = (4/8)(5/8) = 20/64 = 5/16 \\neq 3/8$.\n\nQuindi $X$ e $Y$ **non** sono indipendenti.\n\n\n**Esercizio 4: Minimo e massimo tra due dadi**\n\nAbbiamo:\n\n- $X = \\min(X_1, X_2)$, il minimo tra i due lanci.\n- $Y = \\max(X_1, X_2)$, il massimo tra i due lanci.\n\n**1. Tabella della distribuzione congiunta**\n\nPoiché i due lanci sono indipendenti e simmetrici, ci sono 36 coppie $(X_1, X_2)$, e ogni coppia ha probabilità $\\frac{1}{36}$.\n\nLa tabella congiunta si costruisce considerando che $X = \\min(X_1, X_2)$ e $Y = \\max(X_1, X_2)$:\n\n| $X$ \\ $Y$ | 1    | 2    | 3    | 4    | 5    | 6    |\n|--------------|------|------|------|------|------|------|\n| **1**        | 1/36 | 2/36 | 3/36 | 4/36 | 5/36 | 6/36 |\n| **2**        | -    | 1/36 | 2/36 | 3/36 | 4/36 | 5/36 |\n| **3**        | -    | -    | 1/36 | 2/36 | 3/36 | 4/36 |\n| **4**        | -    | -    | -    | 1/36 | 2/36 | 3/36 |\n| **5**        | -    | -    | -    | -    | 1/36 | 2/36 |\n| **6**        | -    | -    | -    | -    | -    | 1/36 |\n\n**2. Probabilità richieste**\n\n- $P(X = 3, Y = 5) = 3/36$.\n- $P(X \\geq 3, Y \\leq 4) = P(X = 3, Y = 3) + P(X = 3, Y = 4) + P(X = 4, Y = 4) = 1/36 + 2/36 + 1/36 = 4/36 = 1/9$.\n\n**Esercizio 5: Differenza tra due dadi**\n\nAbbiamo:\n\n- $X$ = primo lancio.\n- $Y = |X - X_2|$.\n\n**1. Tabella della distribuzione congiunta $P(X, Y)$**\n\n$Y$ assume valori da 0 a 5, a seconda della differenza tra i due dadi:\n\n| $X$ \\ $Y$ | 0    | 1    | 2    | 3    | 4    | 5    |\n|--------------|------|------|------|------|------|------|\n| **1**        | 1/6  | 1/6  | 1/6  | 1/6  | 1/6  | 1/6  |\n| **2**        | 1/6  | 2/6  | 1/6  | 1/6  | 1/6  | 0    |\n| **3**        | 1/6  | 2/6  | 2/6  | 1/6  | 0    | 0    |\n| **4**        | 1/6  | 2/6  | 2/6  | 1/6  | 0    | 0    |\n| **5**        | 1/6  | 2/6  | 1/6  | 1/6  | 0    | 0    |\n| **6**        | 1/6  | 1/6  | 1/6  | 1/6  | 0    | 0    |\n\n**2. Distribuzione marginale di $Y$**\n\nSommiamo lungo $X$:\n\n| $Y$ | $P(Y)$ |\n|----|----|\n| 0  | 6/36 |\n| 1  | 10/36 |\n| 2  | 8/36 |\n| 3  | 6/36 |\n| 4  | 4/36 |\n| 5  | 2/36 |\n\n**3. Probabilità richieste**\n\n- $P(Y = 0) = 6/36 = 1/6$.\n- $P(Y = 3) = 6/36 = 1/6$.\n\n**4. Indipendenza di $X$ e $Y$**\n\nCome nell’esercizio 3, verifichiamo che $P(X, Y) \\neq P(X) P(Y)$ per alcune coppie. Essendo la tabella non simmetrica, $X$ e $Y$ **non** sono indipendenti.\n:::\n\n::: {.callout-important title=\"Problemi 2\" collapse=\"true\"}\nConsidera il seguente esperimento casuale: si estrae una pallina da un'urna contenente **tre palline numerate** con i valori $1$, $2$ e $3$. \n\nDopo l'estrazione, si definiscono due variabili casuali:\n\n- $X$, il valore della pallina estratta.\n- $Y$, il valore di un'altra variabile definita come $Y = X^2$.\n\n1. Costruisci la distribuzione congiunta di $X$ e $Y$.\n2. Calcola il valore atteso di $X$ e $Y$, ossia $E[X]$ e $E[Y]$.\n3. Calcola la covarianza tra $X$ e $Y$, ossia $\\text{Cov}(X, Y)$.\n4. Calcola la correlazione tra $X$ e $Y$, ossia $\\rho(X, Y)$.\n5. Interpreta il valore della correlazione: cosa indica il segno e il valore ottenuto?\n:::\n\n::: {.callout-tip title=\"Soluzioni 2\" collapse=\"true\"}\n**1. Distribuzione congiunta di $X$ e $Y$**\n\nPoiché ogni pallina ha la stessa probabilità di essere estratta, la distribuzione congiunta è:\n\n| $X$ | $Y = X^2$ | $P(X, Y)$ |\n|-----|-----------|-----------|\n| 1   | 1         | $\\frac{1}{3}$ |\n| 2   | 4         | $\\frac{1}{3}$ |\n| 3   | 9         | $\\frac{1}{3}$ |\n\n**2. Calcolo di $E[X]$ e $E[Y]$**\n\n$$\nE[X] = \\sum_{i} x_i P(X = x_i) = 1 \\cdot \\frac{1}{3} + 2 \\cdot \\frac{1}{3} + 3 \\cdot \\frac{1}{3} = \\frac{1 + 2 + 3}{3} = 2\n$$\n\n$$\nE[Y] = \\sum_{i} y_i P(Y = y_i) = 1 \\cdot \\frac{1}{3} + 4 \\cdot \\frac{1}{3} + 9 \\cdot \\frac{1}{3} = \\frac{1 + 4 + 9}{3} = \\frac{14}{3}\n$$\n\n**3. Calcolo della covarianza $\\text{Cov}(X, Y)$**\n\nLa covarianza è definita come:\n\n$$\n\\text{Cov}(X, Y) = E[XY] - E[X]E[Y]\n$$\n\nPrima calcoliamo $E[XY]$:\n\n$$\nE[XY] = \\sum_{i} x_i y_i P(X = x_i, Y = y_i) = 1 \\cdot 1 \\cdot \\frac{1}{3} + 2 \\cdot 4 \\cdot \\frac{1}{3} + 3 \\cdot 9 \\cdot \\frac{1}{3}\n$$\n\n$$\n= \\frac{1 + 8 + 27}{3} = \\frac{36}{3} = 12\n$$\n\nOra possiamo calcolare la covarianza:\n\n$$\n\\text{Cov}(X, Y) = E[XY] - E[X]E[Y] = 12 - \\left(2 \\cdot \\frac{14}{3}\\right) = 12 - \\frac{28}{3} = \\frac{36 - 28}{3} = \\frac{8}{3}\n$$\n\n**4. Calcolo della correlazione $\\rho(X, Y)$**\n\nLa correlazione è definita come:\n\n$$\n\\rho(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\cdot \\sigma_Y}\n$$\n\nCalcoliamo prima le varianze:\n\n$$\n\\text{Var}(X) = E[X^2] - (E[X])^2\n$$\n\n$$\nE[X^2] = 1^2 \\cdot \\frac{1}{3} + 2^2 \\cdot \\frac{1}{3} + 3^2 \\cdot \\frac{1}{3} = \\frac{1 + 4 + 9}{3} = \\frac{14}{3}\n$$\n\n$$\n\\text{Var}(X) = \\frac{14}{3} - 2^2 = \\frac{14}{3} - 4 = \\frac{14 - 12}{3} = \\frac{2}{3}\n$$\n\nOra la varianza di $Y$:\n\n$$\n\\text{Var}(Y) = E[Y^2] - (E[Y])^2\n$$\n\n$$\nE[Y^2] = 1^2 \\cdot \\frac{1}{3} + 4^2 \\cdot \\frac{1}{3} + 9^2 \\cdot \\frac{1}{3} = \\frac{1 + 16 + 81}{3} = \\frac{98}{3}\n$$\n\n$$\n\\text{Var}(Y) = \\frac{98}{3} - \\left(\\frac{14}{3}\\right)^2 = \\frac{98}{3} - \\frac{196}{9} = \\frac{98 \\cdot 3 - 196}{9} = \\frac{294 - 196}{9} = \\frac{98}{9}\n$$\n\nCalcoliamo le deviazioni standard:\n\n$$\n\\sigma_X = \\sqrt{\\text{Var}(X)} = \\sqrt{\\frac{2}{3}} = \\frac{\\sqrt{6}}{3}\n$$\n\n$$\n\\sigma_Y = \\sqrt{\\text{Var}(Y)} = \\sqrt{\\frac{98}{9}} = \\frac{\\sqrt{98}}{3}\n$$\n\nOra possiamo calcolare la correlazione:\n\n$$\n\\rho(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\cdot \\sigma_Y} = \\frac{\\frac{8}{3}}{\\frac{\\sqrt{6}}{3} \\cdot \\frac{\\sqrt{98}}{3}}\n$$\n\n$$\n= \\frac{\\frac{8}{3}}{\\frac{\\sqrt{6 \\cdot 98}}{9}} = \\frac{8 \\cdot 9}{3 \\cdot \\sqrt{6 \\cdot 98}} = \\frac{24}{\\sqrt{588}}\n$$\n\nPoiché $\\sqrt{588} = \\sqrt{4 \\cdot 147} = 2\\sqrt{147} = 2\\sqrt{49 \\cdot 3} = 2 \\cdot 7 \\cdot \\sqrt{3} = 14\\sqrt{3}$:\n\n$$\n\\rho(X, Y) = \\frac{24}{14\\sqrt{3}} = \\frac{12}{7\\sqrt{3}} = \\frac{12\\sqrt{3}}{21} \\approx 0.995\n$$\n\n**5. Interpretazione della correlazione**\n\n- Il valore $\\rho(X, Y) \\approx 0.995$ è molto vicino a 1, indicando una correlazione positiva quasi perfetta tra $X$ e $Y$.\n- Il segno positivo indica che all'aumentare di $X$, anche $Y$ tende ad aumentare.\n- L'alto valore (prossimo a 1) indica che la relazione tra $X$ e $Y$ è quasi perfettamente lineare, il che è coerente con la definizione $Y = X^2$ nell'intervallo considerato (piccoli valori positivi di $X$).\n:::\n\n::: {.callout-important title=\"Problemi 3\" collapse=\"true\"}\nEsercizi sulla distribuzione di probabilità congiunta sono disponibili sulla seguente [pagina web](https://stats.libretexts.org/Courses/Saint_Mary's_College_Notre_Dame/MATH_345__-_Probability_(Kuter)/5%3A_Probability_Distributions_for_Combinations_of_Random_Variables/5.1%3A_Joint_Distributions_of_Discrete_Random_Variables).\n:::\n\n::: {.callout-note collapse=true title=\"Informazioni sull'ambiente di sviluppo\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#>  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#>  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#> [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#> [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#> [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#> [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#> [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#> [25] here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#>  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#>  [7] digest_0.6.37         timechange_0.3.0      estimability_1.5.1   \n#> [10] lifecycle_1.0.4       survival_3.8-3        magrittr_2.0.3       \n#> [13] compiler_4.5.1        rlang_1.1.6           tools_4.5.1          \n#> [16] knitr_1.50            bridgesampling_1.1-2  htmlwidgets_1.6.4    \n#> [19] curl_7.0.0            pkgbuild_1.4.8        RColorBrewer_1.1-3   \n#> [22] abind_1.4-8           multcomp_1.4-28       withr_3.0.2          \n#> [25] purrr_1.1.0           grid_4.5.1            stats4_4.5.1         \n#> [28] colorspace_2.1-1      xtable_1.8-4          inline_0.3.21        \n#> [31] emmeans_1.11.2-8      scales_1.4.0          MASS_7.3-65          \n#> [34] cli_3.6.5             mvtnorm_1.3-3         rmarkdown_2.29       \n#> [37] ragg_1.5.0            generics_0.1.4        RcppParallel_5.1.11-1\n#> [40] cachem_1.1.0          stringr_1.5.1         splines_4.5.1        \n#> [43] parallel_4.5.1        vctrs_0.6.5           V8_7.0.0             \n#> [46] Matrix_1.7-4          sandwich_3.1-1        jsonlite_2.0.0       \n#> [49] arrayhelpers_1.1-0    systemfonts_1.2.3     glue_1.8.0           \n#> [52] codetools_0.2-20      distributional_0.5.0  lubridate_1.9.4      \n#> [55] stringi_1.8.7         gtable_0.3.6          QuickJSR_1.8.0       \n#> [58] htmltools_0.5.8.1     Brobdingnag_1.2-9     R6_2.6.1             \n#> [61] textshaping_1.0.3     rprojroot_2.1.1       evaluate_1.0.5       \n#> [64] lattice_0.22-7        backports_1.5.0       memoise_2.0.1        \n#> [67] broom_1.0.9           snakecase_0.11.1      rstantools_2.5.0     \n#> [70] coda_0.19-4.1         gridExtra_2.3         nlme_3.1-168         \n#> [73] checkmate_2.3.3       xfun_0.53             zoo_1.8-14           \n#> [76] pkgconfig_2.0.3\n```\n:::\n\n:::\n\n## Bibliografia {.unnumbered .unlisted}\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}