{
  "hash": "5af3e431ca6c3a62c0b1a25f53fa1360",
  "result": {
    "engine": "knitr",
    "markdown": "# Distribuzioni di v.c. discrete {#sec-prob-discrete-prob-distr}\n\n::: callout-important\n## In questo capitolo imparerai a:\n\n- comprendere le principali distribuzioni di massa di probabilità;\n- utilizzare R per manipolare e analizzare queste distribuzioni.\n::: \n\n::: callout-tip\n## Prerequisiti\n\n- Leggere il capitolo *Random variables and their distributions* del testo di @blitzstein2019introduction.\n- Leggere il capitolo *Special Distributions* [@schervish2014probability].\n:::\n\n::: callout-caution\n## Preparazione del Notebook\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(reshape2)\n```\n:::\n\n:::\n\n\n## Introduzione\n\nÈ importante distinguere tra variabili casuali discrete e continue, perché le distribuzioni di probabilità associate sono molto diverse nei due casi [si veda il @sec-prob-random-var].\n\nIn questo capitolo ci focalizzeremo sulle **distribuzioni di probabilità discrete**, strumenti fondamentali per modellare fenomeni aleatori che generano un numero finito o numerabile di possibili esiti. Queste distribuzioni risultano particolarmente efficaci per descrivere eventi che si verificano in contesti discreti, come il numero di successi in un esperimento, l'occorrenza di un evento, o la selezione casuale da un insieme di opzioni finite. \n\n### Panoramica delle Distribuzioni Discrete\n\nDi seguito, vengono presentate alcune delle principali distribuzioni discrete utilizzate in statistica e nella ricerca psicologica Ogni distribuzione è descritta in termini di caratteristiche fondamentali, applicazioni pratiche e importanza teorica.\n\n#### Distribuzione Uniforme Discreta\n\n- **Descrizione**: La distribuzione uniforme discreta rappresenta situazioni in cui tutti gli eventi all'interno di un insieme finito hanno la stessa probabilità di verificarsi.\n- **Applicazioni**: Si applica in contesti di scelta casuale equiprobabile, come:\n  - La selezione casuale di uno stimolo da una lista di parole in un esperimento di memoria.\n  - L'assegnazione casuale di partecipanti a gruppi sperimentali in uno studio di psicologia sociale.\n  - La scelta di un'immagine tra un insieme di stimoli visivi in una ricerca sull'attenzione.\n  - La probabilità uniforme che un partecipante scelga una delle opzioni in un questionario a risposte multiple, in assenza di preferenze o conoscenze specifiche.\n- **Parametri**:\n  - Intervallo di supporto: l'insieme finito di valori possibili (ad esempio, $\\{1, 2, \\dots, k\\}$).\n- **Importanza**: Funziona come modello di riferimento in situazioni di massima incertezza o mancanza di preferenze. È utile per definire un punto di partenza in analisi più complesse e per studiare comportamenti casuali.\n\n\n#### Distribuzione di Bernoulli\n\n- **Descrizione**: La distribuzione di Bernoulli modella esperimenti con due possibili esiti, generalmente etichettati come \"successo\" (con probabilità $p$) e \"fallimento\" (con probabilità $1-p$).\n- **Applicazioni**: Si applica a situazioni binarie, come il lancio di una moneta (testa/croce), la risposta a domande dicotomiche (sì/no), o l'esito di un evento che può verificarsi o meno.\n- **Parametro**: \n  - $p$: probabilità di successo.\n- **Importanza**: Costituisce la base per molte altre distribuzioni discrete, come la distribuzione binomiale e geometrica. È fondamentale per comprendere fenomeni con esiti dichotomici.\n\n#### Distribuzione Binomiale\n\n- **Descrizione**: La distribuzione binomiale descrive il numero totale di successi in un numero fisso $n$ di prove indipendenti, ciascuna governata da una distribuzione di Bernoulli con probabilità di successo $p$.\n- **Applicazioni**: Viene utilizzata per analizzare processi ripetuti con esiti binari, ad esempio:\n  - Il numero di voti favorevoli in un campione di opinione.\n  - Il numero di sintomi osservati in un gruppo di pazienti.\n  - Il conteggio di errori in un test di accuratezza.\n- **Parametri**:\n  - $n$: numero di prove.\n  - $p$: probabilità di successo in ogni prova.\n- **Importanza**: Fornisce uno strumento essenziale per modellare fenomeni ripetuti in condizioni identiche, consentendo analisi probabilistiche avanzate e previsioni statistiche.\n\n#### Distribuzione di Poisson\n\n- **Descrizione**: La distribuzione di Poisson modella il numero di eventi che si verificano in un intervallo fissato di tempo o spazio, quando tali eventi sono rari, indipendenti e accadono a un tasso medio costante $\\lambda$.\n- **Applicazioni**: Trova impiego in contesti dove gli eventi sono sporadici ma prevedibili, ad esempio:\n  - Il numero di episodi di ansia riportati in una settimana.\n  - Il numero di interazioni sociali spontanee di un bambino con disturbo dello spettro autistico durante una sessione di osservazione.\n  - La frequenza di lapsus verbali durante una presentazione pubblica.\n  - Il numero di sogni vividi riportati durante una serie di notti consecutive in uno studio sul sonno.\n- **Parametro**:\n  - $\\lambda$: tasso medio di eventi per unità di tempo o spazio.\n- **Importanza**: È cruciale per analizzare fenomeni psicologici o comportamentali rari ma significativi. Aiuta a comprendere i meccanismi sottostanti e a modellare la variabilità osservata in contesti clinici, sperimentali o quotidiani.\n\n\nIn conclusione, le distribuzioni discrete sopra descritte rappresentano strumenti fondamentali per modellare una vasta gamma di fenomeni osservati in ambito scientifico, psicologico e applicativo. Ciascuna distribuzione offre una cornice teorica ben definita per interpretare e analizzare situazioni caratterizzate da variabili aleatorie discrete, fornendo così le basi per inferenze statistiche robuste e previsioni quantitative affidabili.\n\n## Distribuzioni in R\n\nIn R, per ogni distribuzione sono disponibili quattro funzioni principali, i cui nomi iniziano con le lettere:\n\n- **d** (*density*): per calcolare i valori teorici relativi alla distribuzione,  \n- **p** (*probability*): per ottenere la probabilità cumulativa,  \n- **q** (*quantile*): per determinare i quantili,  \n- **r** (*random*): per generare campioni casuali.  \n\nIl pacchetto di base `stats` include numerose funzioni dedicate alle principali distribuzioni statistiche, permettendo di calcolare valori teorici e simulare dati in modo semplice e flessibile. Per ulteriori dettagli sulle distribuzioni disponibili e sull'uso delle relative funzioni, è possibile consultare la documentazione con il comando `?Distributions`.\n\n\n## Distribuzione Uniforme Discreta\n\nLa **distribuzione uniforme discreta** è una delle più semplici e intuitive distribuzioni di probabilità. È utilizzata per modellare situazioni in cui **tutti gli esiti possibili sono ugualmente probabili**. Si applica, ad esempio, quando si estrae un numero a caso da un insieme finito di interi senza alcuna preferenza.\n\n::: {#def-}\n\nSia $X$ una variabile casuale che può assumere i valori interi da 1 a $N$, tutti con la **stessa probabilità**. Allora diciamo che $X$ ha una distribuzione uniforme discreta sull’intervallo $\\{1, 2, \\dots, N\\}$. In simboli:\n\n$$\nX \\sim \\text{Uniforme Discreta}(1, N) .\n$$\n\nPoiché ci sono $N$ valori possibili e ciascuno ha la stessa probabilità, ogni valore ha probabilità:\n\n$$\nP(X = x) = \\frac{1}{N}, \\quad \\text{per } x \\in \\{1, 2, \\dots, N\\}.\n$$\n:::\n\n### Proprietà di normalizzazione\n\nLa somma delle probabilità di tutti gli esiti deve essere pari a 1:\n\n$$\n\\sum_{x = 1}^{N} P(X = x) = \\sum_{x = 1}^{N} \\frac{1}{N} = \\frac{1}{N} \\cdot N = 1.\n$$\n\nQuesta è una proprietà fondamentale di ogni distribuzione di probabilità.\n\n### Valore atteso\n\nIl **valore atteso** (o media) ci dice qual è il risultato medio atteso nel lungo periodo. Si calcola come:\n\n$$\n\\mathbb{E}(X) = \\sum_{x = 1}^{N} x \\cdot P(X = x) = \\frac{1}{N} \\sum_{x = 1}^{N} x.\n$$\n\nLa somma dei primi $N$ numeri naturali è:\n\n$$\n\\sum_{x = 1}^{N} x = \\frac{N(N + 1)}{2}.\n$$\n\nQuindi:\n\n$$\n\\mathbb{E}(X) = \\frac{1}{N} \\cdot \\frac{N(N + 1)}{2} = \\frac{N + 1}{2}.\n$$\n\nIn conclusione, il valore atteso di una variabile uniforme discreta su $\\{1, \\dots, N\\}$ è $\\frac{N + 1}{2}$.\n\n### Varianza\n\nLa varianza della distribuzione uniforme discreta è:\n\n$$\n\\mathbb{V}(X) = \\frac{(N + 1)(N - 1)}{12}.\n$$\n\n::: {.callout-tip title=\"Dimostrazione\" collapse=\"true\"}\n\nLa **varianza** misura quanto i valori di $X$ si discostano in media dalla media $\\mathbb{E}(X)$. Si calcola come:\n\n$$\n\\mathbb{V}(X) = \\mathbb{E}(X^2) - \\left[\\mathbb{E}(X)\\right]^2.\n$$\n\n1. Calcolo di $\\mathbb{E}(X^2)$. \n\nPoiché tutti i valori hanno la stessa probabilità $\\frac{1}{N}$, otteniamo:\n\n$$\n\\mathbb{E}(X^2) = \\frac{1}{N} \\sum_{x = 1}^{N} x^2.\n$$\n\nLa somma dei quadrati dei primi $N$ interi è:\n\n$$\n\\sum_{x = 1}^{N} x^2 = \\frac{N(N + 1)(2N + 1)}{6}\n$$\n\n(per una dimostrazione, si veda la [ pagina di Wikipedia sui numeri piramidali quadrati](https://it.wikipedia.org/wiki/Numero_piramidale_quadrato)).\n\nQuindi:\n\n$$\n\\mathbb{E}(X^2) = \\frac{1}{N} \\cdot \\frac{N(N + 1)(2N + 1)}{6} = \\frac{(N + 1)(2N + 1)}{6}.\n$$\n\n2. Calcolo della varianza. \n\nSostituendo nella formula della varianza:\n\n$$\n\\begin{aligned}\n\\mathbb{V}(X) &= \\frac{(N + 1)(2N + 1)}{6} - \\left(\\frac{N + 1}{2}\\right)^2 \\\\\n&= \\frac{(N + 1)(2N + 1)}{6} - \\frac{(N + 1)^2}{4}\n\\end{aligned}\n$$\n\nPer semplificare, portiamo tutto allo stesso denominatore:\n\n$$\n\\begin{aligned}\n\\mathbb{V}(X) &= \\frac{2(N + 1)(2N + 1)}{12} - \\frac{3(N + 1)^2}{12} \\\\\n&= \\frac{(N + 1) \\left[2(2N + 1) - 3(N + 1)\\right]}{12} \\\\\n&= \\frac{(N + 1)(4N + 2 - 3N - 3)}{12} \\\\\n&= \\frac{(N + 1)(N - 1)}{12}.\n\\end{aligned}\n$$\n\nIn conclusione, la varianza della distribuzione uniforme discreta è:\n\n$$\n\\mathbb{V}(X) = \\frac{(N + 1)(N - 1)}{12}.\n$$\n:::\n\n**In sintesi**, per una variabile casuale $X$ uniformemente distribuita su $\\{1, 2, \\dots, N\\}$:\n\n| Proprietà        | Formula                                 |\n|------------------|------------------------------------------|\n| Media            | $\\mathbb{E}(X) = \\dfrac{N + 1}{2}$       |\n| Varianza         | $\\mathbb{V}(X) = \\dfrac{(N + 1)(N - 1)}{12}$ |\n\nQuesta distribuzione è utile ogni volta che **non c’è alcuna ragione per preferire un valore a un altro** all’interno di un insieme finito di numeri interi.\n\n::: {#exm-}\nSupponiamo che $X$ sia una variabile casuale con distribuzione uniforme discreta tra 1 e 10, ovvero:\n\n$$\nX \\sim \\text{Uniforme Discreta}(1, 10) .\n$$\n\nVogliamo:\n\n1. generare un grande campione casuale,\n2. calcolare la media e la varianza osservate,\n3. confrontarle con i valori teorici.\n\n**Codice R.**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)  # Per rendere la simulazione riproducibile\n\n# Parametro N\nN <- 10\n\n# Simulazione: 100.000 osservazioni dalla distribuzione uniforme discreta\nx <- sample(1:N, size = 100000, replace = TRUE)\n\n# Media e varianza empiriche\nmedia_empirica <- mean(x)\nvarianza_empirica <- var(x)\n\n# Valori teorici\nmedia_teorica <- (N + 1) / 2\nvarianza_teorica <- ((N + 1) * (N - 1)) / 12\n\n# Risultati\ntibble(\n  `Media empirica` = media_empirica,\n  `Media teorica` = media_teorica,\n  `Varianza empirica` = varianza_empirica,\n  `Varianza teorica` = varianza_teorica\n)\n#> # A tibble: 1 × 4\n#>   `Media empirica` `Media teorica` `Varianza empirica` `Varianza teorica`\n#>              <dbl>           <dbl>               <dbl>              <dbl>\n#> 1             5.51             5.5                8.26               8.25\n```\n:::\n\n\nCon un campione molto grande, le **statistiche empiriche** (cioè calcolate dai dati simulati) saranno molto vicine ai **valori teorici**:\n\n|                     | Valore     |\n|---------------------|------------|\n| Media teorica       | 5.5        |\n| Media empirica      | ≈ 5.5      |\n| Varianza teorica    | 8.25       |\n| Varianza empirica   | ≈ 8.25     |\n\n\\\n\n**In sintesi**, a simulazione conferma che:\n\n- la media empirica converge verso $\\mathbb{E}(X) = \\frac{N + 1}{2}$,\n- la varianza empirica converge verso $\\mathbb{V}(X) = \\frac{(N + 1)(N - 1)}{12}$.\n:::\n\n\n## Distribuzione di Bernoulli \n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nIn statistica, un esperimento che ammette solo due esiti possibili è modellato attraverso quella che viene chiamata \"prova Bernoulliana\". Un esempio tipico è il lancio di una moneta, che può dare come risultato testa o croce.\n\n::: {#def-}\nUna variabile casuale $X$ che assume valori in $\\{0, 1\\}$ è detta variabile di Bernoulli. La sua distribuzione di probabilità è definita come:\n\n$$\nP(X \\mid \\theta) =\n  \\begin{cases}\n    p     & \\text{se $X = 1$ (successo)}, \\\\\n    1 - p & \\text{se $X = 0$ (insuccesso)},\n  \\end{cases}\n$$\n\ndove $0 \\leq p \\leq 1$. Il parametro $p$ rappresenta la probabilità del \"successo\" ($X = 1$), mentre $1 - p$ è la probabilità dell'\"insuccesso\" ($X = 0$).\n:::\n\nLa distribuzione di Bernoulli descrive quindi un contesto in cui la probabilità di osservare l'esito 1 è $p$ e quella di osservare l'esito 0 è $1 - p$. Viene utilizzata per modellare situazioni binarie, come una risposta \"sì\" o \"no\", oppure un \"successo\" o \"insuccesso\".\n\nCalcolando il valore atteso e la varianza, otteniamo:\n\n$$\n\\begin{aligned}\n\\mathbb{E}(X) &= 0 \\cdot P(X=0) + 1 \\cdot P(X=1) = p, \\\\\n\\mathbb{V}(X) &= (0 - p)^2 \\cdot P(X=0) + (1 - p)^2 \\cdot P(X=1) = p(1-p).\n\\end{aligned}\n$$ {#eq-ev-var-bern}\n\n::: {.callout-note}\n## Dimostrazione\n\nEsaminiamo la dimostrazione algebrica del calcolo della varianza.\n\nEspandiamo il calcolo della somma, considerando i due possibili valori di $X$ (0 e 1).\n\n1. **Primo termine ($X = 0$):**\n\n   $$\n   (0 - \\mathbb{E}(X))^2 \\cdot P(X = 0) = (0 - p)^2 \\cdot (1 - p).\n   $$\n\n   Semplificando $(0 - p)^2 = p^2$, quindi:\n\n   $$\n   (0 - \\mathbb{E}(X))^2 \\cdot P(X = 0) = p^2 \\cdot (1 - p).\n   $$\n\n2. **Secondo termine ($X = 1$):**\n\n   $$\n   (1 - \\mathbb{E}(X))^2 \\cdot P(X = 1) = (1 - p)^2 \\cdot p.\n   $$\n\n   Semplificando $(1 - p)^2 = 1 - 2p + p^2$, quindi:\n\n   $$\n   (1 - \\mathbb{E}(X))^2 \\cdot P(X = 1) = (1 - 2p + p^2) \\cdot p = p - 2p^2 + p^3.\n   $$\n\n3. **Somma dei termini**\n\n   Ora sommiamo i due contributi:\n\n   $$\n   \\mathbb{V}(X) = p^2 \\cdot (1 - p) + (p - 2p^2 + p^3).\n   $$\n\n   Espandendo il primo termine:\n\n   $$\n   p^2 \\cdot (1 - p) = p^2 - p^3.\n   $$\n\n   Somma completa:\n\n   $$\n   \\mathbb{V}(X) = (p^2 - p^3) + (p - 2p^2 + p^3).\n   $$\n\n4. **Raggruppiamo i termini**\n\n   $$\n   \\mathbb{V}(X) = p - p^2.\n   $$\n\n   **Risultato finale:**\n\n   $$\n   \\mathbb{V}(X) = p(1 - p).\n   $$\n\nIn sintesi, la varianza di una variabile aleatoria binaria $X$, distribuita secondo Bernoulli con parametro $p$, è data da $p(1-p)$. \n:::\n\nTale risultato mostra come la varianza massima si ottenga per $p = 0.5$, condizione che corrisponde alla massima incertezza intrinseca nel processo, ossia quando la probabilità di successo eguaglia quella di insuccesso.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Valori di p tra 0 e 1\np <- seq(0, 1, length.out = 100)\nvariance <- p * (1 - p)\ndata <- data.frame(p = p, Variance = variance)\n\n# Creazione del grafico\nggplot(data, aes(x = p, y = Variance)) +\n  geom_line(linewidth = 1.2) +\n  labs(\n    x = expression(p),\n    y = \"Varianza\"\n  )\n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Notazione\n\nPer indicare che la variabile casuale $X$ segue una distribuzione Bernoulliana di parametro $p$ Utilizziamo la notazione $X \\sim \\mathcal{Bern}(p)$, o in maniera equivalente $\\mathcal{Bern}(X \\mid p)$.\n\n::: {#exm-}\nNel caso del lancio di una moneta equilibrata, la variabile di Bernoulli assume i valori $0$ e $1$ con uguale probabilità di $\\frac{1}{2}$. Pertanto, la funzione di massa di probabilità assegna una probabilità di $\\frac{1}{2}$ sia per $X = 0$ che per $X = 1$, mentre la funzione di distribuzione cumulativa risulta essere $\\frac{1}{2}$ per $X = 0$ e $1$ per $X = 1$.\n\nGeneriamo dei valori casuali dalla distribuzione di Bernoulli. Iniziamo con un singolo valore:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Probabilità di successo\np <- 0.5\n\n# Genera un singolo valore\nbernoulli_sample <- rbinom(n = 1, size = 1, prob = p)\nprint(bernoulli_sample)\n#> [1] 0\n\n# Genera un campione di 10 valori\nbernoulli_sample <- rbinom(n = 10, size = 1, prob = p)\nprint(bernoulli_sample)\n#>  [1] 0 1 0 1 0 0 0 1 0 0\n```\n:::\n\n:::\n\n\n\n## Distribuzione Binomiale\n\nLa distribuzione binomiale è una distribuzione di probabilità discreta che modella il numero di successi $y$ in un numero fissato $n$ di prove di Bernoulli indipendenti e identiche, dove ciascuna prova ha solo due esiti possibili: \"successo\" (rappresentato da \"1\") con probabilità $p$ o \"insuccesso\" (rappresentato da \"0\") con probabilità $1 - p$. La notazione utilizzata è la seguente:\n\n$$\nY \\sim \\mathcal{Binom}(n, p).\n$$\n\n::: {#def-}\nLa distribuzione binomiale descrive la probabilità di osservare esattamente $y$ successi in $n$ prove di Bernoulli indipendenti:\n\n$$\nP(Y = y) = \\binom{n}{y} p^{y} (1 - p)^{n - y} = \\frac{n!}{y!(n - y)!} p^{y} (1 - p)^{n - y},\n$$ {#eq-binom-distr}\n\ndove $\\binom{n}{y}$, noto come coefficiente binomiale, rappresenta il numero di modi possibili per ottenere $y$ successi in $n$ prove, e $p$ è la probabilità di successo in ciascuna prova.\n:::\n\nLa distribuzione binomiale si presta bene a esempi classici come il lancio ripetuto di una moneta o l'estrazione di biglie da un'urna. Ad esempio, nel caso del lancio di una moneta, questa distribuzione descrive la probabilità di ottenere un determinato numero di \"teste\" in un certo numero di lanci, con ogni lancio che segue una distribuzione di Bernoulli con probabilità di successo $p$.\n\nUna caratteristica interessante della distribuzione binomiale è la sua *proprietà di riproducibilità*: se due variabili casuali indipendenti, $y_1$ e $y_2$, seguono entrambe distribuzioni binomiali con lo stesso parametro $p$, ma con un diverso numero di prove ($n_1$ e $n_2$), la loro somma, $y = y_1 + y_2$, sarà ancora distribuita binomialmente, con parametri $n_1 + n_2$ e $p$.\n\n::: callout-note\n## Dimostrazione\n\nPer chiarire il calcolo delle probabilità nella **distribuzione binomiale**, consideriamo una serie di prove di **Bernoulli**. Supponiamo di avere $n$ prove indipendenti, ciascuna con probabilità $p$ di successo, e di osservare esattamente $y$ successi.\n\nUna possibile configurazione dei risultati può essere rappresentata come:\n\n$$\n\\overbrace{SS\\dots S}^\\text{$y$ successi} \\, \\overbrace{II\\dots I}^\\text{$n - y$ insuccessi}\n$$\n\nLa probabilità di ottenere **esattamente $y$ successi in una sequenza specifica** (cioè in un ordine fissato) è:\n\n$$\np^y \\cdot (1 - p)^{n - y},\n$$\n\ndove $p^y$ è la probabilità dei $y$ successi e $(1 - p)^{n - y}$ quella dei $n - y$ insuccessi.\n\nTuttavia, ciò che ci interessa è la **probabilità complessiva** di ottenere $y$ successi in *qualsiasi ordine*. In altre parole, vogliamo calcolare la probabilità dell'**unione** di tutte le possibili sequenze di $n$ prove che contengono esattamente $y$ successi.\n\nIl numero di tali sequenze è dato dal **coefficiente binomiale** $\\binom{n}{y}$, che rappresenta il numero di modi diversi in cui possiamo scegliere le $y$ posizioni dei successi tra le $n$ prove.\n\nMoltiplicando la probabilità di una singola sequenza per il numero totale di sequenze possibili, otteniamo la **funzione di probabilità della distribuzione binomiale**:\n\n$$\nP(Y = y) = \\binom{n}{y} p^y (1 - p)^{n - y}.\n$$\n:::\n\n### Caso particolare $n = 1$\n\nOra consideriamo il caso particolare in cui $n = 1$. Quando $n = 1$, il coefficiente binomiale diventa:\n\n$$\n\\binom{1}{y} = \\frac{1!}{y! (1-y)!}.\n$$\n\nEspandiamo i fattoriali per i due possibili valori di $y$, che può assumere solo 0 o 1 (poiché $y \\in \\{0, 1, \\dots, n\\}$).\n\n**Caso 1: $y = 0$**\n\n$$\n\\binom{1}{0} = \\frac{1!}{0! (1-0)!} = \\frac{1}{1 \\cdot 1} = 1.\n$$\n\nQuindi, per $y = 0$:\n$$\nP(Y = 0) = \\binom{1}{0} p^0 (1-p)^{1-0} = 1 \\cdot 1 \\cdot (1-p) = 1-p.\n$$\n\n**Caso 2: $y = 1$**\n\n$$\n\\binom{1}{1} = \\frac{1!}{1! (1-1)!} = \\frac{1}{1 \\cdot 1} = 1.\n$$\n\nQuindi, per $y = 1$:\n$$\nP(Y = 1) = \\binom{1}{1} p^1 (1-p)^{1-1} = 1 \\cdot p \\cdot 1 = p.\n$$\n\nIn conclusione, la PMF per la distribuzione binomiale con $n = 1$ diventa:\n\n$$\nP(Y = y) =\n\\begin{cases}\n1-p, & \\text{se } y = 0, \\\\\np, & \\text{se } y = 1.\n\\end{cases}\n$$\n\nQuesta è esattamente la PMF della distribuzione di Bernoulli con parametro $p$:\n\n$$\nP(Y = y) = p^y (1-p)^{1-y}, \\quad y \\in \\{0, 1\\}.\n$$ \n\nPertanto, la distribuzione binomiale con $n = 1$ è equivalente alla distribuzione di Bernoulli con parametro $p$.\n\n### Applicazioni Pratiche della Distribuzione Binomiale\n\nPer illustrare l’applicazione della distribuzione binomiale, consideriamo un esempio semplice. Supponiamo di osservare **2 successi su 4 prove di Bernoulli**, dove la probabilità di successo in ogni prova è $p = 0.2$. La probabilità di ottenere esattamente questo risultato si calcola con la formula:\n\n$$\nP(Y = 2) = \\binom{4}{2} \\cdot 0.2^2 \\cdot (1 - 0.2)^{2} = 0.1536.\n$$\n\nIn R, questo calcolo si può fare in modo diretto:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri\nn <- 4\np <- 0.2\ny <- 2\n\n# Calcolo della probabilità esatta\nprob <- choose(n, y) * p^y * (1 - p)^(n - y)\nprint(prob)\n#> [1] 0.154\n```\n:::\n\n\nIn alternativa, possiamo usare la funzione `dbinom()` per ottenere la stessa probabilità:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprob <- dbinom(x = y, size = n, prob = p)\nprint(prob)\n#> [1] 0.154\n```\n:::\n\n\n#### Visualizzazione della distribuzione di probabilità\n\nPossiamo rappresentare graficamente la **distribuzione di massa di probabilità** per tutti i possibili valori di $y$ da $0$ a $n$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny <- 0:n\nprobabilities <- dbinom(y, size = n, prob = p)\n\ndf <- data.frame(Successi = y, Probabilità = probabilities)\n\ndf |>\n  ggplot(aes(x = Successi, y = Probabilità)) +\n    geom_segment(\n      aes(xend = Successi, yend = 0), lwd = 1.2\n      ) +\n    geom_point(size = 3) +\n    labs(\n      x = \"Numero di successi y\",\n      y = \"Probabilità\"\n    )\n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n#### Generazione di un campione casuale\n\nLa funzione `rbinom()` permette di generare un campione casuale da una distribuzione binomiale:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\nsamples <- rbinom(n = 30, size = 5, prob = 0.5)\nprint(samples)\n#>  [1] 4 4 2 4 3 3 3 1 3 3 2 3 4 2 2 4 5 1 2 3 4 1 5 4 1 3 2 4 2 4\n```\n:::\n\n\n#### Variazione della distribuzione al variare di $p$\n\nPer esplorare l’effetto di diversi valori di $p$ sulla forma della distribuzione, possiamo visualizzare più curve binomiali per $n = 20$ e $p$ variabile:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 20\np_values <- seq(0.3, 0.9, by = 0.3)\ny <- 0:25\n\ndf <- data.frame()\n\nfor (p in p_values) {\n  binom_dist <- dbinom(y, size = n, prob = p)\n  df <- rbind(df, data.frame(y = y, Prob = binom_dist, p = factor(p)))\n}\n\ndf |>\n  ggplot(aes(x = y, y = Prob, color = p)) +\n    geom_point() +\n    geom_line() +\n    labs(\n      x = \"Numero di successi y\",\n      y = \"Probabilità\",\n      color = expression(p)\n    )\n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n#### Funzione di ripartizione cumulativa\n\nPossiamo anche rappresentare la **funzione di distribuzione cumulativa** (CDF) per $n = 5$ e $p = 0.5$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 5\np <- 0.5\ny <- 0:n\n\ncdf_values <- pbinom(y, size = n, prob = p)\ndf <- data.frame(y = y, cdf = cdf_values)\n\ndf |>\n  ggplot(aes(x = y, y = cdf)) +\n    geom_line() +\n    geom_point() +\n    geom_hline(\n      yintercept = 1, linetype = \"dashed\", color = \"black\", alpha = 0.7\n    ) +\n    labs(\n      x = \"Numero di successi y\",\n      y = \"Probabilità cumulativa\"\n    )\n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n::: {#exm-}\nSupponiamo di lanciare una **moneta equa** (cioè con probabilità $p = 0.5$ di ottenere testa) **5 volte**. Vogliamo calcolare la probabilità di ottenere **almeno 2 teste**, ovvero:\n\n$$\nP(Y \\geq 2) = P(Y = 2) + P(Y = 3) + P(Y = 4) + P(Y = 5).\n$$\n\nPossiamo sommare direttamente queste probabilità usando `dbinom()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresult <- sum(dbinom(2:5, size = 5, prob = 0.5))\nprint(result)\n#> [1] 0.812\n```\n:::\n\n\nUn modo alternativo, più efficiente, consiste nel calcolare il **complemento** della probabilità di ottenere **meno di 2 teste** (cioè 0 o 1):\n\n$$\nP(Y \\geq 2) = 1 - P(Y \\leq 1)\n$$\n\nIn R, possiamo usare la funzione `pbinom()` per calcolare questa probabilità cumulativa:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresult <- 1 - pbinom(q = 1, size = 5, prob = 0.5)\nprint(result)\n#> [1] 0.812\n```\n:::\n\n\nEntrambi i metodi restituiscono lo stesso risultato numerico, ma il secondo è spesso preferibile quando $n$ è grande o quando si vuole calcolare una probabilità di coda.\n:::\n\n#### Quantili di una distribuzione binomiale\n\nHai perfettamente ragione — grazie per l'osservazione!\n\nInfatti, con i parametri `size = 5`, `prob = 0.5` e `target_probability = 0.60`, la funzione `qbinom()` restituisce **3**, non **2**. Questo perché `qbinom()` restituisce **il più piccolo valore di $y$ tale che $P(Y \\leq y) \\geq p$**. Verifichiamolo in R:\n\n```r\npbinom(2, 5, 0.5)  # = 0.5\npbinom(3, 5, 0.5)  # = 0.8125\n```\n\nQuindi:\n\n- $P(Y \\leq 2) = 0.5$ → troppo poco\n- $P(Y \\leq 3) = 0.8125$ → supera il 60%\n\nPertanto, `qbinom(0.6, 5, 0.5)` restituisce `3`.\n\n#### Quantili di una distribuzione binomiale\n\nLa funzione `qbinom()` permette di calcolare il **quantile** di una distribuzione binomiale, cioè il **numero minimo di successi** $y$ tale che la probabilità cumulativa $P(Y \\leq y)$ sia **maggiore o uguale** a una certa soglia.\n\nAd esempio, supponiamo di voler sapere **qual è il numero minimo di successi** tale che la probabilità cumulativa sia **almeno 60%**. Possiamo usare:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Probabilità cumulativa desiderata\ntarget_probability <- 0.60\n\n# Calcolo del quantile\nresult <- qbinom(p = target_probability, size = 5, prob = 0.5)\nprint(result)\n#> [1] 3\n```\n:::\n\n\nIl risultato è `3`, il che significa che:\n\n$$\nP(Y \\leq 3) = 0.8125 \\geq 0.60,\n$$\n\nmentre\n\n$$\nP(Y \\leq 2) = 0.5 < 0.60.\n$$\n\nQuindi, servono **almeno 3 successi** per superare la soglia del 60% di probabilità cumulativa.\n\n> 🔎 `qbinom(p, size, prob)` restituisce il **più piccolo valore di $y$** tale che $P(Y \\leq y) \\geq p$.\n\n#### Rappresentazione grafica del quantile\n\nPer visualizzare il comportamento della funzione di ripartizione cumulativa e individuare il quantile per $p = 0.60$, possiamo usare il seguente codice in R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri\nn <- 5\np <- 0.5\ntarget_probability <- 0.60\n\n# Asse y: numero di successi\ny <- 0:n\n\n# Calcolo dei valori cumulativi\ncdf <- pbinom(y, size = n, prob = p)\n\n# Calcolo del quantile\nq <- qbinom(target_probability, size = n, prob = p)\n\n# Data frame\ndf <- data.frame(Successi = y, CDF = cdf)\n\n# Grafico\ndf |>\n  ggplot(aes(x = Successi, y = CDF)) +\n  geom_step(direction = \"hv\", linewidth = 1.1) +\n  geom_point(size = 2) +\n  geom_hline(\n    yintercept = target_probability, linetype = \"dashed\", color = \"red\"\n  ) +\n  geom_vline(xintercept = q, linetype = \"dotted\", color = \"blue\") +\n  annotate(\n    \"text\",\n    x = q + 0.4, y = 0.05, label = paste(\"quantile =\", q),\n    color = \"blue\"\n  ) +\n  annotate(\n    \"text\",\n    x = 0.5, y = target_probability + 0.05,\n    label = paste(\"soglia =\", target_probability), color = \"red\"\n  ) +\n  labs(\n    x = \"Numero di successi\",\n    y = \"Probabilità cumulativa\"\n  ) +\n  ylim(0, 1.05)\n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIn questo grafico:\n\n- la **linea rossa tratteggiata** rappresenta la soglia di probabilità desiderata (es. 0.60);\n- la **linea blu tratteggiata verticale** indica il quantile corrispondente, cioè il più piccolo valore di $y$ per cui $P(Y \\leq y) \\geq 0.60$;\n- il valore calcolato è `3`, quindi con **al massimo 3 successi**, la probabilità cumulativa supera il 60%.\n\n::: {#exm-}\nConsideriamo una **distribuzione binomiale** con $n = 10$ prove e probabilità di successo $p = 0.2$. Supponiamo di voler calcolare la probabilità di ottenere **al massimo 4 successi**. In termini matematici, vogliamo calcolare:\n\n$$\nP(Y \\leq 4) .\n$$\n\nIn R, questo si ottiene con la funzione `pbinom()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo della probabilità cumulativa fino a 4 successi\np_cumulativa <- pbinom(4, size = 10, prob = 0.2)\nprint(p_cumulativa)\n#> [1] 0.967\n```\n:::\n\n\nIl risultato indica che c'è circa l'**97%** di probabilità di ottenere 4 o meno successi su 10 prove, quando la probabilità di successo in ciascuna prova è 0.2.\n\nOra facciamo il **passaggio inverso**: immaginiamo di conoscere la probabilità cumulativa (per esempio, 0.97) e vogliamo sapere **quanti successi** bisogna considerare per raggiungere quella probabilità.\n\nPer questo usiamo la funzione `qbinom()`, che ci restituisce il **più piccolo numero di successi $y$ tale che $P(Y \\leq y) \\geq$ quella probabilità**:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo del numero di successi associato alla probabilità cumulativa\nnumero_successi <- qbinom(p_cumulativa, size = 10, prob = 0.2)\nprint(numero_successi)\n#> [1] 4\n```\n:::\n\n\nIl valore ottenuto sarà `4`, cioè il **minimo numero di successi** per cui la probabilità cumulativa è almeno il 97%.\n\n**Riepilogo concetti chiave**:\n\n- `pbinom(y, n, p)` calcola la probabilità di ottenere **al massimo $y$ successi**;\n- `qbinom(prob, n, p)` calcola **il numero minimo di successi** necessari per raggiungere almeno quella probabilità.\n\nIn sintesi, `pbinom()` e `qbinom()` sono strumenti complementari: `pbinom` ci dà la probabilità di ottenere fino a un certo numero di successi, mentre `qbinom` ci dice fino a quanti successi possiamo ottenere per raggiungere una certa probabilità. Nell’analisi di una distribuzione binomiale (e di molte altre distribuzioni) queste funzioni aiutano a calcolare e interpretare facilmente probabilità cumulate e quantili in R, rendendo più semplice l’analisi di eventi aleatori.\n:::\n\n### Valore atteso e deviazione standard nella distribuzione binomiale \n\nNella distribuzione binomiale, possiamo calcolare facilmente due quantità molto importanti:\n\n- **il valore atteso** (o media), che ci dice **quanti successi ci aspettiamo in media** su un certo numero di prove;\n- **la deviazione standard**, che ci dice **quanto i risultati tendono a variare** attorno alla media.\n\nLe formule sono le seguenti:\n\n$$\n\\text{Media (valore atteso):} \\quad \\mu = n p ,\n$$ {#eq-binom-distr-expval}\n\n$$\n\\text{Deviazione standard:} \\quad \\sigma = \\sqrt{n p (1 - p)} ,\n$$ {#eq-binom-distr-var}\n\ndove:\n\n- $n$ è il numero di prove (per esempio, il numero di lanci di una moneta),\n- $p$ è la probabilità di successo in ogni prova.\n\n::: callout-note\n**Dimostrazione.**\n\nLa variabile $Y$ rappresenta il numero di successi in $n$ prove di Bernoulli indipendenti. Possiamo scriverla come somma di $n$ variabili casuali indipendenti:\n\n$$\nY = Y_1 + Y_2 + \\cdots + Y_n,\n$$\n\ndove ciascuna $Y_i \\sim \\text{Bernoulli}(p)$, cioè:\n\n$$\nY_i =\n\\begin{cases}\n1 & \\text{con probabilità } p \\\\\n0 & \\text{con probabilità } 1 - p\n\\end{cases}\n$$\n\n**Valore atteso di $Y_i$.**\n\nPer definizione del valore atteso:\n\n$$\n\\mathbb{E}(Y_i) = 1 \\cdot p + 0 \\cdot (1 - p) = p.\n$$\n\n**Valore atteso di $Y_i^2$.**\n\nPoiché $Y_i$ assume solo i valori 0 e 1, si ha $Y_i^2 = Y_i$. Quindi:\n\n$$\n\\mathbb{E}(Y_i^2) = \\mathbb{E}(Y_i) = p.\n$$\n\nLa **varianza** di una variabile casuale si definisce come:\n\n$$\n\\operatorname{Var}(Y_i) = \\mathbb{E}(Y_i^2) - [\\mathbb{E}(Y_i)]^2.\n$$\n\nSostituendo i valori trovati sopra:\n\n$$\n\\operatorname{Var}(Y_i) = p - p^2 = p(1 - p).\n$$\n\nRicordiamo che $Y = \\sum_{i=1}^{n} Y_i$ e che le $Y_i$ sono **indipendenti**. Una proprietà fondamentale della varianza è che se $Z_1, \\dots, Z_n$ sono indipendenti:\n\n$$\n\\operatorname{Var}(Z_1 + \\cdots + Z_n) = \\operatorname{Var}(Z_1) + \\cdots + \\operatorname{Var}(Z_n).\n$$\n\nApplichiamola al nostro caso:\n\n$$\n\\operatorname{Var}(Y) = \\sum_{i=1}^{n} \\operatorname{Var}(Y_i).\n$$\n\nPoiché tutte le $Y_i$ hanno la stessa varianza $p(1 - p)$, la somma diventa:\n\n$$\n\\operatorname{Var}(Y) = n \\cdot p(1 - p).\n$$\n\nAbbiamo dimostrato da definizione che, se $Y \\sim \\text{Bin}(n, p)$, allora:\n\n$$\n\\operatorname{Var}(Y) = n \\cdot p \\cdot (1 - p).\n$$\n\nQuesta formula descrive la dispersione attesa nel numero di successi su $n$ prove indipendenti, ciascuna con probabilità di successo $p$.\n:::\n\n::: {#exm-binom-var}\nSupponiamo di lanciare **4 volte una moneta truccata** che ha una probabilità di successo (es. ottenere *testa*) pari a $p = 0.2$.\n\nVogliamo calcolare:\n\n- la media attesa del numero di teste,\n- la varianza,\n- e la deviazione standard.\n\n1. Calcolo del valore atteso (media):\n\n$$\n\\mu = n \\cdot p = 4 \\cdot 0.2 = 0.8 .\n$$\n\nQuindi, **in media**, ci aspettiamo di ottenere **0.8 teste ogni 4 lanci** (cioè meno di 1, ma ricordiamo che si tratta di una **media**).\n\n2. Calcolo della varianza:\n\n$$\n\\text{Varianza} = n \\cdot p \\cdot (1 - p) = 4 \\cdot 0.2 \\cdot 0.8 = 0.64 .\n$$\n\n3. Calcolo della deviazione standard:\n\n$$\n\\sigma = \\sqrt{0.64} \\approx 0.8 .\n$$\n\nLa deviazione standard ci dà un'idea della **variabilità** dei risultati: in questo caso, i valori osservati (numero di teste su 4 lanci) si discostano dalla media di circa 0.8 in media.\n:::\n\n### Verifica con una simulazione in R\n\nPer vedere se i calcoli teorici dell'@exm-binom-var funzionano anche nella pratica, possiamo **simulare** l’esperimento in R: lanciamo 4 monete, ma lo facciamo **tantissime volte** (ad esempio 1 milione) e calcoliamo la media e la varianza dei risultati ottenuti.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\n\n# Generiamo 1 milione di esperimenti: 4 lanci con probabilità di successo 0.2\nx <- rbinom(n = 1e6, size = 4, prob = 0.2)\n\n# Calcoliamo la media empirica\nmean(x)\n#> [1] 0.8\n# [1] circa 0.8\n\n# Calcoliamo la varianza empirica\nvar(x)\n#> [1] 0.639\n# [1] circa 0.64\n```\n:::\n\n\nCome possiamo vedere, i risultati ottenuti dalla simulazione sono **molto vicini ai valori teorici**: la media è circa $\\mu = 0.8$ e la varianza circa $0.64$, proprio come previsto dalle formule.\n\nQuesto non solo conferma che le **formule per media e varianza nella distribuzione binomiale sono corrette**, ma ci aiuta anche a capire meglio **cosa significano**: \n\n- il **valore atteso** rappresenta la **media dei risultati** se ripetiamo l'esperimento moltissime volte;  \n- la **varianza** (e la sua radice quadrata, la **deviazione standard**) misura **quanto i risultati si allontanano dalla media**.\n\nLa simulazione mostra quindi in modo concreto che **il valore atteso e la varianza descrivono il comportamento \"medio\" della variabile aleatoria**, quando viene osservata in un numero molto grande di situazioni. In altre parole, questi concetti non sono solo teorici: ci dicono cosa aspettarci nella pratica, se ripetiamo molte volte lo stesso esperimento.\n\n\n## Funzioni R per le distribuzioni di probabilità\n\nIn R, le distribuzioni di probabilità (sia discrete che continue) sono gestite in modo sistematico. Per ogni distribuzione, esistono quattro funzioni principali, ognuna con un prefisso diverso che indica il tipo di operazione desiderata:\n\n- **`d*`**: calcola la *densità* (per distribuzioni continue) o la *probabilità* (per distribuzioni discrete);  \n- **`p*`**: calcola la *funzione di ripartizione cumulativa* (CDF), cioè $P(Y \\leq y)$;  \n- **`q*`**: calcola la *funzione quantile* (inversa della CDF);  \n- **`r*`**: genera *valori casuali* secondo la distribuzione specificata.\n\nQuesta struttura è identica per tutte le distribuzioni implementate in R. La tabella seguente mostra un confronto tra le funzioni disponibili per due distribuzioni fondamentali: la **binomiale** (discreta) e la **normale** (continua).\n\n| Tipo di funzione                   | Binomiale ($Y \\sim \\text{Bin}(n, p)$)         | Normale ($Y \\sim \\mathcal{N}(\\mu, \\sigma)$)         |\n|:----------------------------------|:----------------------------------------------|:--------------------------------------------------|\n| Densità o probabilità esatta      | `dbinom(y, size = n, prob = p)`               | `dnorm(y, mean = mu, sd = sigma)`                 |\n| $P(Y = y)$                        | `dbinom(...)`                                 | ❌ Non definita: per variabili continue si usa la densità |\n| Probabilità cumulativa            | `pbinom(y, size = n, prob = p)`               | `pnorm(y, mean = mu, sd = sigma)`                |\n| $P(Y \\geq y)$                     | `1 - pbinom(y - 1, ...)`                       | `1 - pnorm(y, ...)`                               |\n| $P(y_1 < Y < y_2)$                | `pbinom(y2, ...) - pbinom(y1, ...)`            | `pnorm(y2, ...) - pnorm(y1, ...)`                |\n| Quantile (inversa della CDF)      | `qbinom(q, size = n, prob = p)`               | `qnorm(q, mean = mu, sd = sigma)`                |\n| Simulazione di dati casuali       | `rbinom(n, size = trials, prob = p)`          | `rnorm(n, mean = mu, sd = sigma)`                |\n\n::: {.callout-tip title=\"Nota\"}\n- Per le **distribuzioni discrete** (come la binomiale), `dbinom(y, ...)` restituisce la **probabilità esatta** di osservare il valore $y$: ad esempio, $P(Y = 2)$.\n- Per le **distribuzioni continue** (come la normale), `dnorm(y, ...)` restituisce la **densità** in $y$, che non rappresenta direttamente una probabilità, ma è utile per visualizzare la forma della distribuzione.\n- Le probabilità cumulative (funzioni `p*`) e i quantili (funzioni `q*`) sono sempre definiti, sia per distribuzioni discrete che continue.\n- La generazione di dati casuali con `r*` è molto utile per simulazioni e verifiche empiriche.\n\nPiù avanti, vedremo altre distribuzioni (Uniforme, Beta, Poisson, ecc.), tutte con lo stesso schema di funzioni: `d`, `p`, `q`, `r`.\n\nQuesta coerenza rende molto semplice imparare a usare le distribuzioni in R: una volta compreso lo schema, lo si può applicare a qualsiasi caso.\n:::\n\n::: {#exm-}\n\n1. Calcolare la probabilità di esattamente $y = 3$ successi su $n = 5$ prove con $p = 0.5$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndbinom(3, size = 5, prob = 0.5)\n#> [1] 0.312\n```\n:::\n\n\n2. Calcolare la probabilità cumulativa $P(Y \\leq 3)$:\n   \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npbinom(3, size = 5, prob = 0.5)\n#> [1] 0.812\n```\n:::\n\n\n3. Calcolare il valore minimo $y$ tale che $P(Y \\leq y) \\geq 0.9$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqbinom(0.9, size = 5, prob = 0.5)\n#> [1] 4\n```\n:::\n\n\n4. Generare un campione di 100 numeri casuali da una distribuzione binomiale:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrbinom(100, size = 5, prob = 0.5)\n#>   [1] 2 2 4 1 3 2 3 2 2 2 3 3 3 3 1 4 1 1 0 2 2 2 4 1 2 3 3 1 5 2 3 3 0 3 2 3 4\n#>  [38] 2 3 3 3 3 1 5 3 3 2 3 1 2 1 3 3 2 2 4 1 4 2 3 1 4 2 2 3 4 1 1 4 2 3 2 3 3\n#>  [75] 3 1 4 4 3 3 4 3 3 3 2 2 3 3 2 4 4 3 2 2 0 3 1 3 1 2\n```\n:::\n\n:::\n\n\n## Distribuzione di Poisson \n\nLa **distribuzione di Poisson** è utilizzata per modellare il numero di eventi che si verificano in un determinato intervallo di tempo o spazio, con eventi indipendenti e un tasso costante di occorrenza.\n\nLa funzione di massa di probabilità (PMF) è data da:\n\n$$\nP(Y = y \\mid \\lambda) = \\frac{\\lambda^y \\cdot e^{-\\lambda}}{y!}, \\quad y = 0, 1, 2, \\ldots\n$$\n\ndove $\\lambda$ rappresenta il tasso medio di eventi e $y$ è il numero di eventi.\n\nLa distribuzione di Poisson può essere derivata come il limite di una distribuzione binomiale quando il numero di prove, $n$, tende all'infinito e la probabilità di successo in ciascuna prova, $p$, tende a zero, in modo tale che $np = \\lambda$. \n\n::: callout-note\n## Dimostrazione\n\nPartiamo dalla funzione di probabilità binomiale:\n\n$$\np(k) = \\frac{n!}{k!(n - k)!} p^k (1 - p)^{n - k}.\n$$\n\nImpostiamo $np = \\lambda$, il che implica che $p = \\frac{\\lambda}{n}$. Sostituendo $p$ con $\\frac{\\lambda}{n}$ nella formula binomiale, otteniamo:\n\n$$\np(k) = \\frac{n!}{k!(n - k)!} \\left(\\frac{\\lambda}{n}\\right)^k \\left(1 - \\frac{\\lambda}{n}\\right)^{n - k}.\n$$\n\nOra, separiamo i termini per rendere più chiara la semplificazione. Possiamo riscrivere $\\left(\\frac{\\lambda}{n}\\right)^k$ come $\\frac{\\lambda^k}{n^k}$, e $\\left(1 - \\frac{\\lambda}{n}\\right)^{n - k}$ come $\\left(1 - \\frac{\\lambda}{n}\\right)^n \\cdot \\left(1 - \\frac{\\lambda}{n}\\right)^{-k}$. Quindi, l'espressione diventa:\n\n$$\np(k) = \\frac{n!}{k!(n - k)!} \\cdot \\frac{\\lambda^k}{n^k} \\cdot \\left(1 - \\frac{\\lambda}{n}\\right)^n \\cdot \\left(1 - \\frac{\\lambda}{n}\\right)^{-k}.\n$$\n\nOra, separiamo ulteriormente i termini:\n\n$$\np(k) = \\frac{\\lambda^k}{k!} \\cdot \\frac{n!}{(n - k)! n^k} \\cdot \\left(1 - \\frac{\\lambda}{n}\\right)^n \\cdot \\left(1 - \\frac{\\lambda}{n}\\right)^{-k}.\n$$\n\nQuesto passaggio mostra come la funzione di probabilità binomiale, sotto le condizioni $np = \\lambda$ e $n \\to \\infty$, si trasformi gradualmente nella forma che conduce alla distribuzione di Poisson.\n\nQuando $n \\to \\infty$:\n\n$$\n\\frac{\\lambda}{n} \\to 0\n$$\n\n$$\n\\frac{n!}{(n - k)! n^k} \\to 1\n$$\n\n$$\n\\left(1 - \\frac{\\lambda}{n}\\right)^n \\to e^{-\\lambda}\n$$\n\n$$\n\\left(1 - \\frac{\\lambda}{n}\\right)^{-k} \\to 1\n$$\n\nSi ottiene quindi:\n\n$$\np(k) \\to \\frac{\\lambda^k e^{-\\lambda}}{k!}\n$$\n\nche è la funzione di Poisson.\n:::\n\n::: callout-note\n## Dimostrazione\n\nAnalizziamo il limite:\n\n$$\n\\left(1 - \\frac{\\lambda}{n}\\right)^n \\to e^{-\\lambda} \\quad \\text{quando} \\quad n \\to \\infty.\n$$\n\nUn limite fondamentale in analisi matematica è:\n\n$$\n\\lim_{n \\to \\infty} \\left(1 + \\frac{a}{n}\\right)^n = e^a,\n$$\n\ndove $e$ è la base del logaritmo naturale ($e \\approx 2.71828$) e $a$ è una costante. Questo limite è alla base della definizione della funzione esponenziale.\n\nNel nostro caso, abbiamo l'espressione:\n\n$$\n\\left(1 - \\frac{\\lambda}{n}\\right)^n .\n$$\n\nNotiamo che questa è molto simile al limite notevole, ma con un segno negativo. Possiamo riscriverla come:\n\n$$\n\\left(1 - \\frac{\\lambda}{n}\\right)^n = \\left(1 + \\frac{-\\lambda}{n}\\right)^n.\n$$\n\nApplicando il limite notevole con $a = -\\lambda$, otteniamo:\n\n$$\n\\lim_{n \\to \\infty} \\left(1 + \\frac{-\\lambda}{n}\\right)^n = e^{-\\lambda}.\n$$\n\nQuindi, quando $n$ diventa molto grande, l'espressione $\\left(1 - \\frac{\\lambda}{n}\\right)^n$ si avvicina sempre di più a $e^{-\\lambda}$. \n:::\n\n### Proprietà principali\n\n- **Media**: $\\mathbb{E}[Y] = \\lambda$\n- **Varianza**: $\\text{Var}(Y) = \\lambda$\n\nDi seguito, presentiamo esempi di calcolo e simulazione con R.\n\n\n### Grafico della distribuzione di Poisson con $\\lambda = 2$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametro lambda\nlambda <- 2\n\n# Valori di y (numero di eventi)\ny <- 0:10\n\n# Calcolo delle probabilità\nprobabilities <- dpois(y, lambda = lambda)\n\n# Creazione di un dataframe per ggplot\ndata <- data.frame(\n  Numero_eventi = y,\n  Probabilita = probabilities\n)\n\n# Grafico della funzione di massa di probabilità \nggplot(data, aes(x = Numero_eventi, y = Probabilita)) +\n  geom_col() +  \n  labs(\n    x = \"Numero di eventi (k)\",\n    y = \"Probabilità\"\n  ) \n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-23-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Calcolo della probabilità per un numero specifico di eventi\n\nPer calcolare la probabilità di osservare esattamente 3 eventi con $\\lambda = 2$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprob <- dpois(3, lambda = 2)\nprint(prob)\n#> [1] 0.18\n```\n:::\n\n\n### Calcolo della probabilità cumulativa $P(Y \\leq 3)$\n\nPer calcolare $P(Y \\leq 3)$, la probabilità cumulativa:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncum_prob <- ppois(3, lambda = 2)\nprint(cum_prob)\n#> [1] 0.857\n```\n:::\n\n\n### Trovare il quantile corrispondente a una probabilità data\n\nPer trovare il numero massimo di eventi per cui la probabilità cumulativa è al massimo $0.8125$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nquantile <- qpois(0.8125, lambda = 2)\nprint(quantile)\n#> [1] 3\n```\n:::\n\n\n### Generazione di numeri casuali\n\nPer generare un campione di 1.000.000 di osservazioni da una distribuzione di Poisson con $\\lambda = 2$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\nsample <- rpois(1000000, lambda = 2)\n\n# Calcolo di media e varianza del campione\nmean_sample <- mean(sample)\nvar_sample <- var(sample)\n\nprint(mean_sample)\n#> [1] 2\nprint(var_sample)\n#> [1] 2\n```\n:::\n\n\n\n\n::: {#exm-poisson-v2-london}\n\nUn esempio classico dell'uso della distribuzione di Poisson viene dalla **Seconda Guerra Mondiale**.\n\n**Il contesto storico.** Tra il 1944 e il 1945, Londra fu colpita da centinaia di **missili V1 e V2** lanciati dalla Germania nazista. Le autorità britanniche si chiesero se i bombardamenti seguissero una **strategia mirata**: i missili venivano forse lanciati intenzionalmente su certi quartieri? O si trattava invece di **un comportamento casuale**, come se fossero stati distribuiti a caso?\n\nPer rispondere a questa domanda, il Ministero della Guerra britannico **divise Londra in 576 aree di uguale superficie** (ogni area misurava 0.25 km²) e **registrò quanti missili avevano colpito ciascuna area**. I dati furono poi analizzati dal matematico R. D. Clarke, che li pubblicò nel 1946.\n\n**I dati osservati.** Ecco una sintesi della distribuzione osservata:\n\n| Missili per area | Numero di aree | Frequenza relativa |\n|------------------|----------------|---------------------|\n| 0                | 229            | 0.398               |\n| 1                | 211            | 0.367               |\n| 2                | 93             | 0.161               |\n| 3                | 35             | 0.061               |\n| 4                | 7              | 0.012               |\n| ≥5               | 1              | 0.002               |\n\nIl **numero medio di missili per area** era $\\lambda \\approx 0.93$. L'idea era confrontare queste frequenze con le probabilità teoriche previste da una distribuzione di Poisson con media $\\lambda = 0.93$.\n\n**Interpretazione con la distribuzione di Poisson.** Utilizzando la funzione `dpois()` in R, possiamo calcolare le probabilità teoriche per ciascun valore osservato, da 0 a 4 missili per area (valori superiori sono troppo rari per essere trattati separatamente).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametro medio osservato\nlambda <- 0.93\n\n# Valori possibili di missili per area\ny <- 0:4\n\n# Probabilità teoriche secondo la distribuzione di Poisson\nprob_teoriche <- dpois(y, lambda = lambda)\n\n# Aggiungiamo la probabilità per y >= 5\nprob_teoriche <- c(prob_teoriche, 1 - sum(prob_teoriche))  # y >= 5\n\n# Visualizziamo\ndata.frame(\n  Missili_per_area = c(0:4, \">=5\"),\n  Probabilita_teorica = round(prob_teoriche, 3)\n)\n#>   Missili_per_area Probabilita_teorica\n#> 1                0               0.395\n#> 2                1               0.367\n#> 3                2               0.171\n#> 4                3               0.053\n#> 5                4               0.012\n#> 6              >=5               0.003\n```\n:::\n\n\nConfrontando le probabilità teoriche della Poisson con quelle osservate nei dati reali, **i risultati erano sorprendentemente simili**. Questo suggeriva che **i missili non erano lanciati su bersagli specifici**, ma seguivano un comportamento **statisticamente compatibile con una distribuzione casuale**.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Frequenze osservate (dati originali di Clarke, 1946)\nfrequenze_osservate <- c(229, 211, 93, 35, 7, 1)\nvalori_missili <- c(0, 1, 2, 3, 4, \"≥5\")\n\n# Calcolo frequenze teoriche con Poisson (lambda = 0.93)\nlambda <- 0.93\nprob_teoriche <- dpois(0:4, lambda)\nprob_teoriche <- c(prob_teoriche, 1 - sum(prob_teoriche))  # Per y >= 5\n\n# Numero totale di aree (come somma delle osservazioni)\nn_aree <- sum(frequenze_osservate)\n\n# Frequenze attese = probabilità teoriche * numero totale di aree\nfrequenze_attese <- round(prob_teoriche * n_aree)\n\n# Costruzione del data frame\ndf <- data.frame(\n  Missili_per_area = factor(valori_missili, levels = c(\"0\", \"1\", \"2\", \"3\", \"4\", \"≥5\")),\n  Osservate = frequenze_osservate,\n  Attese = frequenze_attese\n)\n\n# Conversione in formato lungo per ggplot2\ndf_long <- reshape2::melt(df, id.vars = \"Missili_per_area\", variable.name = \"Tipo\", value.name = \"Frequenza\")\n\n# Creazione del grafico\nggplot(df_long, aes(x = Missili_per_area, y = Frequenza, fill = Tipo)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    x = \"Numero di missili per area\",\n    y = \"Numero di aree\",\n    fill = \"Frequenza\"\n  ) \n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n- Le **barre blu** rappresentano le **frequenze osservate** (quante aree hanno ricevuto 0, 1, 2... missili).\n- Le **barre rosse** mostrano le **frequenze attese** se i missili fossero stati lanciati in modo **completamente casuale**, seguendo una distribuzione di **Poisson con $\\lambda = 0.93$**.\n\nLa sovrapposizione tra i due andamenti è molto buona, il che rafforza l’idea che i bombardamenti fossero distribuiti casualmente — **senza un pattern strategico** apparente.\n\n**Cosa ci insegna questo esempio?**\n\n- La distribuzione di Poisson è adatta quando vogliamo **modellare eventi rari e indipendenti** nello spazio o nel tempo.\n- I dati dei missili su Londra mostrano come un fenomeno che a prima vista potrebbe sembrare non casuale (per via della concentrazione locale degli eventi) possa invece essere **ben descritto da un modello probabilistico semplice**, se considerato su una scala adatta.\n:::\n\n:::{#exm-}\nSupponiamo di avere osservato, nel corso degli anni, che la **frequenza relativa di bocciature** all’esame di **Psicometria** è di circa **10%** (cioè $p = 0.1$). Tuttavia, il **numero di studenti iscritti a ciascun appello varia in modo estremo**: al primo appello dell’anno partecipano più di 200 studenti, mentre negli ultimi appelli solo 2 o 3.\n\nQuesto rende **inadeguato l’uso della distribuzione binomiale**, che richiede un numero di prove ($n$) fisso o noto per ciascun appello.\n\nIn questi casi, possiamo **modellare il numero di bocciature per appello** usando una distribuzione di Poisson.\n\nPer ogni appello, possiamo stimare $\\lambda$ moltiplicando il **numero di studenti iscritti** ($n$) per la **frequenza attesa di bocciature** ($p = 0.1$). A quel punto, il numero di bocciature osservate può essere approssimato da:\n\n$$\nY \\sim \\text{Poisson}(\\lambda = n \\cdot p) .\n$$\n\nQuindi la distribuzione cambia da appello ad appello, perché **$\\lambda$ cambia con $n$**, ma il **modello rimane Poissoniano**.\n\nSupponiamo di avere osservato i seguenti dati.\n\n| Appello | Numero iscritti ($n$) | $\\lambda = n \\cdot p$ | Distribuzione di bocciature |\n|--------:|-----------------------:|-----------------------:|-----------------------------|\n| 1       | 220                   | $220 \\cdot 0.1 = 22$   | $Y \\sim \\text{Poisson}(22)$ |\n| 2       | 95                    | $95 \\cdot 0.1 = 9.5$    | $Y \\sim \\text{Poisson}(9.5)$ |\n| 8       | 3                     | $3 \\cdot 0.1 = 0.3$     | $Y \\sim \\text{Poisson}(0.3)$ |\n\nPer ogni appello, possiamo usare la funzione `dpois()` in R per calcolare la probabilità di osservare un certo numero di bocciature, dato il valore di $\\lambda$ specifico per quell’appello.\n\nAd esempio, possiamo chiederci quale sia la probabilità che, nel secondo appello (95 iscritti), si registrino esattamente 8 bocciature.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlambda <- 95 * 0.1  # = 9.5\ndpois(8, lambda = lambda)\n#> [1] 0.123\n```\n:::\n\n\nQuesta funzione calcola $P(Y = 8)$ per una variabile $Y \\sim \\text{Poisson}(9.5)$, cioè la probabilità di osservare esattamente 8 bocciature su 95 iscritti.\n\nSupponiamo di voler simulare il numero di bocciature in **8 appelli** con numeri di iscritti variabili. Possiamo fare così:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\n\n# Numero iscritti per ciascun appello\nn_iscritti <- c(220, 95, 60, 45, 20, 12, 6, 3)\n\n# Probabilità storica di bocciatura\np <- 0.1\n\n# Parametri lambda per ogni appello\nlambda <- n_iscritti * p\n\n# Simulazione delle bocciature per ciascun appello\nbocciature <- rpois(length(lambda), lambda = lambda)\n\ndata.frame(\n  Appello = 1:8, \n  Iscritti = n_iscritti, \n  Lambda = lambda, \n  Bocciature = bocciature\n)\n#>   Appello Iscritti Lambda Bocciature\n#> 1       1      220   22.0         28\n#> 2       2       95    9.5          8\n#> 3       3       60    6.0          8\n#> 4       4       45    4.5          5\n#> 5       5       20    2.0          2\n#> 6       6       12    1.2          2\n#> 7       7        6    0.6          0\n#> 8       8        3    0.3          0\n```\n:::\n\n\n📈 **Visualizzazione.**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf <- data.frame(Appello = factor(1:8), Bocciature = bocciature)\n\nggplot(df, aes(x = Appello, y = Bocciature)) +\n  geom_col() +\n  labs(\n    x = \"Appello\",\n    y = \"Numero di bocciature\"\n  )\n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-32-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIn sintesi,\n\n- quando il numero di studenti iscritti a un appello **non è noto a priori** o **varia fortemente**, non è adeguato usare la distribuzione binomiale;\n- se conosciamo la **frequenza relativa di bocciature** (es. $p = 0.1$), possiamo usare la distribuzione di **Poisson con $\\lambda = n \\cdot p$**, adattandola a ciascun appello;\n- questo approccio è particolarmente utile per fare **stima e simulazione** del numero di bocciature attese, senza dover modellare tutti i singoli esiti.\n:::\n\n::: {#exm-}\nUno degli esempi più comuni per introdurre la distribuzione di Poisson riguarda il numero di **nascite giornaliere** in un ospedale.\n\nSupponiamo che, in un grande ospedale, la **media storica** sia di **4.5 nascite al giorno**. Possiamo allora descrivere il numero di nascite in un giorno con una **variabile casuale Poisson** con parametro $\\lambda = 4.5$:\n\n$$\nY \\sim \\text{Poisson}(\\lambda = 4.5) .\n$$\n\nCi chiediamo, ad esempio: qual è la probabilità che in un giorno nascano **esattamente 6 bambini**?\n\nPossiamo calcolarla con la funzione `dpois()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametro medio: 4.5 nascite al giorno\nlambda <- 4.5\n\n# Probabilità di osservare esattamente 6 nascite\nprob <- dpois(6, lambda = lambda)\nprint(prob)\n#> [1] 0.128\n```\n:::\n\n\nQuesto valore rappresenta la probabilità che, in un giorno qualsiasi, si verifichino **esattamente 6 nascite**.\n\n**Simulazione.** Simuliamo ora il numero di nascite in **365 giorni consecutivi**, supponendo che la media rimanga costante a 4.5:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)  # Per rendere i risultati riproducibili\n\nn_days <- 365\nsimulated_births <- rpois(n_days, lambda = lambda)\n\n# Proporzione di giorni con esattamente 6 nascite\nproportion_six_births <- mean(simulated_births == 6)\nprint(proportion_six_births)\n#> [1] 0.14\n```\n:::\n\n\nQuesto ci dice, tra i 365 giorni simulati, **quanta parte dell’anno ha avuto esattamente 6 nascite**. Il valore ottenuto può essere confrontato con la probabilità teorica calcolata prima.\n\n**Visualizzazione.** Possiamo rappresentiamo graficamente i dati simulati con un istogramma:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Costruzione del data frame\ndata <- data.frame(Nascite = simulated_births)\n\n# Istogramma\nggplot(data, aes(x = Nascite)) +\n  geom_histogram(\n    breaks = seq(-0.5, max(simulated_births) + 0.5, by = 1)\n  ) +\n  labs(\n    x = \"Numero di nascite per giorno\",\n    y = \"Frequenza (numero di giorni)\"\n  )\n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-35-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nL’istogramma mostra quante volte si sono verificati 0, 1, 2, ..., 10 o più nascite in un giorno, evidenziando la **variabilità naturale** attorno alla media.\n\nCalcoliamo ora quanto è probabile che si verifichino **più di 6 nascite in un giorno**.\n\nProbabilità teorica:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprob_more_than_six <- 1 - ppois(6, lambda = lambda)\nprint(prob_more_than_six)\n#> [1] 0.169\n```\n:::\n\n\nProporzione osservata nella simulazione:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nproportion_more_than_six <- mean(simulated_births > 6)\nprint(proportion_more_than_six)\n#> [1] 0.17\n```\n:::\n\n\nIl confronto tra probabilità teorica e proporzione simulata mostra come la distribuzione di Poisson **riproduca bene i fenomeni reali**, quando gli eventi sono **indipendenti**, **discreti** e **relativamente frequenti ma non troppo**.\n:::\n\n::: {#exm-}\nQuesto esempio è tratto dal celebre lavoro di **Ladislaus von Bortkiewicz** del 1898, spesso citato come una delle prime applicazioni reali della **distribuzione di Poisson**.\n\nVon Bortkiewicz studiò un evento piuttosto inusuale: le **morti causate da calci di cavallo** all’interno della **cavalleria dell’esercito prussiano**. L'obiettivo era capire se questi eventi, seppur rari, potessero essere considerati **casuali e indipendenti**, oppure se fossero distribuiti in modo irregolare e non prevedibile.\n\nPer farlo, raccolse i dati su **10 squadroni** osservati per **20 anni consecutivi**, ottenendo così **200 unità di osservazione**, che possiamo chiamare **\"squadroni-anno\"**.\n\n**I dati raccolti.** Per ogni squadrone-anno, fu registrato il **numero di morti per calci di cavallo**. I dati furono poi raggruppati per numero di decessi:\n\n| Numero di decessi annui | Frequenza osservata | Frequenza relativa | Probabilità teorica (Poisson) |\n|-------------------------|---------------------|--------------------|-------------------------------|\n| 0                       | 109                 | 0.545              | 0.543                         |\n| 1                       | 65                  | 0.325              | 0.331                         |\n| 2                       | 22                  | 0.110              | 0.101                         |\n| 3                       | 3                   | 0.015              | 0.021                         |\n| 4                       | 1                   | 0.005              | 0.003                         |\n\n- **Frequenza osservata**: Quante volte ciascun numero di decessi è stato osservato tra i 200 squadroni-anno.\n- **Frequenza relativa**: Frequenza osservata divisa per 200.\n- **Probabilità teorica**: Calcolata con la **distribuzione di Poisson con parametro $\\lambda = 0.61$**, pari alla media osservata dei decessi annui.\n\nLa distribuzione di Poisson è perfetta per questo tipo di situazione perché:\n\n- stiamo contando il numero di **eventi rari** (decessi accidentali),\n- che si verificano in **unità di tempo o spazio fisse** (lo \"squadrone-anno\"),\n- e presumiamo che questi eventi siano **indipendenti tra loro**.\n\nIn questo caso, $\\lambda = 0.61$ rappresenta il **numero medio di decessi per squadrone in un anno**. La variabilità intorno a questo valore può essere descritta dalla distribuzione di Poisson, che assegna a ciascun possibile numero di decessi (0, 1, 2, …) una **probabilità teorica**.\n\n**Confronto tra dati osservati e modello di Poisson.** Come si può notare dalla tabella, **le frequenze osservate sono sorprendentemente simili** alle probabilità teoriche ottenute dal modello di Poisson. Ad esempio:\n\n- la proporzione di squadroni-anno con **zero decessi** è 0.545, contro una probabilità teorica di 0.543;\n- per **un decesso**, la frequenza relativa è 0.325, vicina alla probabilità teorica di 0.331;\n- anche le classi meno frequenti (2, 3 e 4 decessi) sono coerenti con i valori attesi.\n\nQuesto esempio dimostra che la distribuzione di Poisson **non solo è utile per modellare eventi rari**, ma fornisce anche una buona descrizione **quantitativa** del comportamento osservato nel mondo reale.\n\nIn sintesi,\n\n- il lavoro di von Bortkiewicz è uno dei primi esempi storici di **modellizzazione di dati reali con la teoria delle probabilità**;\n- la distribuzione di Poisson si è rivelata efficace nel **descrivere un fenomeno raro, ma regolare**, suggerendo che i decessi fossero eventi **casuali e indipendenti**, non dovuti a fattori sistematici;\n- ancora oggi, questo esempio viene usato per insegnare che anche gli eventi accidentali e poco frequenti possono essere **prevedibili in media** e descritti in modo elegante da un modello probabilistico.\n\nQui di seguito viene fornito il **codice R** che riproduce l’analisi di von Bortkiewicz, calcola le **probabilità teoriche** secondo la distribuzione di Poisson con parametro $\\lambda = 0.61$ e confronta visivamente le **frequenze osservate** con le **frequenze attese**.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Dati osservati da von Bortkiewicz\ndecessi <- 0:4\nfrequenze_osservate <- c(109, 65, 22, 3, 1)\nn_total <- sum(frequenze_osservate)  # Totale = 200 squadroni-anno\n\n# Frequenze relative\nfrequenze_relative <- frequenze_osservate / n_total\n```\n:::\n\n\nCalcolo delle probabilità teoriche con la distribuzione di Poisson:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametro medio osservato\nlambda <- 0.61\n\n# Calcolo delle probabilità teoriche di Poisson\nprob_poisson <- dpois(decessi, lambda = lambda)\n```\n:::\n\n\nConfronto: osservato vs teorico.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Frequenze attese = probabilità teoriche * numero totale di casi\nfrequenze_attese <- round(prob_poisson * n_total)\n\n# Creazione del data frame per il confronto\ndf <- data.frame(\n  Decessi = factor(decessi),\n  Osservato = frequenze_osservate,\n  Atteso = frequenze_attese\n)\ndf\n#>   Decessi Osservato Atteso\n#> 1       0       109    109\n#> 2       1        65     66\n#> 3       2        22     20\n#> 4       3         3      4\n#> 5       4         1      1\n```\n:::\n\n\nVisualizzazione: confronto tra frequenze osservate e attese.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Conversione da wide a long format con pivot_longer()\ndf_long <- df |> \n  pivot_longer(\n    cols = c(Osservato, Atteso),\n    names_to = \"Tipo\",\n    values_to = \"Frequenza\"\n  )\n\n# Mostra le prime righe\nhead(df_long)\n#> # A tibble: 6 × 3\n#>   Decessi Tipo      Frequenza\n#>   <fct>   <chr>         <dbl>\n#> 1 0       Osservato       109\n#> 2 0       Atteso          109\n#> 3 1       Osservato        65\n#> 4 1       Atteso           66\n#> 5 2       Osservato        22\n#> 6 2       Atteso           20\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Grafico a barre affiancate\nggplot(df_long, aes(x = Decessi, y = Frequenza, fill = Tipo)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(\n    x = \"Numero di decessi per squadrone-anno\",\n    y = \"Frequenza\",\n    fill = \"Tipo\"\n  ) \n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-42-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n- Le **barre blu** mostrano i dati **osservati** da von Bortkiewicz.\n- Le **barre rosse** indicano le **frequenze attese** se il numero di decessi segue una distribuzione di Poisson con media $\\lambda = 0.61$.\n- La **buona corrispondenza visiva** tra le due serie supporta l’idea che i decessi siano **eventi rari, indipendenti e distribuiti casualmente**.\n\n:::\n\n## Distribuzione Beta-Binomiale\n\nLa distribuzione beta-binomiale rappresenta una estensione della distribuzione binomiale che tiene conto della variabilità nella probabilità di successo tra i vari tentativi. Viene descritta da tre parametri principali: $N$, $\\alpha$ e $\\beta$.\n\nNel dettaglio, la funzione di massa di probabilità per la distribuzione beta-binomiale è data da:\n\n$$\n\\text{BetaBinomiale}(y | N, \\alpha, \\beta) = \\binom{N}{y} \\cdot \\frac{B(y + \\alpha, N - y + \\beta)}{B(\\alpha, \\beta)},\n$$ {#eq-beta-binom-formula}\n\ndove:\n\n- $y$ indica il numero di successi osservati.\n- $N$ rappresenta il numero totale di tentativi.\n- $\\alpha$ e $\\beta$ sono i parametri della distribuzione beta, che modellano la variabilità nella probabilità di successo tra i tentativi.\n\nLa funzione $B(u, v)$, nota come funzione beta, è definita tramite l'uso della funzione gamma $\\Gamma$, secondo la formula:\n\n$$\nB(u, v) = \\frac{\\Gamma(u) \\Gamma(v)}{\\Gamma(u + v)},\n$$\n\ndove la funzione gamma $\\Gamma$ generalizza il concetto di fattoriale a numeri reali e complessi.\n\nL'importanza della distribuzione beta-binomiale deriva dalla sua capacità di modellare situazioni in cui la probabilità di successo non è fissa, ma segue una distribuzione di probabilità, specificatamente una distribuzione beta. Ciò la rende particolarmente adatta per applicazioni in cui le probabilità di successo cambiano in maniera incerta da un tentativo all'altro, come può avvenire in contesti di ricerca clinica o in studi comportamentali. Rispetto alla distribuzione binomiale, che assume una probabilità di successo costante per tutti i tentativi, la beta-binomiale offre una rappresentazione più realistica e flessibile per dati empirici che presentano variabilità nelle probabilità di successo.\n\n## Riflessioni Conclusive\n\nIn questo capitolo, abbiamo approfondito alcune delle distribuzioni discrete più importanti, ognuna con caratteristiche uniche e campi di applicazione specifici. Abbiamo iniziato con la **distribuzione di Bernoulli**, che modella esperimenti con due soli esiti possibili, per poi passare alla **distribuzione Binomiale**, che generalizza la Bernoulli considerando un numero fisso di prove indipendenti. Successivamente, abbiamo esaminato la **distribuzione di Poisson**, utile per descrivere eventi rari in un intervallo di tempo o spazio, e la **distribuzione Beta-Binomiale**, un'estensione della Binomiale che incorpora la variabilità nella probabilità di successo, rendendola particolarmente adatta per modellare situazioni in cui tale probabilità non è fissa. Infine, abbiamo discusso la **distribuzione Discreta Uniforme**, che assegna la stessa probabilità a ciascun evento in un insieme finito e discreto.\n\nQueste distribuzioni rappresentano il fondamento dell'analisi statistica discreta e trovano applicazione in numerosi ambiti. In particolare, nel contesto dell'**inferenza bayesiana**, la comprensione della distribuzione Binomiale e della sua estensione Beta-Binomiale è essenziale. Queste distribuzioni, infatti, forniscono gli strumenti necessari per l'**aggiornamento bayesiano**, un processo chiave che permette di rivedere le nostre credenze iniziali alla luce di nuovi dati. Questo concetto sarà ulteriormente esplorato nei capitoli successivi, dove approfondiremo come le distribuzioni a priori e a posteriori interagiscono nel quadro bayesiano.\n\n## Esercitazione in Classe\n\nValutate le emozioni che verranno presentate sullo schermo usando questo [link](https://docs.google.com/forms/d/e/1FAIpQLScWZD9XQoWJvf58fvQ6KGPJ562JxiKeg_azCIWagi1_BVtdpg/viewform?usp=header).\n\nScala di risposta:\n\n- Rabbia: 1\n- Disgusto: 2\n- Paura: 3\n- Felicità: 4\n- Tristezza: 5\n\n\n## Esercizi {.unnumbered} \n\n::: {.callout-important title=\"Problemi 1\" collapse=\"true\"}\nPer ciascuna delle distribuzioni di massa di probabilità discusse, utilizzare R per:\n\n- creare un grafico della funzione, scegliendo opportunamente i parametri;\n- estrarre un campione di 1000 valori casuali dalla distribuzione e visualizzarlo con un istogramma;\n- calcolare la media e la deviazione standard dei campioni e confrontarle con i valori teorici attesi;\n- stimare l'intervallo centrale del 94% utilizzando i campioni simulati;\n- determinare i quantili della distribuzione per gli ordini 0.05, 0.25, 0.75 e 0.95;\n- scegliendo un valore della distribuzione pari alla media più una deviazione standard, calcolare la probabilità che la variabile aleatoria assuma un valore minore o uguale a questo valore.\n:::\n\n::: {.callout-important title=\"Problemi 2\" collapse=\"true\"}\nEsercizi sulla distribuzione binomiale, risolvibili usando R, sono disponibili sulla seguente [pagina web](https://mathcenter.oxford.emory.edu/site/math117/probSetBinomialProbabilities/). \n:::\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered} \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] reshape2_1.4.4        pillar_1.11.0         tinytable_0.13.0     \n#>  [4] patchwork_1.3.2       ggdist_3.3.3          tidybayes_3.0.7      \n#>  [7] bayesplot_1.14.0      ggplot2_3.5.2         reliabilitydiag_0.2.1\n#> [10] priorsense_1.1.1      posterior_1.6.1       loo_2.8.0            \n#> [13] rstan_2.32.7          StanHeaders_2.32.10   brms_2.22.0          \n#> [16] Rcpp_1.1.0            sessioninfo_1.2.3     conflicted_1.2.0     \n#> [19] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        \n#> [22] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          \n#> [25] rio_1.2.3             here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#>  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#>  [7] snakecase_0.11.1      compiler_4.5.1        systemfonts_1.2.3    \n#> [10] vctrs_0.6.5           stringr_1.5.1         pkgconfig_2.0.3      \n#> [13] arrayhelpers_1.1-0    fastmap_1.2.0         backports_1.5.0      \n#> [16] labeling_0.4.3        utf8_1.2.6            rmarkdown_2.29       \n#> [19] ragg_1.5.0            purrr_1.1.0           xfun_0.53            \n#> [22] cachem_1.1.0          jsonlite_2.0.0        broom_1.0.9          \n#> [25] parallel_4.5.1        R6_2.6.1              stringi_1.8.7        \n#> [28] RColorBrewer_1.1-3    lubridate_1.9.4       estimability_1.5.1   \n#> [31] knitr_1.50            zoo_1.8-14            pacman_0.5.1         \n#> [34] Matrix_1.7-4          splines_4.5.1         timechange_0.3.0     \n#> [37] tidyselect_1.2.1      abind_1.4-8           yaml_2.3.10          \n#> [40] codetools_0.2-20      curl_7.0.0            pkgbuild_1.4.8       \n#> [43] lattice_0.22-7        plyr_1.8.9            withr_3.0.2          \n#> [46] bridgesampling_1.1-2  coda_0.19-4.1         evaluate_1.0.5       \n#> [49] survival_3.8-3        RcppParallel_5.1.11-1 tensorA_0.36.2.1     \n#> [52] checkmate_2.3.3       stats4_4.5.1          distributional_0.5.0 \n#> [55] generics_0.1.4        rprojroot_2.1.1       rstantools_2.5.0     \n#> [58] scales_1.4.0          xtable_1.8-4          glue_1.8.0           \n#> [61] emmeans_1.11.2-8      tools_4.5.1           mvtnorm_1.3-3        \n#> [64] grid_4.5.1            QuickJSR_1.8.0        colorspace_2.1-1     \n#> [67] nlme_3.1-168          cli_3.6.5             textshaping_1.0.3    \n#> [70] svUnit_1.0.8          Brobdingnag_1.2-9     V8_7.0.0             \n#> [73] gtable_0.3.6          digest_0.6.37         TH.data_1.1-4        \n#> [76] htmlwidgets_1.6.4     farver_2.1.2          memoise_2.0.1        \n#> [79] htmltools_0.5.8.1     lifecycle_1.0.4       MASS_7.3-65\n```\n:::\n\n\n## Bibliografia {.unnumbered}\n\n",
    "supporting": [
      "13_discr_rv_distr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}