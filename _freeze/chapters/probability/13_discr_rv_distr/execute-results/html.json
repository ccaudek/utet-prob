{
  "hash": "5af3e431ca6c3a62c0b1a25f53fa1360",
  "result": {
    "engine": "knitr",
    "markdown": "# Distribuzioni di v.c. discrete {#sec-prob-discrete-prob-distr}\n\n::: callout-important\n## In questo capitolo imparerai a:\n\n- comprendere le principali distribuzioni di massa di probabilit√†;\n- utilizzare R per manipolare e analizzare queste distribuzioni.\n::: \n\n::: callout-tip\n## Prerequisiti\n\n- Leggere il capitolo *Random variables and their distributions* del testo di @blitzstein2019introduction.\n- Leggere il capitolo *Special Distributions* [@schervish2014probability].\n:::\n\n::: callout-caution\n## Preparazione del Notebook\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(reshape2)\n```\n:::\n\n:::\n\n\n## Introduzione\n\n√à importante distinguere tra variabili casuali discrete e continue, perch√© le distribuzioni di probabilit√† associate sono molto diverse nei due casi [si veda il @sec-prob-random-var].\n\nIn questo capitolo ci focalizzeremo sulle **distribuzioni di probabilit√† discrete**, strumenti fondamentali per modellare fenomeni aleatori che generano un numero finito o numerabile di possibili esiti. Queste distribuzioni risultano particolarmente efficaci per descrivere eventi che si verificano in contesti discreti, come il numero di successi in un esperimento, l'occorrenza di un evento, o la selezione casuale da un insieme di opzioni finite. \n\n### Panoramica delle Distribuzioni Discrete\n\nDi seguito, vengono presentate alcune delle principali distribuzioni discrete utilizzate in statistica e nella ricerca psicologica Ogni distribuzione √® descritta in termini di caratteristiche fondamentali, applicazioni pratiche e importanza teorica.\n\n#### Distribuzione Uniforme Discreta\n\n- **Descrizione**: La distribuzione uniforme discreta rappresenta situazioni in cui tutti gli eventi all'interno di un insieme finito hanno la stessa probabilit√† di verificarsi.\n- **Applicazioni**: Si applica in contesti di scelta casuale equiprobabile, come:\n  - La selezione casuale di uno stimolo da una lista di parole in un esperimento di memoria.\n  - L'assegnazione casuale di partecipanti a gruppi sperimentali in uno studio di psicologia sociale.\n  - La scelta di un'immagine tra un insieme di stimoli visivi in una ricerca sull'attenzione.\n  - La probabilit√† uniforme che un partecipante scelga una delle opzioni in un questionario a risposte multiple, in assenza di preferenze o conoscenze specifiche.\n- **Parametri**:\n  - Intervallo di supporto: l'insieme finito di valori possibili (ad esempio, $\\{1, 2, \\dots, k\\}$).\n- **Importanza**: Funziona come modello di riferimento in situazioni di massima incertezza o mancanza di preferenze. √à utile per definire un punto di partenza in analisi pi√π complesse e per studiare comportamenti casuali.\n\n\n#### Distribuzione di Bernoulli\n\n- **Descrizione**: La distribuzione di Bernoulli modella esperimenti con due possibili esiti, generalmente etichettati come \"successo\" (con probabilit√† $p$) e \"fallimento\" (con probabilit√† $1-p$).\n- **Applicazioni**: Si applica a situazioni binarie, come il lancio di una moneta (testa/croce), la risposta a domande dicotomiche (s√¨/no), o l'esito di un evento che pu√≤ verificarsi o meno.\n- **Parametro**: \n  - $p$: probabilit√† di successo.\n- **Importanza**: Costituisce la base per molte altre distribuzioni discrete, come la distribuzione binomiale e geometrica. √à fondamentale per comprendere fenomeni con esiti dichotomici.\n\n#### Distribuzione Binomiale\n\n- **Descrizione**: La distribuzione binomiale descrive il numero totale di successi in un numero fisso $n$ di prove indipendenti, ciascuna governata da una distribuzione di Bernoulli con probabilit√† di successo $p$.\n- **Applicazioni**: Viene utilizzata per analizzare processi ripetuti con esiti binari, ad esempio:\n  - Il numero di voti favorevoli in un campione di opinione.\n  - Il numero di sintomi osservati in un gruppo di pazienti.\n  - Il conteggio di errori in un test di accuratezza.\n- **Parametri**:\n  - $n$: numero di prove.\n  - $p$: probabilit√† di successo in ogni prova.\n- **Importanza**: Fornisce uno strumento essenziale per modellare fenomeni ripetuti in condizioni identiche, consentendo analisi probabilistiche avanzate e previsioni statistiche.\n\n#### Distribuzione di Poisson\n\n- **Descrizione**: La distribuzione di Poisson modella il numero di eventi che si verificano in un intervallo fissato di tempo o spazio, quando tali eventi sono rari, indipendenti e accadono a un tasso medio costante $\\lambda$.\n- **Applicazioni**: Trova impiego in contesti dove gli eventi sono sporadici ma prevedibili, ad esempio:\n  - Il numero di episodi di ansia riportati in una settimana.\n  - Il numero di interazioni sociali spontanee di un bambino con disturbo dello spettro autistico durante una sessione di osservazione.\n  - La frequenza di lapsus verbali durante una presentazione pubblica.\n  - Il numero di sogni vividi riportati durante una serie di notti consecutive in uno studio sul sonno.\n- **Parametro**:\n  - $\\lambda$: tasso medio di eventi per unit√† di tempo o spazio.\n- **Importanza**: √à cruciale per analizzare fenomeni psicologici o comportamentali rari ma significativi. Aiuta a comprendere i meccanismi sottostanti e a modellare la variabilit√† osservata in contesti clinici, sperimentali o quotidiani.\n\n\nIn conclusione, le distribuzioni discrete sopra descritte rappresentano strumenti fondamentali per modellare una vasta gamma di fenomeni osservati in ambito scientifico, psicologico e applicativo. Ciascuna distribuzione offre una cornice teorica ben definita per interpretare e analizzare situazioni caratterizzate da variabili aleatorie discrete, fornendo cos√¨ le basi per inferenze statistiche robuste e previsioni quantitative affidabili.\n\n## Distribuzioni in R\n\nIn R, per ogni distribuzione sono disponibili quattro funzioni principali, i cui nomi iniziano con le lettere:\n\n- **d** (*density*): per calcolare i valori teorici relativi alla distribuzione,  \n- **p** (*probability*): per ottenere la probabilit√† cumulativa,  \n- **q** (*quantile*): per determinare i quantili,  \n- **r** (*random*): per generare campioni casuali.  \n\nIl pacchetto di base `stats` include numerose funzioni dedicate alle principali distribuzioni statistiche, permettendo di calcolare valori teorici e simulare dati in modo semplice e flessibile. Per ulteriori dettagli sulle distribuzioni disponibili e sull'uso delle relative funzioni, √® possibile consultare la documentazione con il comando `?Distributions`.\n\n\n## Distribuzione Uniforme Discreta\n\nLa **distribuzione uniforme discreta** √® una delle pi√π semplici e intuitive distribuzioni di probabilit√†. √à utilizzata per modellare situazioni in cui **tutti gli esiti possibili sono ugualmente probabili**. Si applica, ad esempio, quando si estrae un numero a caso da un insieme finito di interi senza alcuna preferenza.\n\n::: {#def-}\n\nSia $X$ una variabile casuale che pu√≤ assumere i valori interi da 1 a $N$, tutti con la **stessa probabilit√†**. Allora diciamo che $X$ ha una distribuzione uniforme discreta sull‚Äôintervallo $\\{1, 2, \\dots, N\\}$. In simboli:\n\n$$\nX \\sim \\text{Uniforme Discreta}(1, N) .\n$$\n\nPoich√© ci sono $N$ valori possibili e ciascuno ha la stessa probabilit√†, ogni valore ha probabilit√†:\n\n$$\nP(X = x) = \\frac{1}{N}, \\quad \\text{per } x \\in \\{1, 2, \\dots, N\\}.\n$$\n:::\n\n### Propriet√† di normalizzazione\n\nLa somma delle probabilit√† di tutti gli esiti deve essere pari a 1:\n\n$$\n\\sum_{x = 1}^{N} P(X = x) = \\sum_{x = 1}^{N} \\frac{1}{N} = \\frac{1}{N} \\cdot N = 1.\n$$\n\nQuesta √® una propriet√† fondamentale di ogni distribuzione di probabilit√†.\n\n### Valore atteso\n\nIl **valore atteso** (o media) ci dice qual √® il risultato medio atteso nel lungo periodo. Si calcola come:\n\n$$\n\\mathbb{E}(X) = \\sum_{x = 1}^{N} x \\cdot P(X = x) = \\frac{1}{N} \\sum_{x = 1}^{N} x.\n$$\n\nLa somma dei primi $N$ numeri naturali √®:\n\n$$\n\\sum_{x = 1}^{N} x = \\frac{N(N + 1)}{2}.\n$$\n\nQuindi:\n\n$$\n\\mathbb{E}(X) = \\frac{1}{N} \\cdot \\frac{N(N + 1)}{2} = \\frac{N + 1}{2}.\n$$\n\nIn conclusione, il valore atteso di una variabile uniforme discreta su $\\{1, \\dots, N\\}$ √® $\\frac{N + 1}{2}$.\n\n### Varianza\n\nLa varianza della distribuzione uniforme discreta √®:\n\n$$\n\\mathbb{V}(X) = \\frac{(N + 1)(N - 1)}{12}.\n$$\n\n::: {.callout-tip title=\"Dimostrazione\" collapse=\"true\"}\n\nLa **varianza** misura quanto i valori di $X$ si discostano in media dalla media $\\mathbb{E}(X)$. Si calcola come:\n\n$$\n\\mathbb{V}(X) = \\mathbb{E}(X^2) - \\left[\\mathbb{E}(X)\\right]^2.\n$$\n\n1. Calcolo di $\\mathbb{E}(X^2)$. \n\nPoich√© tutti i valori hanno la stessa probabilit√† $\\frac{1}{N}$, otteniamo:\n\n$$\n\\mathbb{E}(X^2) = \\frac{1}{N} \\sum_{x = 1}^{N} x^2.\n$$\n\nLa somma dei quadrati dei primi $N$ interi √®:\n\n$$\n\\sum_{x = 1}^{N} x^2 = \\frac{N(N + 1)(2N + 1)}{6}\n$$\n\n(per una dimostrazione, si veda la [ pagina di Wikipedia sui numeri piramidali quadrati](https://it.wikipedia.org/wiki/Numero_piramidale_quadrato)).\n\nQuindi:\n\n$$\n\\mathbb{E}(X^2) = \\frac{1}{N} \\cdot \\frac{N(N + 1)(2N + 1)}{6} = \\frac{(N + 1)(2N + 1)}{6}.\n$$\n\n2. Calcolo della varianza. \n\nSostituendo nella formula della varianza:\n\n$$\n\\begin{aligned}\n\\mathbb{V}(X) &= \\frac{(N + 1)(2N + 1)}{6} - \\left(\\frac{N + 1}{2}\\right)^2 \\\\\n&= \\frac{(N + 1)(2N + 1)}{6} - \\frac{(N + 1)^2}{4}\n\\end{aligned}\n$$\n\nPer semplificare, portiamo tutto allo stesso denominatore:\n\n$$\n\\begin{aligned}\n\\mathbb{V}(X) &= \\frac{2(N + 1)(2N + 1)}{12} - \\frac{3(N + 1)^2}{12} \\\\\n&= \\frac{(N + 1) \\left[2(2N + 1) - 3(N + 1)\\right]}{12} \\\\\n&= \\frac{(N + 1)(4N + 2 - 3N - 3)}{12} \\\\\n&= \\frac{(N + 1)(N - 1)}{12}.\n\\end{aligned}\n$$\n\nIn conclusione, la varianza della distribuzione uniforme discreta √®:\n\n$$\n\\mathbb{V}(X) = \\frac{(N + 1)(N - 1)}{12}.\n$$\n:::\n\n**In sintesi**, per una variabile casuale $X$ uniformemente distribuita su $\\{1, 2, \\dots, N\\}$:\n\n| Propriet√†        | Formula                                 |\n|------------------|------------------------------------------|\n| Media            | $\\mathbb{E}(X) = \\dfrac{N + 1}{2}$       |\n| Varianza         | $\\mathbb{V}(X) = \\dfrac{(N + 1)(N - 1)}{12}$ |\n\nQuesta distribuzione √® utile ogni volta che **non c‚Äô√® alcuna ragione per preferire un valore a un altro** all‚Äôinterno di un insieme finito di numeri interi.\n\n::: {#exm-}\nSupponiamo che $X$ sia una variabile casuale con distribuzione uniforme discreta tra 1 e 10, ovvero:\n\n$$\nX \\sim \\text{Uniforme Discreta}(1, 10) .\n$$\n\nVogliamo:\n\n1. generare un grande campione casuale,\n2. calcolare la media e la varianza osservate,\n3. confrontarle con i valori teorici.\n\n**Codice R.**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)  # Per rendere la simulazione riproducibile\n\n# Parametro N\nN <- 10\n\n# Simulazione: 100.000 osservazioni dalla distribuzione uniforme discreta\nx <- sample(1:N, size = 100000, replace = TRUE)\n\n# Media e varianza empiriche\nmedia_empirica <- mean(x)\nvarianza_empirica <- var(x)\n\n# Valori teorici\nmedia_teorica <- (N + 1) / 2\nvarianza_teorica <- ((N + 1) * (N - 1)) / 12\n\n# Risultati\ntibble(\n  `Media empirica` = media_empirica,\n  `Media teorica` = media_teorica,\n  `Varianza empirica` = varianza_empirica,\n  `Varianza teorica` = varianza_teorica\n)\n#> # A tibble: 1 √ó 4\n#>   `Media empirica` `Media teorica` `Varianza empirica` `Varianza teorica`\n#>              <dbl>           <dbl>               <dbl>              <dbl>\n#> 1             5.51             5.5                8.26               8.25\n```\n:::\n\n\nCon un campione molto grande, le **statistiche empiriche** (cio√® calcolate dai dati simulati) saranno molto vicine ai **valori teorici**:\n\n|                     | Valore     |\n|---------------------|------------|\n| Media teorica       | 5.5        |\n| Media empirica      | ‚âà 5.5      |\n| Varianza teorica    | 8.25       |\n| Varianza empirica   | ‚âà 8.25     |\n\n\\\n\n**In sintesi**, a simulazione conferma che:\n\n- la media empirica converge verso $\\mathbb{E}(X) = \\frac{N + 1}{2}$,\n- la varianza empirica converge verso $\\mathbb{V}(X) = \\frac{(N + 1)(N - 1)}{12}$.\n:::\n\n\n## Distribuzione di Bernoulli \n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nIn statistica, un esperimento che ammette solo due esiti possibili √® modellato attraverso quella che viene chiamata \"prova Bernoulliana\". Un esempio tipico √® il lancio di una moneta, che pu√≤ dare come risultato testa o croce.\n\n::: {#def-}\nUna variabile casuale $X$ che assume valori in $\\{0, 1\\}$ √® detta variabile di Bernoulli. La sua distribuzione di probabilit√† √® definita come:\n\n$$\nP(X \\mid \\theta) =\n  \\begin{cases}\n    p     & \\text{se $X = 1$ (successo)}, \\\\\n    1 - p & \\text{se $X = 0$ (insuccesso)},\n  \\end{cases}\n$$\n\ndove $0 \\leq p \\leq 1$. Il parametro $p$ rappresenta la probabilit√† del \"successo\" ($X = 1$), mentre $1 - p$ √® la probabilit√† dell'\"insuccesso\" ($X = 0$).\n:::\n\nLa distribuzione di Bernoulli descrive quindi un contesto in cui la probabilit√† di osservare l'esito 1 √® $p$ e quella di osservare l'esito 0 √® $1 - p$. Viene utilizzata per modellare situazioni binarie, come una risposta \"s√¨\" o \"no\", oppure un \"successo\" o \"insuccesso\".\n\nCalcolando il valore atteso e la varianza, otteniamo:\n\n$$\n\\begin{aligned}\n\\mathbb{E}(X) &= 0 \\cdot P(X=0) + 1 \\cdot P(X=1) = p, \\\\\n\\mathbb{V}(X) &= (0 - p)^2 \\cdot P(X=0) + (1 - p)^2 \\cdot P(X=1) = p(1-p).\n\\end{aligned}\n$$ {#eq-ev-var-bern}\n\n::: {.callout-note}\n## Dimostrazione\n\nEsaminiamo la dimostrazione algebrica del calcolo della varianza.\n\nEspandiamo il calcolo della somma, considerando i due possibili valori di $X$ (0 e 1).\n\n1. **Primo termine ($X = 0$):**\n\n   $$\n   (0 - \\mathbb{E}(X))^2 \\cdot P(X = 0) = (0 - p)^2 \\cdot (1 - p).\n   $$\n\n   Semplificando $(0 - p)^2 = p^2$, quindi:\n\n   $$\n   (0 - \\mathbb{E}(X))^2 \\cdot P(X = 0) = p^2 \\cdot (1 - p).\n   $$\n\n2. **Secondo termine ($X = 1$):**\n\n   $$\n   (1 - \\mathbb{E}(X))^2 \\cdot P(X = 1) = (1 - p)^2 \\cdot p.\n   $$\n\n   Semplificando $(1 - p)^2 = 1 - 2p + p^2$, quindi:\n\n   $$\n   (1 - \\mathbb{E}(X))^2 \\cdot P(X = 1) = (1 - 2p + p^2) \\cdot p = p - 2p^2 + p^3.\n   $$\n\n3. **Somma dei termini**\n\n   Ora sommiamo i due contributi:\n\n   $$\n   \\mathbb{V}(X) = p^2 \\cdot (1 - p) + (p - 2p^2 + p^3).\n   $$\n\n   Espandendo il primo termine:\n\n   $$\n   p^2 \\cdot (1 - p) = p^2 - p^3.\n   $$\n\n   Somma completa:\n\n   $$\n   \\mathbb{V}(X) = (p^2 - p^3) + (p - 2p^2 + p^3).\n   $$\n\n4. **Raggruppiamo i termini**\n\n   $$\n   \\mathbb{V}(X) = p - p^2.\n   $$\n\n   **Risultato finale:**\n\n   $$\n   \\mathbb{V}(X) = p(1 - p).\n   $$\n\nIn sintesi, la varianza di una variabile aleatoria binaria $X$, distribuita secondo Bernoulli con parametro $p$, √® data da $p(1-p)$. \n:::\n\nTale risultato mostra come la varianza massima si ottenga per $p = 0.5$, condizione che corrisponde alla massima incertezza intrinseca nel processo, ossia quando la probabilit√† di successo eguaglia quella di insuccesso.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Valori di p tra 0 e 1\np <- seq(0, 1, length.out = 100)\nvariance <- p * (1 - p)\ndata <- data.frame(p = p, Variance = variance)\n\n# Creazione del grafico\nggplot(data, aes(x = p, y = Variance)) +\n  geom_line(linewidth = 1.2) +\n  labs(\n    x = expression(p),\n    y = \"Varianza\"\n  )\n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Notazione\n\nPer indicare che la variabile casuale $X$ segue una distribuzione Bernoulliana di parametro $p$ Utilizziamo la notazione $X \\sim \\mathcal{Bern}(p)$, o in maniera equivalente $\\mathcal{Bern}(X \\mid p)$.\n\n::: {#exm-}\nNel caso del lancio di una moneta equilibrata, la variabile di Bernoulli assume i valori $0$ e $1$ con uguale probabilit√† di $\\frac{1}{2}$. Pertanto, la funzione di massa di probabilit√† assegna una probabilit√† di $\\frac{1}{2}$ sia per $X = 0$ che per $X = 1$, mentre la funzione di distribuzione cumulativa risulta essere $\\frac{1}{2}$ per $X = 0$ e $1$ per $X = 1$.\n\nGeneriamo dei valori casuali dalla distribuzione di Bernoulli. Iniziamo con un singolo valore:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Probabilit√† di successo\np <- 0.5\n\n# Genera un singolo valore\nbernoulli_sample <- rbinom(n = 1, size = 1, prob = p)\nprint(bernoulli_sample)\n#> [1] 0\n\n# Genera un campione di 10 valori\nbernoulli_sample <- rbinom(n = 10, size = 1, prob = p)\nprint(bernoulli_sample)\n#>  [1] 0 1 0 1 0 0 0 1 0 0\n```\n:::\n\n:::\n\n\n\n## Distribuzione Binomiale\n\nLa distribuzione binomiale √® una distribuzione di probabilit√† discreta che modella il numero di successi $y$ in un numero fissato $n$ di prove di Bernoulli indipendenti e identiche, dove ciascuna prova ha solo due esiti possibili: \"successo\" (rappresentato da \"1\") con probabilit√† $p$ o \"insuccesso\" (rappresentato da \"0\") con probabilit√† $1 - p$. La notazione utilizzata √® la seguente:\n\n$$\nY \\sim \\mathcal{Binom}(n, p).\n$$\n\n::: {#def-}\nLa distribuzione binomiale descrive la probabilit√† di osservare esattamente $y$ successi in $n$ prove di Bernoulli indipendenti:\n\n$$\nP(Y = y) = \\binom{n}{y} p^{y} (1 - p)^{n - y} = \\frac{n!}{y!(n - y)!} p^{y} (1 - p)^{n - y},\n$$ {#eq-binom-distr}\n\ndove $\\binom{n}{y}$, noto come coefficiente binomiale, rappresenta il numero di modi possibili per ottenere $y$ successi in $n$ prove, e $p$ √® la probabilit√† di successo in ciascuna prova.\n:::\n\nLa distribuzione binomiale si presta bene a esempi classici come il lancio ripetuto di una moneta o l'estrazione di biglie da un'urna. Ad esempio, nel caso del lancio di una moneta, questa distribuzione descrive la probabilit√† di ottenere un determinato numero di \"teste\" in un certo numero di lanci, con ogni lancio che segue una distribuzione di Bernoulli con probabilit√† di successo $p$.\n\nUna caratteristica interessante della distribuzione binomiale √® la sua *propriet√† di riproducibilit√†*: se due variabili casuali indipendenti, $y_1$ e $y_2$, seguono entrambe distribuzioni binomiali con lo stesso parametro $p$, ma con un diverso numero di prove ($n_1$ e $n_2$), la loro somma, $y = y_1 + y_2$, sar√† ancora distribuita binomialmente, con parametri $n_1 + n_2$ e $p$.\n\n::: callout-note\n## Dimostrazione\n\nPer chiarire il calcolo delle probabilit√† nella **distribuzione binomiale**, consideriamo una serie di prove di **Bernoulli**. Supponiamo di avere $n$ prove indipendenti, ciascuna con probabilit√† $p$ di successo, e di osservare esattamente $y$ successi.\n\nUna possibile configurazione dei risultati pu√≤ essere rappresentata come:\n\n$$\n\\overbrace{SS\\dots S}^\\text{$y$ successi} \\, \\overbrace{II\\dots I}^\\text{$n - y$ insuccessi}\n$$\n\nLa probabilit√† di ottenere **esattamente $y$ successi in una sequenza specifica** (cio√® in un ordine fissato) √®:\n\n$$\np^y \\cdot (1 - p)^{n - y},\n$$\n\ndove $p^y$ √® la probabilit√† dei $y$ successi e $(1 - p)^{n - y}$ quella dei $n - y$ insuccessi.\n\nTuttavia, ci√≤ che ci interessa √® la **probabilit√† complessiva** di ottenere $y$ successi in *qualsiasi ordine*. In altre parole, vogliamo calcolare la probabilit√† dell'**unione** di tutte le possibili sequenze di $n$ prove che contengono esattamente $y$ successi.\n\nIl numero di tali sequenze √® dato dal **coefficiente binomiale** $\\binom{n}{y}$, che rappresenta il numero di modi diversi in cui possiamo scegliere le $y$ posizioni dei successi tra le $n$ prove.\n\nMoltiplicando la probabilit√† di una singola sequenza per il numero totale di sequenze possibili, otteniamo la **funzione di probabilit√† della distribuzione binomiale**:\n\n$$\nP(Y = y) = \\binom{n}{y} p^y (1 - p)^{n - y}.\n$$\n:::\n\n### Caso particolare $n = 1$\n\nOra consideriamo il caso particolare in cui $n = 1$. Quando $n = 1$, il coefficiente binomiale diventa:\n\n$$\n\\binom{1}{y} = \\frac{1!}{y! (1-y)!}.\n$$\n\nEspandiamo i fattoriali per i due possibili valori di $y$, che pu√≤ assumere solo 0 o 1 (poich√© $y \\in \\{0, 1, \\dots, n\\}$).\n\n**Caso 1: $y = 0$**\n\n$$\n\\binom{1}{0} = \\frac{1!}{0! (1-0)!} = \\frac{1}{1 \\cdot 1} = 1.\n$$\n\nQuindi, per $y = 0$:\n$$\nP(Y = 0) = \\binom{1}{0} p^0 (1-p)^{1-0} = 1 \\cdot 1 \\cdot (1-p) = 1-p.\n$$\n\n**Caso 2: $y = 1$**\n\n$$\n\\binom{1}{1} = \\frac{1!}{1! (1-1)!} = \\frac{1}{1 \\cdot 1} = 1.\n$$\n\nQuindi, per $y = 1$:\n$$\nP(Y = 1) = \\binom{1}{1} p^1 (1-p)^{1-1} = 1 \\cdot p \\cdot 1 = p.\n$$\n\nIn conclusione, la PMF per la distribuzione binomiale con $n = 1$ diventa:\n\n$$\nP(Y = y) =\n\\begin{cases}\n1-p, & \\text{se } y = 0, \\\\\np, & \\text{se } y = 1.\n\\end{cases}\n$$\n\nQuesta √® esattamente la PMF della distribuzione di Bernoulli con parametro $p$:\n\n$$\nP(Y = y) = p^y (1-p)^{1-y}, \\quad y \\in \\{0, 1\\}.\n$$ \n\nPertanto, la distribuzione binomiale con $n = 1$ √® equivalente alla distribuzione di Bernoulli con parametro $p$.\n\n### Applicazioni Pratiche della Distribuzione Binomiale\n\nPer illustrare l‚Äôapplicazione della distribuzione binomiale, consideriamo un esempio semplice. Supponiamo di osservare **2 successi su 4 prove di Bernoulli**, dove la probabilit√† di successo in ogni prova √® $p = 0.2$. La probabilit√† di ottenere esattamente questo risultato si calcola con la formula:\n\n$$\nP(Y = 2) = \\binom{4}{2} \\cdot 0.2^2 \\cdot (1 - 0.2)^{2} = 0.1536.\n$$\n\nIn R, questo calcolo si pu√≤ fare in modo diretto:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri\nn <- 4\np <- 0.2\ny <- 2\n\n# Calcolo della probabilit√† esatta\nprob <- choose(n, y) * p^y * (1 - p)^(n - y)\nprint(prob)\n#> [1] 0.154\n```\n:::\n\n\nIn alternativa, possiamo usare la funzione `dbinom()` per ottenere la stessa probabilit√†:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprob <- dbinom(x = y, size = n, prob = p)\nprint(prob)\n#> [1] 0.154\n```\n:::\n\n\n#### Visualizzazione della distribuzione di probabilit√†\n\nPossiamo rappresentare graficamente la **distribuzione di massa di probabilit√†** per tutti i possibili valori di $y$ da $0$ a $n$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny <- 0:n\nprobabilities <- dbinom(y, size = n, prob = p)\n\ndf <- data.frame(Successi = y, Probabilit√† = probabilities)\n\ndf |>\n  ggplot(aes(x = Successi, y = Probabilit√†)) +\n    geom_segment(\n      aes(xend = Successi, yend = 0), lwd = 1.2\n      ) +\n    geom_point(size = 3) +\n    labs(\n      x = \"Numero di successi y\",\n      y = \"Probabilit√†\"\n    )\n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n#### Generazione di un campione casuale\n\nLa funzione `rbinom()` permette di generare un campione casuale da una distribuzione binomiale:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\nsamples <- rbinom(n = 30, size = 5, prob = 0.5)\nprint(samples)\n#>  [1] 4 4 2 4 3 3 3 1 3 3 2 3 4 2 2 4 5 1 2 3 4 1 5 4 1 3 2 4 2 4\n```\n:::\n\n\n#### Variazione della distribuzione al variare di $p$\n\nPer esplorare l‚Äôeffetto di diversi valori di $p$ sulla forma della distribuzione, possiamo visualizzare pi√π curve binomiali per $n = 20$ e $p$ variabile:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 20\np_values <- seq(0.3, 0.9, by = 0.3)\ny <- 0:25\n\ndf <- data.frame()\n\nfor (p in p_values) {\n  binom_dist <- dbinom(y, size = n, prob = p)\n  df <- rbind(df, data.frame(y = y, Prob = binom_dist, p = factor(p)))\n}\n\ndf |>\n  ggplot(aes(x = y, y = Prob, color = p)) +\n    geom_point() +\n    geom_line() +\n    labs(\n      x = \"Numero di successi y\",\n      y = \"Probabilit√†\",\n      color = expression(p)\n    )\n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n#### Funzione di ripartizione cumulativa\n\nPossiamo anche rappresentare la **funzione di distribuzione cumulativa** (CDF) per $n = 5$ e $p = 0.5$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 5\np <- 0.5\ny <- 0:n\n\ncdf_values <- pbinom(y, size = n, prob = p)\ndf <- data.frame(y = y, cdf = cdf_values)\n\ndf |>\n  ggplot(aes(x = y, y = cdf)) +\n    geom_line() +\n    geom_point() +\n    geom_hline(\n      yintercept = 1, linetype = \"dashed\", color = \"black\", alpha = 0.7\n    ) +\n    labs(\n      x = \"Numero di successi y\",\n      y = \"Probabilit√† cumulativa\"\n    )\n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n::: {#exm-}\nSupponiamo di lanciare una **moneta equa** (cio√® con probabilit√† $p = 0.5$ di ottenere testa) **5 volte**. Vogliamo calcolare la probabilit√† di ottenere **almeno 2 teste**, ovvero:\n\n$$\nP(Y \\geq 2) = P(Y = 2) + P(Y = 3) + P(Y = 4) + P(Y = 5).\n$$\n\nPossiamo sommare direttamente queste probabilit√† usando `dbinom()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresult <- sum(dbinom(2:5, size = 5, prob = 0.5))\nprint(result)\n#> [1] 0.812\n```\n:::\n\n\nUn modo alternativo, pi√π efficiente, consiste nel calcolare il **complemento** della probabilit√† di ottenere **meno di 2 teste** (cio√® 0 o 1):\n\n$$\nP(Y \\geq 2) = 1 - P(Y \\leq 1)\n$$\n\nIn R, possiamo usare la funzione `pbinom()` per calcolare questa probabilit√† cumulativa:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresult <- 1 - pbinom(q = 1, size = 5, prob = 0.5)\nprint(result)\n#> [1] 0.812\n```\n:::\n\n\nEntrambi i metodi restituiscono lo stesso risultato numerico, ma il secondo √® spesso preferibile quando $n$ √® grande o quando si vuole calcolare una probabilit√† di coda.\n:::\n\n#### Quantili di una distribuzione binomiale\n\nHai perfettamente ragione ‚Äî grazie per l'osservazione!\n\nInfatti, con i parametri `size = 5`, `prob = 0.5` e `target_probability = 0.60`, la funzione `qbinom()` restituisce **3**, non **2**. Questo perch√© `qbinom()` restituisce **il pi√π piccolo valore di $y$ tale che $P(Y \\leq y) \\geq p$**. Verifichiamolo in R:\n\n```r\npbinom(2, 5, 0.5)  # = 0.5\npbinom(3, 5, 0.5)  # = 0.8125\n```\n\nQuindi:\n\n- $P(Y \\leq 2) = 0.5$ ‚Üí troppo poco\n- $P(Y \\leq 3) = 0.8125$ ‚Üí supera il 60%\n\nPertanto, `qbinom(0.6, 5, 0.5)` restituisce `3`.\n\n#### Quantili di una distribuzione binomiale\n\nLa funzione `qbinom()` permette di calcolare il **quantile** di una distribuzione binomiale, cio√® il **numero minimo di successi** $y$ tale che la probabilit√† cumulativa $P(Y \\leq y)$ sia **maggiore o uguale** a una certa soglia.\n\nAd esempio, supponiamo di voler sapere **qual √® il numero minimo di successi** tale che la probabilit√† cumulativa sia **almeno 60%**. Possiamo usare:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Probabilit√† cumulativa desiderata\ntarget_probability <- 0.60\n\n# Calcolo del quantile\nresult <- qbinom(p = target_probability, size = 5, prob = 0.5)\nprint(result)\n#> [1] 3\n```\n:::\n\n\nIl risultato √® `3`, il che significa che:\n\n$$\nP(Y \\leq 3) = 0.8125 \\geq 0.60,\n$$\n\nmentre\n\n$$\nP(Y \\leq 2) = 0.5 < 0.60.\n$$\n\nQuindi, servono **almeno 3 successi** per superare la soglia del 60% di probabilit√† cumulativa.\n\n> üîé `qbinom(p, size, prob)` restituisce il **pi√π piccolo valore di $y$** tale che $P(Y \\leq y) \\geq p$.\n\n#### Rappresentazione grafica del quantile\n\nPer visualizzare il comportamento della funzione di ripartizione cumulativa e individuare il quantile per $p = 0.60$, possiamo usare il seguente codice in R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri\nn <- 5\np <- 0.5\ntarget_probability <- 0.60\n\n# Asse y: numero di successi\ny <- 0:n\n\n# Calcolo dei valori cumulativi\ncdf <- pbinom(y, size = n, prob = p)\n\n# Calcolo del quantile\nq <- qbinom(target_probability, size = n, prob = p)\n\n# Data frame\ndf <- data.frame(Successi = y, CDF = cdf)\n\n# Grafico\ndf |>\n  ggplot(aes(x = Successi, y = CDF)) +\n  geom_step(direction = \"hv\", linewidth = 1.1) +\n  geom_point(size = 2) +\n  geom_hline(\n    yintercept = target_probability, linetype = \"dashed\", color = \"red\"\n  ) +\n  geom_vline(xintercept = q, linetype = \"dotted\", color = \"blue\") +\n  annotate(\n    \"text\",\n    x = q + 0.4, y = 0.05, label = paste(\"quantile =\", q),\n    color = \"blue\"\n  ) +\n  annotate(\n    \"text\",\n    x = 0.5, y = target_probability + 0.05,\n    label = paste(\"soglia =\", target_probability), color = \"red\"\n  ) +\n  labs(\n    x = \"Numero di successi\",\n    y = \"Probabilit√† cumulativa\"\n  ) +\n  ylim(0, 1.05)\n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIn questo grafico:\n\n- la **linea rossa tratteggiata** rappresenta la soglia di probabilit√† desiderata (es. 0.60);\n- la **linea blu tratteggiata verticale** indica il quantile corrispondente, cio√® il pi√π piccolo valore di $y$ per cui $P(Y \\leq y) \\geq 0.60$;\n- il valore calcolato √® `3`, quindi con **al massimo 3 successi**, la probabilit√† cumulativa supera il 60%.\n\n::: {#exm-}\nConsideriamo una **distribuzione binomiale** con $n = 10$ prove e probabilit√† di successo $p = 0.2$. Supponiamo di voler calcolare la probabilit√† di ottenere **al massimo 4 successi**. In termini matematici, vogliamo calcolare:\n\n$$\nP(Y \\leq 4) .\n$$\n\nIn R, questo si ottiene con la funzione `pbinom()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo della probabilit√† cumulativa fino a 4 successi\np_cumulativa <- pbinom(4, size = 10, prob = 0.2)\nprint(p_cumulativa)\n#> [1] 0.967\n```\n:::\n\n\nIl risultato indica che c'√® circa l'**97%** di probabilit√† di ottenere 4 o meno successi su 10 prove, quando la probabilit√† di successo in ciascuna prova √® 0.2.\n\nOra facciamo il **passaggio inverso**: immaginiamo di conoscere la probabilit√† cumulativa (per esempio, 0.97) e vogliamo sapere **quanti successi** bisogna considerare per raggiungere quella probabilit√†.\n\nPer questo usiamo la funzione `qbinom()`, che ci restituisce il **pi√π piccolo numero di successi $y$ tale che $P(Y \\leq y) \\geq$ quella probabilit√†**:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo del numero di successi associato alla probabilit√† cumulativa\nnumero_successi <- qbinom(p_cumulativa, size = 10, prob = 0.2)\nprint(numero_successi)\n#> [1] 4\n```\n:::\n\n\nIl valore ottenuto sar√† `4`, cio√® il **minimo numero di successi** per cui la probabilit√† cumulativa √® almeno il 97%.\n\n**Riepilogo concetti chiave**:\n\n- `pbinom(y, n, p)` calcola la probabilit√† di ottenere **al massimo $y$ successi**;\n- `qbinom(prob, n, p)` calcola **il numero minimo di successi** necessari per raggiungere almeno quella probabilit√†.\n\nIn sintesi, `pbinom()` e `qbinom()` sono strumenti complementari: `pbinom` ci d√† la probabilit√† di ottenere fino a un certo numero di successi, mentre `qbinom` ci dice fino a quanti successi possiamo ottenere per raggiungere una certa probabilit√†. Nell‚Äôanalisi di una distribuzione binomiale (e di molte altre distribuzioni) queste funzioni aiutano a calcolare e interpretare facilmente probabilit√† cumulate e quantili in R, rendendo pi√π semplice l‚Äôanalisi di eventi aleatori.\n:::\n\n### Valore atteso e deviazione standard nella distribuzione binomiale \n\nNella distribuzione binomiale, possiamo calcolare facilmente due quantit√† molto importanti:\n\n- **il valore atteso** (o media), che ci dice **quanti successi ci aspettiamo in media** su un certo numero di prove;\n- **la deviazione standard**, che ci dice **quanto i risultati tendono a variare** attorno alla media.\n\nLe formule sono le seguenti:\n\n$$\n\\text{Media (valore atteso):} \\quad \\mu = n p ,\n$$ {#eq-binom-distr-expval}\n\n$$\n\\text{Deviazione standard:} \\quad \\sigma = \\sqrt{n p (1 - p)} ,\n$$ {#eq-binom-distr-var}\n\ndove:\n\n- $n$ √® il numero di prove (per esempio, il numero di lanci di una moneta),\n- $p$ √® la probabilit√† di successo in ogni prova.\n\n::: callout-note\n**Dimostrazione.**\n\nLa variabile $Y$ rappresenta il numero di successi in $n$ prove di Bernoulli indipendenti. Possiamo scriverla come somma di $n$ variabili casuali indipendenti:\n\n$$\nY = Y_1 + Y_2 + \\cdots + Y_n,\n$$\n\ndove ciascuna $Y_i \\sim \\text{Bernoulli}(p)$, cio√®:\n\n$$\nY_i =\n\\begin{cases}\n1 & \\text{con probabilit√† } p \\\\\n0 & \\text{con probabilit√† } 1 - p\n\\end{cases}\n$$\n\n**Valore atteso di $Y_i$.**\n\nPer definizione del valore atteso:\n\n$$\n\\mathbb{E}(Y_i) = 1 \\cdot p + 0 \\cdot (1 - p) = p.\n$$\n\n**Valore atteso di $Y_i^2$.**\n\nPoich√© $Y_i$ assume solo i valori 0 e 1, si ha $Y_i^2 = Y_i$. Quindi:\n\n$$\n\\mathbb{E}(Y_i^2) = \\mathbb{E}(Y_i) = p.\n$$\n\nLa **varianza** di una variabile casuale si definisce come:\n\n$$\n\\operatorname{Var}(Y_i) = \\mathbb{E}(Y_i^2) - [\\mathbb{E}(Y_i)]^2.\n$$\n\nSostituendo i valori trovati sopra:\n\n$$\n\\operatorname{Var}(Y_i) = p - p^2 = p(1 - p).\n$$\n\nRicordiamo che $Y = \\sum_{i=1}^{n} Y_i$ e che le $Y_i$ sono **indipendenti**. Una propriet√† fondamentale della varianza √® che se $Z_1, \\dots, Z_n$ sono indipendenti:\n\n$$\n\\operatorname{Var}(Z_1 + \\cdots + Z_n) = \\operatorname{Var}(Z_1) + \\cdots + \\operatorname{Var}(Z_n).\n$$\n\nApplichiamola al nostro caso:\n\n$$\n\\operatorname{Var}(Y) = \\sum_{i=1}^{n} \\operatorname{Var}(Y_i).\n$$\n\nPoich√© tutte le $Y_i$ hanno la stessa varianza $p(1 - p)$, la somma diventa:\n\n$$\n\\operatorname{Var}(Y) = n \\cdot p(1 - p).\n$$\n\nAbbiamo dimostrato da definizione che, se $Y \\sim \\text{Bin}(n, p)$, allora:\n\n$$\n\\operatorname{Var}(Y) = n \\cdot p \\cdot (1 - p).\n$$\n\nQuesta formula descrive la dispersione attesa nel numero di successi su $n$ prove indipendenti, ciascuna con probabilit√† di successo $p$.\n:::\n\n::: {#exm-binom-var}\nSupponiamo di lanciare **4 volte una moneta truccata** che ha una probabilit√† di successo (es. ottenere *testa*) pari a $p = 0.2$.\n\nVogliamo calcolare:\n\n- la media attesa del numero di teste,\n- la varianza,\n- e la deviazione standard.\n\n1. Calcolo del valore atteso (media):\n\n$$\n\\mu = n \\cdot p = 4 \\cdot 0.2 = 0.8 .\n$$\n\nQuindi, **in media**, ci aspettiamo di ottenere **0.8 teste ogni 4 lanci** (cio√® meno di 1, ma ricordiamo che si tratta di una **media**).\n\n2. Calcolo della varianza:\n\n$$\n\\text{Varianza} = n \\cdot p \\cdot (1 - p) = 4 \\cdot 0.2 \\cdot 0.8 = 0.64 .\n$$\n\n3. Calcolo della deviazione standard:\n\n$$\n\\sigma = \\sqrt{0.64} \\approx 0.8 .\n$$\n\nLa deviazione standard ci d√† un'idea della **variabilit√†** dei risultati: in questo caso, i valori osservati (numero di teste su 4 lanci) si discostano dalla media di circa 0.8 in media.\n:::\n\n### Verifica con una simulazione in R\n\nPer vedere se i calcoli teorici dell'@exm-binom-var funzionano anche nella pratica, possiamo **simulare** l‚Äôesperimento in R: lanciamo 4 monete, ma lo facciamo **tantissime volte** (ad esempio 1 milione) e calcoliamo la media e la varianza dei risultati ottenuti.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\n\n# Generiamo 1 milione di esperimenti: 4 lanci con probabilit√† di successo 0.2\nx <- rbinom(n = 1e6, size = 4, prob = 0.2)\n\n# Calcoliamo la media empirica\nmean(x)\n#> [1] 0.8\n# [1] circa 0.8\n\n# Calcoliamo la varianza empirica\nvar(x)\n#> [1] 0.639\n# [1] circa 0.64\n```\n:::\n\n\nCome possiamo vedere, i risultati ottenuti dalla simulazione sono **molto vicini ai valori teorici**: la media √® circa $\\mu = 0.8$ e la varianza circa $0.64$, proprio come previsto dalle formule.\n\nQuesto non solo conferma che le **formule per media e varianza nella distribuzione binomiale sono corrette**, ma ci aiuta anche a capire meglio **cosa significano**: \n\n- il **valore atteso** rappresenta la **media dei risultati** se ripetiamo l'esperimento moltissime volte;  \n- la **varianza** (e la sua radice quadrata, la **deviazione standard**) misura **quanto i risultati si allontanano dalla media**.\n\nLa simulazione mostra quindi in modo concreto che **il valore atteso e la varianza descrivono il comportamento \"medio\" della variabile aleatoria**, quando viene osservata in un numero molto grande di situazioni. In altre parole, questi concetti non sono solo teorici: ci dicono cosa aspettarci nella pratica, se ripetiamo molte volte lo stesso esperimento.\n\n\n## Funzioni R per le distribuzioni di probabilit√†\n\nIn R, le distribuzioni di probabilit√† (sia discrete che continue) sono gestite in modo sistematico. Per ogni distribuzione, esistono quattro funzioni principali, ognuna con un prefisso diverso che indica il tipo di operazione desiderata:\n\n- **`d*`**: calcola la *densit√†* (per distribuzioni continue) o la *probabilit√†* (per distribuzioni discrete);  \n- **`p*`**: calcola la *funzione di ripartizione cumulativa* (CDF), cio√® $P(Y \\leq y)$;  \n- **`q*`**: calcola la *funzione quantile* (inversa della CDF);  \n- **`r*`**: genera *valori casuali* secondo la distribuzione specificata.\n\nQuesta struttura √® identica per tutte le distribuzioni implementate in R. La tabella seguente mostra un confronto tra le funzioni disponibili per due distribuzioni fondamentali: la **binomiale** (discreta) e la **normale** (continua).\n\n| Tipo di funzione                   | Binomiale ($Y \\sim \\text{Bin}(n, p)$)         | Normale ($Y \\sim \\mathcal{N}(\\mu, \\sigma)$)         |\n|:----------------------------------|:----------------------------------------------|:--------------------------------------------------|\n| Densit√† o probabilit√† esatta      | `dbinom(y, size = n, prob = p)`               | `dnorm(y, mean = mu, sd = sigma)`                 |\n| $P(Y = y)$                        | `dbinom(...)`                                 | ‚ùå Non definita: per variabili continue si usa la densit√† |\n| Probabilit√† cumulativa            | `pbinom(y, size = n, prob = p)`               | `pnorm(y, mean = mu, sd = sigma)`                |\n| $P(Y \\geq y)$                     | `1 - pbinom(y - 1, ...)`                       | `1 - pnorm(y, ...)`                               |\n| $P(y_1 < Y < y_2)$                | `pbinom(y2, ...) - pbinom(y1, ...)`            | `pnorm(y2, ...) - pnorm(y1, ...)`                |\n| Quantile (inversa della CDF)      | `qbinom(q, size = n, prob = p)`               | `qnorm(q, mean = mu, sd = sigma)`                |\n| Simulazione di dati casuali       | `rbinom(n, size = trials, prob = p)`          | `rnorm(n, mean = mu, sd = sigma)`                |\n\n::: {.callout-tip title=\"Nota\"}\n- Per le **distribuzioni discrete** (come la binomiale), `dbinom(y, ...)` restituisce la **probabilit√† esatta** di osservare il valore $y$: ad esempio, $P(Y = 2)$.\n- Per le **distribuzioni continue** (come la normale), `dnorm(y, ...)` restituisce la **densit√†** in $y$, che non rappresenta direttamente una probabilit√†, ma √® utile per visualizzare la forma della distribuzione.\n- Le probabilit√† cumulative (funzioni `p*`) e i quantili (funzioni `q*`) sono sempre definiti, sia per distribuzioni discrete che continue.\n- La generazione di dati casuali con `r*` √® molto utile per simulazioni e verifiche empiriche.\n\nPi√π avanti, vedremo altre distribuzioni (Uniforme, Beta, Poisson, ecc.), tutte con lo stesso schema di funzioni: `d`, `p`, `q`, `r`.\n\nQuesta coerenza rende molto semplice imparare a usare le distribuzioni in R: una volta compreso lo schema, lo si pu√≤ applicare a qualsiasi caso.\n:::\n\n::: {#exm-}\n\n1. Calcolare la probabilit√† di esattamente $y = 3$ successi su $n = 5$ prove con $p = 0.5$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndbinom(3, size = 5, prob = 0.5)\n#> [1] 0.312\n```\n:::\n\n\n2. Calcolare la probabilit√† cumulativa $P(Y \\leq 3)$:\n   \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npbinom(3, size = 5, prob = 0.5)\n#> [1] 0.812\n```\n:::\n\n\n3. Calcolare il valore minimo $y$ tale che $P(Y \\leq y) \\geq 0.9$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqbinom(0.9, size = 5, prob = 0.5)\n#> [1] 4\n```\n:::\n\n\n4. Generare un campione di 100 numeri casuali da una distribuzione binomiale:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrbinom(100, size = 5, prob = 0.5)\n#>   [1] 2 2 4 1 3 2 3 2 2 2 3 3 3 3 1 4 1 1 0 2 2 2 4 1 2 3 3 1 5 2 3 3 0 3 2 3 4\n#>  [38] 2 3 3 3 3 1 5 3 3 2 3 1 2 1 3 3 2 2 4 1 4 2 3 1 4 2 2 3 4 1 1 4 2 3 2 3 3\n#>  [75] 3 1 4 4 3 3 4 3 3 3 2 2 3 3 2 4 4 3 2 2 0 3 1 3 1 2\n```\n:::\n\n:::\n\n\n## Distribuzione di Poisson \n\nLa **distribuzione di Poisson** √® utilizzata per modellare il numero di eventi che si verificano in un determinato intervallo di tempo o spazio, con eventi indipendenti e un tasso costante di occorrenza.\n\nLa funzione di massa di probabilit√† (PMF) √® data da:\n\n$$\nP(Y = y \\mid \\lambda) = \\frac{\\lambda^y \\cdot e^{-\\lambda}}{y!}, \\quad y = 0, 1, 2, \\ldots\n$$\n\ndove $\\lambda$ rappresenta il tasso medio di eventi e $y$ √® il numero di eventi.\n\nLa distribuzione di Poisson pu√≤ essere derivata come il limite di una distribuzione binomiale quando il numero di prove, $n$, tende all'infinito e la probabilit√† di successo in ciascuna prova, $p$, tende a zero, in modo tale che $np = \\lambda$. \n\n::: callout-note\n## Dimostrazione\n\nPartiamo dalla funzione di probabilit√† binomiale:\n\n$$\np(k) = \\frac{n!}{k!(n - k)!} p^k (1 - p)^{n - k}.\n$$\n\nImpostiamo $np = \\lambda$, il che implica che $p = \\frac{\\lambda}{n}$. Sostituendo $p$ con $\\frac{\\lambda}{n}$ nella formula binomiale, otteniamo:\n\n$$\np(k) = \\frac{n!}{k!(n - k)!} \\left(\\frac{\\lambda}{n}\\right)^k \\left(1 - \\frac{\\lambda}{n}\\right)^{n - k}.\n$$\n\nOra, separiamo i termini per rendere pi√π chiara la semplificazione. Possiamo riscrivere $\\left(\\frac{\\lambda}{n}\\right)^k$ come $\\frac{\\lambda^k}{n^k}$, e $\\left(1 - \\frac{\\lambda}{n}\\right)^{n - k}$ come $\\left(1 - \\frac{\\lambda}{n}\\right)^n \\cdot \\left(1 - \\frac{\\lambda}{n}\\right)^{-k}$. Quindi, l'espressione diventa:\n\n$$\np(k) = \\frac{n!}{k!(n - k)!} \\cdot \\frac{\\lambda^k}{n^k} \\cdot \\left(1 - \\frac{\\lambda}{n}\\right)^n \\cdot \\left(1 - \\frac{\\lambda}{n}\\right)^{-k}.\n$$\n\nOra, separiamo ulteriormente i termini:\n\n$$\np(k) = \\frac{\\lambda^k}{k!} \\cdot \\frac{n!}{(n - k)! n^k} \\cdot \\left(1 - \\frac{\\lambda}{n}\\right)^n \\cdot \\left(1 - \\frac{\\lambda}{n}\\right)^{-k}.\n$$\n\nQuesto passaggio mostra come la funzione di probabilit√† binomiale, sotto le condizioni $np = \\lambda$ e $n \\to \\infty$, si trasformi gradualmente nella forma che conduce alla distribuzione di Poisson.\n\nQuando $n \\to \\infty$:\n\n$$\n\\frac{\\lambda}{n} \\to 0\n$$\n\n$$\n\\frac{n!}{(n - k)! n^k} \\to 1\n$$\n\n$$\n\\left(1 - \\frac{\\lambda}{n}\\right)^n \\to e^{-\\lambda}\n$$\n\n$$\n\\left(1 - \\frac{\\lambda}{n}\\right)^{-k} \\to 1\n$$\n\nSi ottiene quindi:\n\n$$\np(k) \\to \\frac{\\lambda^k e^{-\\lambda}}{k!}\n$$\n\nche √® la funzione di Poisson.\n:::\n\n::: callout-note\n## Dimostrazione\n\nAnalizziamo il limite:\n\n$$\n\\left(1 - \\frac{\\lambda}{n}\\right)^n \\to e^{-\\lambda} \\quad \\text{quando} \\quad n \\to \\infty.\n$$\n\nUn limite fondamentale in analisi matematica √®:\n\n$$\n\\lim_{n \\to \\infty} \\left(1 + \\frac{a}{n}\\right)^n = e^a,\n$$\n\ndove $e$ √® la base del logaritmo naturale ($e \\approx 2.71828$) e $a$ √® una costante. Questo limite √® alla base della definizione della funzione esponenziale.\n\nNel nostro caso, abbiamo l'espressione:\n\n$$\n\\left(1 - \\frac{\\lambda}{n}\\right)^n .\n$$\n\nNotiamo che questa √® molto simile al limite notevole, ma con un segno negativo. Possiamo riscriverla come:\n\n$$\n\\left(1 - \\frac{\\lambda}{n}\\right)^n = \\left(1 + \\frac{-\\lambda}{n}\\right)^n.\n$$\n\nApplicando il limite notevole con $a = -\\lambda$, otteniamo:\n\n$$\n\\lim_{n \\to \\infty} \\left(1 + \\frac{-\\lambda}{n}\\right)^n = e^{-\\lambda}.\n$$\n\nQuindi, quando $n$ diventa molto grande, l'espressione $\\left(1 - \\frac{\\lambda}{n}\\right)^n$ si avvicina sempre di pi√π a $e^{-\\lambda}$. \n:::\n\n### Propriet√† principali\n\n- **Media**: $\\mathbb{E}[Y] = \\lambda$\n- **Varianza**: $\\text{Var}(Y) = \\lambda$\n\nDi seguito, presentiamo esempi di calcolo e simulazione con R.\n\n\n### Grafico della distribuzione di Poisson con $\\lambda = 2$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametro lambda\nlambda <- 2\n\n# Valori di y (numero di eventi)\ny <- 0:10\n\n# Calcolo delle probabilit√†\nprobabilities <- dpois(y, lambda = lambda)\n\n# Creazione di un dataframe per ggplot\ndata <- data.frame(\n  Numero_eventi = y,\n  Probabilita = probabilities\n)\n\n# Grafico della funzione di massa di probabilit√† \nggplot(data, aes(x = Numero_eventi, y = Probabilita)) +\n  geom_col() +  \n  labs(\n    x = \"Numero di eventi (k)\",\n    y = \"Probabilit√†\"\n  ) \n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-23-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Calcolo della probabilit√† per un numero specifico di eventi\n\nPer calcolare la probabilit√† di osservare esattamente 3 eventi con $\\lambda = 2$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprob <- dpois(3, lambda = 2)\nprint(prob)\n#> [1] 0.18\n```\n:::\n\n\n### Calcolo della probabilit√† cumulativa $P(Y \\leq 3)$\n\nPer calcolare $P(Y \\leq 3)$, la probabilit√† cumulativa:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncum_prob <- ppois(3, lambda = 2)\nprint(cum_prob)\n#> [1] 0.857\n```\n:::\n\n\n### Trovare il quantile corrispondente a una probabilit√† data\n\nPer trovare il numero massimo di eventi per cui la probabilit√† cumulativa √® al massimo $0.8125$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nquantile <- qpois(0.8125, lambda = 2)\nprint(quantile)\n#> [1] 3\n```\n:::\n\n\n### Generazione di numeri casuali\n\nPer generare un campione di 1.000.000 di osservazioni da una distribuzione di Poisson con $\\lambda = 2$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\nsample <- rpois(1000000, lambda = 2)\n\n# Calcolo di media e varianza del campione\nmean_sample <- mean(sample)\nvar_sample <- var(sample)\n\nprint(mean_sample)\n#> [1] 2\nprint(var_sample)\n#> [1] 2\n```\n:::\n\n\n\n\n::: {#exm-poisson-v2-london}\n\nUn esempio classico dell'uso della distribuzione di Poisson viene dalla **Seconda Guerra Mondiale**.\n\n**Il contesto storico.** Tra il 1944 e il 1945, Londra fu colpita da centinaia di **missili V1 e V2** lanciati dalla Germania nazista. Le autorit√† britanniche si chiesero se i bombardamenti seguissero una **strategia mirata**: i missili venivano forse lanciati intenzionalmente su certi quartieri? O si trattava invece di **un comportamento casuale**, come se fossero stati distribuiti a caso?\n\nPer rispondere a questa domanda, il Ministero della Guerra britannico **divise Londra in 576 aree di uguale superficie** (ogni area misurava 0.25 km¬≤) e **registr√≤ quanti missili avevano colpito ciascuna area**. I dati furono poi analizzati dal matematico R. D. Clarke, che li pubblic√≤ nel 1946.\n\n**I dati osservati.** Ecco una sintesi della distribuzione osservata:\n\n| Missili per area | Numero di aree | Frequenza relativa |\n|------------------|----------------|---------------------|\n| 0                | 229            | 0.398               |\n| 1                | 211            | 0.367               |\n| 2                | 93             | 0.161               |\n| 3                | 35             | 0.061               |\n| 4                | 7              | 0.012               |\n| ‚â•5               | 1              | 0.002               |\n\nIl **numero medio di missili per area** era $\\lambda \\approx 0.93$. L'idea era confrontare queste frequenze con le probabilit√† teoriche previste da una distribuzione di Poisson con media $\\lambda = 0.93$.\n\n**Interpretazione con la distribuzione di Poisson.** Utilizzando la funzione `dpois()` in R, possiamo calcolare le probabilit√† teoriche per ciascun valore osservato, da 0 a 4 missili per area (valori superiori sono troppo rari per essere trattati separatamente).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametro medio osservato\nlambda <- 0.93\n\n# Valori possibili di missili per area\ny <- 0:4\n\n# Probabilit√† teoriche secondo la distribuzione di Poisson\nprob_teoriche <- dpois(y, lambda = lambda)\n\n# Aggiungiamo la probabilit√† per y >= 5\nprob_teoriche <- c(prob_teoriche, 1 - sum(prob_teoriche))  # y >= 5\n\n# Visualizziamo\ndata.frame(\n  Missili_per_area = c(0:4, \">=5\"),\n  Probabilita_teorica = round(prob_teoriche, 3)\n)\n#>   Missili_per_area Probabilita_teorica\n#> 1                0               0.395\n#> 2                1               0.367\n#> 3                2               0.171\n#> 4                3               0.053\n#> 5                4               0.012\n#> 6              >=5               0.003\n```\n:::\n\n\nConfrontando le probabilit√† teoriche della Poisson con quelle osservate nei dati reali, **i risultati erano sorprendentemente simili**. Questo suggeriva che **i missili non erano lanciati su bersagli specifici**, ma seguivano un comportamento **statisticamente compatibile con una distribuzione casuale**.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Frequenze osservate (dati originali di Clarke, 1946)\nfrequenze_osservate <- c(229, 211, 93, 35, 7, 1)\nvalori_missili <- c(0, 1, 2, 3, 4, \"‚â•5\")\n\n# Calcolo frequenze teoriche con Poisson (lambda = 0.93)\nlambda <- 0.93\nprob_teoriche <- dpois(0:4, lambda)\nprob_teoriche <- c(prob_teoriche, 1 - sum(prob_teoriche))  # Per y >= 5\n\n# Numero totale di aree (come somma delle osservazioni)\nn_aree <- sum(frequenze_osservate)\n\n# Frequenze attese = probabilit√† teoriche * numero totale di aree\nfrequenze_attese <- round(prob_teoriche * n_aree)\n\n# Costruzione del data frame\ndf <- data.frame(\n  Missili_per_area = factor(valori_missili, levels = c(\"0\", \"1\", \"2\", \"3\", \"4\", \"‚â•5\")),\n  Osservate = frequenze_osservate,\n  Attese = frequenze_attese\n)\n\n# Conversione in formato lungo per ggplot2\ndf_long <- reshape2::melt(df, id.vars = \"Missili_per_area\", variable.name = \"Tipo\", value.name = \"Frequenza\")\n\n# Creazione del grafico\nggplot(df_long, aes(x = Missili_per_area, y = Frequenza, fill = Tipo)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    x = \"Numero di missili per area\",\n    y = \"Numero di aree\",\n    fill = \"Frequenza\"\n  ) \n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n- Le **barre blu** rappresentano le **frequenze osservate** (quante aree hanno ricevuto 0, 1, 2... missili).\n- Le **barre rosse** mostrano le **frequenze attese** se i missili fossero stati lanciati in modo **completamente casuale**, seguendo una distribuzione di **Poisson con $\\lambda = 0.93$**.\n\nLa sovrapposizione tra i due andamenti √® molto buona, il che rafforza l‚Äôidea che i bombardamenti fossero distribuiti casualmente ‚Äî **senza un pattern strategico** apparente.\n\n**Cosa ci insegna questo esempio?**\n\n- La distribuzione di Poisson √® adatta quando vogliamo **modellare eventi rari e indipendenti** nello spazio o nel tempo.\n- I dati dei missili su Londra mostrano come un fenomeno che a prima vista potrebbe sembrare non casuale (per via della concentrazione locale degli eventi) possa invece essere **ben descritto da un modello probabilistico semplice**, se considerato su una scala adatta.\n:::\n\n:::{#exm-}\nSupponiamo di avere osservato, nel corso degli anni, che la **frequenza relativa di bocciature** all‚Äôesame di **Psicometria** √® di circa **10%** (cio√® $p = 0.1$). Tuttavia, il **numero di studenti iscritti a ciascun appello varia in modo estremo**: al primo appello dell‚Äôanno partecipano pi√π di 200 studenti, mentre negli ultimi appelli solo 2 o 3.\n\nQuesto rende **inadeguato l‚Äôuso della distribuzione binomiale**, che richiede un numero di prove ($n$) fisso o noto per ciascun appello.\n\nIn questi casi, possiamo **modellare il numero di bocciature per appello** usando una distribuzione di Poisson.\n\nPer ogni appello, possiamo stimare $\\lambda$ moltiplicando il **numero di studenti iscritti** ($n$) per la **frequenza attesa di bocciature** ($p = 0.1$). A quel punto, il numero di bocciature osservate pu√≤ essere approssimato da:\n\n$$\nY \\sim \\text{Poisson}(\\lambda = n \\cdot p) .\n$$\n\nQuindi la distribuzione cambia da appello ad appello, perch√© **$\\lambda$ cambia con $n$**, ma il **modello rimane Poissoniano**.\n\nSupponiamo di avere osservato i seguenti dati.\n\n| Appello | Numero iscritti ($n$) | $\\lambda = n \\cdot p$ | Distribuzione di bocciature |\n|--------:|-----------------------:|-----------------------:|-----------------------------|\n| 1       | 220                   | $220 \\cdot 0.1 = 22$   | $Y \\sim \\text{Poisson}(22)$ |\n| 2       | 95                    | $95 \\cdot 0.1 = 9.5$    | $Y \\sim \\text{Poisson}(9.5)$ |\n| 8       | 3                     | $3 \\cdot 0.1 = 0.3$     | $Y \\sim \\text{Poisson}(0.3)$ |\n\nPer ogni appello, possiamo usare la funzione `dpois()` in R per calcolare la probabilit√† di osservare un certo numero di bocciature, dato il valore di $\\lambda$ specifico per quell‚Äôappello.\n\nAd esempio, possiamo chiederci quale sia la probabilit√† che, nel secondo appello (95 iscritti), si registrino esattamente 8 bocciature.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlambda <- 95 * 0.1  # = 9.5\ndpois(8, lambda = lambda)\n#> [1] 0.123\n```\n:::\n\n\nQuesta funzione calcola $P(Y = 8)$ per una variabile $Y \\sim \\text{Poisson}(9.5)$, cio√® la probabilit√† di osservare esattamente 8 bocciature su 95 iscritti.\n\nSupponiamo di voler simulare il numero di bocciature in **8 appelli** con numeri di iscritti variabili. Possiamo fare cos√¨:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\n\n# Numero iscritti per ciascun appello\nn_iscritti <- c(220, 95, 60, 45, 20, 12, 6, 3)\n\n# Probabilit√† storica di bocciatura\np <- 0.1\n\n# Parametri lambda per ogni appello\nlambda <- n_iscritti * p\n\n# Simulazione delle bocciature per ciascun appello\nbocciature <- rpois(length(lambda), lambda = lambda)\n\ndata.frame(\n  Appello = 1:8, \n  Iscritti = n_iscritti, \n  Lambda = lambda, \n  Bocciature = bocciature\n)\n#>   Appello Iscritti Lambda Bocciature\n#> 1       1      220   22.0         28\n#> 2       2       95    9.5          8\n#> 3       3       60    6.0          8\n#> 4       4       45    4.5          5\n#> 5       5       20    2.0          2\n#> 6       6       12    1.2          2\n#> 7       7        6    0.6          0\n#> 8       8        3    0.3          0\n```\n:::\n\n\nüìà **Visualizzazione.**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf <- data.frame(Appello = factor(1:8), Bocciature = bocciature)\n\nggplot(df, aes(x = Appello, y = Bocciature)) +\n  geom_col() +\n  labs(\n    x = \"Appello\",\n    y = \"Numero di bocciature\"\n  )\n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-32-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIn sintesi,\n\n- quando il numero di studenti iscritti a un appello **non √® noto a priori** o **varia fortemente**, non √® adeguato usare la distribuzione binomiale;\n- se conosciamo la **frequenza relativa di bocciature** (es. $p = 0.1$), possiamo usare la distribuzione di **Poisson con $\\lambda = n \\cdot p$**, adattandola a ciascun appello;\n- questo approccio √® particolarmente utile per fare **stima e simulazione** del numero di bocciature attese, senza dover modellare tutti i singoli esiti.\n:::\n\n::: {#exm-}\nUno degli esempi pi√π comuni per introdurre la distribuzione di Poisson riguarda il numero di **nascite giornaliere** in un ospedale.\n\nSupponiamo che, in un grande ospedale, la **media storica** sia di **4.5 nascite al giorno**. Possiamo allora descrivere il numero di nascite in un giorno con una **variabile casuale Poisson** con parametro $\\lambda = 4.5$:\n\n$$\nY \\sim \\text{Poisson}(\\lambda = 4.5) .\n$$\n\nCi chiediamo, ad esempio: qual √® la probabilit√† che in un giorno nascano **esattamente 6 bambini**?\n\nPossiamo calcolarla con la funzione `dpois()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametro medio: 4.5 nascite al giorno\nlambda <- 4.5\n\n# Probabilit√† di osservare esattamente 6 nascite\nprob <- dpois(6, lambda = lambda)\nprint(prob)\n#> [1] 0.128\n```\n:::\n\n\nQuesto valore rappresenta la probabilit√† che, in un giorno qualsiasi, si verifichino **esattamente 6 nascite**.\n\n**Simulazione.** Simuliamo ora il numero di nascite in **365 giorni consecutivi**, supponendo che la media rimanga costante a 4.5:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)  # Per rendere i risultati riproducibili\n\nn_days <- 365\nsimulated_births <- rpois(n_days, lambda = lambda)\n\n# Proporzione di giorni con esattamente 6 nascite\nproportion_six_births <- mean(simulated_births == 6)\nprint(proportion_six_births)\n#> [1] 0.14\n```\n:::\n\n\nQuesto ci dice, tra i 365 giorni simulati, **quanta parte dell‚Äôanno ha avuto esattamente 6 nascite**. Il valore ottenuto pu√≤ essere confrontato con la probabilit√† teorica calcolata prima.\n\n**Visualizzazione.** Possiamo rappresentiamo graficamente i dati simulati con un istogramma:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Costruzione del data frame\ndata <- data.frame(Nascite = simulated_births)\n\n# Istogramma\nggplot(data, aes(x = Nascite)) +\n  geom_histogram(\n    breaks = seq(-0.5, max(simulated_births) + 0.5, by = 1)\n  ) +\n  labs(\n    x = \"Numero di nascite per giorno\",\n    y = \"Frequenza (numero di giorni)\"\n  )\n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-35-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nL‚Äôistogramma mostra quante volte si sono verificati 0, 1, 2, ..., 10 o pi√π nascite in un giorno, evidenziando la **variabilit√† naturale** attorno alla media.\n\nCalcoliamo ora quanto √® probabile che si verifichino **pi√π di 6 nascite in un giorno**.\n\nProbabilit√† teorica:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprob_more_than_six <- 1 - ppois(6, lambda = lambda)\nprint(prob_more_than_six)\n#> [1] 0.169\n```\n:::\n\n\nProporzione osservata nella simulazione:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nproportion_more_than_six <- mean(simulated_births > 6)\nprint(proportion_more_than_six)\n#> [1] 0.17\n```\n:::\n\n\nIl confronto tra probabilit√† teorica e proporzione simulata mostra come la distribuzione di Poisson **riproduca bene i fenomeni reali**, quando gli eventi sono **indipendenti**, **discreti** e **relativamente frequenti ma non troppo**.\n:::\n\n::: {#exm-}\nQuesto esempio √® tratto dal celebre lavoro di **Ladislaus von Bortkiewicz** del 1898, spesso citato come una delle prime applicazioni reali della **distribuzione di Poisson**.\n\nVon Bortkiewicz studi√≤ un evento piuttosto inusuale: le **morti causate da calci di cavallo** all‚Äôinterno della **cavalleria dell‚Äôesercito prussiano**. L'obiettivo era capire se questi eventi, seppur rari, potessero essere considerati **casuali e indipendenti**, oppure se fossero distribuiti in modo irregolare e non prevedibile.\n\nPer farlo, raccolse i dati su **10 squadroni** osservati per **20 anni consecutivi**, ottenendo cos√¨ **200 unit√† di osservazione**, che possiamo chiamare **\"squadroni-anno\"**.\n\n**I dati raccolti.** Per ogni squadrone-anno, fu registrato il **numero di morti per calci di cavallo**. I dati furono poi raggruppati per numero di decessi:\n\n| Numero di decessi annui | Frequenza osservata | Frequenza relativa | Probabilit√† teorica (Poisson) |\n|-------------------------|---------------------|--------------------|-------------------------------|\n| 0                       | 109                 | 0.545              | 0.543                         |\n| 1                       | 65                  | 0.325              | 0.331                         |\n| 2                       | 22                  | 0.110              | 0.101                         |\n| 3                       | 3                   | 0.015              | 0.021                         |\n| 4                       | 1                   | 0.005              | 0.003                         |\n\n- **Frequenza osservata**: Quante volte ciascun numero di decessi √® stato osservato tra i 200 squadroni-anno.\n- **Frequenza relativa**: Frequenza osservata divisa per 200.\n- **Probabilit√† teorica**: Calcolata con la **distribuzione di Poisson con parametro $\\lambda = 0.61$**, pari alla media osservata dei decessi annui.\n\nLa distribuzione di Poisson √® perfetta per questo tipo di situazione perch√©:\n\n- stiamo contando il numero di **eventi rari** (decessi accidentali),\n- che si verificano in **unit√† di tempo o spazio fisse** (lo \"squadrone-anno\"),\n- e presumiamo che questi eventi siano **indipendenti tra loro**.\n\nIn questo caso, $\\lambda = 0.61$ rappresenta il **numero medio di decessi per squadrone in un anno**. La variabilit√† intorno a questo valore pu√≤ essere descritta dalla distribuzione di Poisson, che assegna a ciascun possibile numero di decessi (0, 1, 2, ‚Ä¶) una **probabilit√† teorica**.\n\n**Confronto tra dati osservati e modello di Poisson.** Come si pu√≤ notare dalla tabella, **le frequenze osservate sono sorprendentemente simili** alle probabilit√† teoriche ottenute dal modello di Poisson. Ad esempio:\n\n- la proporzione di squadroni-anno con **zero decessi** √® 0.545, contro una probabilit√† teorica di 0.543;\n- per **un decesso**, la frequenza relativa √® 0.325, vicina alla probabilit√† teorica di 0.331;\n- anche le classi meno frequenti (2, 3 e 4 decessi) sono coerenti con i valori attesi.\n\nQuesto esempio dimostra che la distribuzione di Poisson **non solo √® utile per modellare eventi rari**, ma fornisce anche una buona descrizione **quantitativa** del comportamento osservato nel mondo reale.\n\nIn sintesi,\n\n- il lavoro di von Bortkiewicz √® uno dei primi esempi storici di **modellizzazione di dati reali con la teoria delle probabilit√†**;\n- la distribuzione di Poisson si √® rivelata efficace nel **descrivere un fenomeno raro, ma regolare**, suggerendo che i decessi fossero eventi **casuali e indipendenti**, non dovuti a fattori sistematici;\n- ancora oggi, questo esempio viene usato per insegnare che anche gli eventi accidentali e poco frequenti possono essere **prevedibili in media** e descritti in modo elegante da un modello probabilistico.\n\nQui di seguito viene fornito il **codice R** che riproduce l‚Äôanalisi di von Bortkiewicz, calcola le **probabilit√† teoriche** secondo la distribuzione di Poisson con parametro $\\lambda = 0.61$ e confronta visivamente le **frequenze osservate** con le **frequenze attese**.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Dati osservati da von Bortkiewicz\ndecessi <- 0:4\nfrequenze_osservate <- c(109, 65, 22, 3, 1)\nn_total <- sum(frequenze_osservate)  # Totale = 200 squadroni-anno\n\n# Frequenze relative\nfrequenze_relative <- frequenze_osservate / n_total\n```\n:::\n\n\nCalcolo delle probabilit√† teoriche con la distribuzione di Poisson:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametro medio osservato\nlambda <- 0.61\n\n# Calcolo delle probabilit√† teoriche di Poisson\nprob_poisson <- dpois(decessi, lambda = lambda)\n```\n:::\n\n\nConfronto: osservato vs teorico.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Frequenze attese = probabilit√† teoriche * numero totale di casi\nfrequenze_attese <- round(prob_poisson * n_total)\n\n# Creazione del data frame per il confronto\ndf <- data.frame(\n  Decessi = factor(decessi),\n  Osservato = frequenze_osservate,\n  Atteso = frequenze_attese\n)\ndf\n#>   Decessi Osservato Atteso\n#> 1       0       109    109\n#> 2       1        65     66\n#> 3       2        22     20\n#> 4       3         3      4\n#> 5       4         1      1\n```\n:::\n\n\nVisualizzazione: confronto tra frequenze osservate e attese.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Conversione da wide a long format con pivot_longer()\ndf_long <- df |> \n  pivot_longer(\n    cols = c(Osservato, Atteso),\n    names_to = \"Tipo\",\n    values_to = \"Frequenza\"\n  )\n\n# Mostra le prime righe\nhead(df_long)\n#> # A tibble: 6 √ó 3\n#>   Decessi Tipo      Frequenza\n#>   <fct>   <chr>         <dbl>\n#> 1 0       Osservato       109\n#> 2 0       Atteso          109\n#> 3 1       Osservato        65\n#> 4 1       Atteso           66\n#> 5 2       Osservato        22\n#> 6 2       Atteso           20\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Grafico a barre affiancate\nggplot(df_long, aes(x = Decessi, y = Frequenza, fill = Tipo)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(\n    x = \"Numero di decessi per squadrone-anno\",\n    y = \"Frequenza\",\n    fill = \"Tipo\"\n  ) \n```\n\n::: {.cell-output-display}\n![](13_discr_rv_distr_files/figure-html/unnamed-chunk-42-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n- Le **barre blu** mostrano i dati **osservati** da von Bortkiewicz.\n- Le **barre rosse** indicano le **frequenze attese** se il numero di decessi segue una distribuzione di Poisson con media $\\lambda = 0.61$.\n- La **buona corrispondenza visiva** tra le due serie supporta l‚Äôidea che i decessi siano **eventi rari, indipendenti e distribuiti casualmente**.\n\n:::\n\n## Distribuzione Beta-Binomiale\n\nLa distribuzione beta-binomiale rappresenta una estensione della distribuzione binomiale che tiene conto della variabilit√† nella probabilit√† di successo tra i vari tentativi. Viene descritta da tre parametri principali: $N$, $\\alpha$ e $\\beta$.\n\nNel dettaglio, la funzione di massa di probabilit√† per la distribuzione beta-binomiale √® data da:\n\n$$\n\\text{BetaBinomiale}(y | N, \\alpha, \\beta) = \\binom{N}{y} \\cdot \\frac{B(y + \\alpha, N - y + \\beta)}{B(\\alpha, \\beta)},\n$$ {#eq-beta-binom-formula}\n\ndove:\n\n- $y$ indica il numero di successi osservati.\n- $N$ rappresenta il numero totale di tentativi.\n- $\\alpha$ e $\\beta$ sono i parametri della distribuzione beta, che modellano la variabilit√† nella probabilit√† di successo tra i tentativi.\n\nLa funzione $B(u, v)$, nota come funzione beta, √® definita tramite l'uso della funzione gamma $\\Gamma$, secondo la formula:\n\n$$\nB(u, v) = \\frac{\\Gamma(u) \\Gamma(v)}{\\Gamma(u + v)},\n$$\n\ndove la funzione gamma $\\Gamma$ generalizza il concetto di fattoriale a numeri reali e complessi.\n\nL'importanza della distribuzione beta-binomiale deriva dalla sua capacit√† di modellare situazioni in cui la probabilit√† di successo non √® fissa, ma segue una distribuzione di probabilit√†, specificatamente una distribuzione beta. Ci√≤ la rende particolarmente adatta per applicazioni in cui le probabilit√† di successo cambiano in maniera incerta da un tentativo all'altro, come pu√≤ avvenire in contesti di ricerca clinica o in studi comportamentali. Rispetto alla distribuzione binomiale, che assume una probabilit√† di successo costante per tutti i tentativi, la beta-binomiale offre una rappresentazione pi√π realistica e flessibile per dati empirici che presentano variabilit√† nelle probabilit√† di successo.\n\n## Riflessioni Conclusive\n\nIn questo capitolo, abbiamo approfondito alcune delle distribuzioni discrete pi√π importanti, ognuna con caratteristiche uniche e campi di applicazione specifici. Abbiamo iniziato con la **distribuzione di Bernoulli**, che modella esperimenti con due soli esiti possibili, per poi passare alla **distribuzione Binomiale**, che generalizza la Bernoulli considerando un numero fisso di prove indipendenti. Successivamente, abbiamo esaminato la **distribuzione di Poisson**, utile per descrivere eventi rari in un intervallo di tempo o spazio, e la **distribuzione Beta-Binomiale**, un'estensione della Binomiale che incorpora la variabilit√† nella probabilit√† di successo, rendendola particolarmente adatta per modellare situazioni in cui tale probabilit√† non √® fissa. Infine, abbiamo discusso la **distribuzione Discreta Uniforme**, che assegna la stessa probabilit√† a ciascun evento in un insieme finito e discreto.\n\nQueste distribuzioni rappresentano il fondamento dell'analisi statistica discreta e trovano applicazione in numerosi ambiti. In particolare, nel contesto dell'**inferenza bayesiana**, la comprensione della distribuzione Binomiale e della sua estensione Beta-Binomiale √® essenziale. Queste distribuzioni, infatti, forniscono gli strumenti necessari per l'**aggiornamento bayesiano**, un processo chiave che permette di rivedere le nostre credenze iniziali alla luce di nuovi dati. Questo concetto sar√† ulteriormente esplorato nei capitoli successivi, dove approfondiremo come le distribuzioni a priori e a posteriori interagiscono nel quadro bayesiano.\n\n## Esercitazione in Classe\n\nValutate le emozioni che verranno presentate sullo schermo usando questo [link](https://docs.google.com/forms/d/e/1FAIpQLScWZD9XQoWJvf58fvQ6KGPJ562JxiKeg_azCIWagi1_BVtdpg/viewform?usp=header).\n\nScala di risposta:\n\n- Rabbia: 1\n- Disgusto: 2\n- Paura: 3\n- Felicit√†: 4\n- Tristezza: 5\n\n\n## Esercizi {.unnumbered} \n\n::: {.callout-important title=\"Problemi 1\" collapse=\"true\"}\nPer ciascuna delle distribuzioni di massa di probabilit√† discusse, utilizzare R per:\n\n- creare un grafico della funzione, scegliendo opportunamente i parametri;\n- estrarre un campione di 1000 valori casuali dalla distribuzione e visualizzarlo con un istogramma;\n- calcolare la media e la deviazione standard dei campioni e confrontarle con i valori teorici attesi;\n- stimare l'intervallo centrale del 94% utilizzando i campioni simulati;\n- determinare i quantili della distribuzione per gli ordini 0.05, 0.25, 0.75 e 0.95;\n- scegliendo un valore della distribuzione pari alla media pi√π una deviazione standard, calcolare la probabilit√† che la variabile aleatoria assuma un valore minore o uguale a questo valore.\n:::\n\n::: {.callout-important title=\"Problemi 2\" collapse=\"true\"}\nEsercizi sulla distribuzione binomiale, risolvibili usando R, sono disponibili sulla seguente [pagina web](https://mathcenter.oxford.emory.edu/site/math117/probSetBinomialProbabilities/). \n:::\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered} \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] reshape2_1.4.4        pillar_1.11.0         tinytable_0.13.0     \n#>  [4] patchwork_1.3.2       ggdist_3.3.3          tidybayes_3.0.7      \n#>  [7] bayesplot_1.14.0      ggplot2_3.5.2         reliabilitydiag_0.2.1\n#> [10] priorsense_1.1.1      posterior_1.6.1       loo_2.8.0            \n#> [13] rstan_2.32.7          StanHeaders_2.32.10   brms_2.22.0          \n#> [16] Rcpp_1.1.0            sessioninfo_1.2.3     conflicted_1.2.0     \n#> [19] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        \n#> [22] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          \n#> [25] rio_1.2.3             here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#>  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#>  [7] snakecase_0.11.1      compiler_4.5.1        systemfonts_1.2.3    \n#> [10] vctrs_0.6.5           stringr_1.5.1         pkgconfig_2.0.3      \n#> [13] arrayhelpers_1.1-0    fastmap_1.2.0         backports_1.5.0      \n#> [16] labeling_0.4.3        utf8_1.2.6            rmarkdown_2.29       \n#> [19] ragg_1.5.0            purrr_1.1.0           xfun_0.53            \n#> [22] cachem_1.1.0          jsonlite_2.0.0        broom_1.0.9          \n#> [25] parallel_4.5.1        R6_2.6.1              stringi_1.8.7        \n#> [28] RColorBrewer_1.1-3    lubridate_1.9.4       estimability_1.5.1   \n#> [31] knitr_1.50            zoo_1.8-14            pacman_0.5.1         \n#> [34] Matrix_1.7-4          splines_4.5.1         timechange_0.3.0     \n#> [37] tidyselect_1.2.1      abind_1.4-8           yaml_2.3.10          \n#> [40] codetools_0.2-20      curl_7.0.0            pkgbuild_1.4.8       \n#> [43] lattice_0.22-7        plyr_1.8.9            withr_3.0.2          \n#> [46] bridgesampling_1.1-2  coda_0.19-4.1         evaluate_1.0.5       \n#> [49] survival_3.8-3        RcppParallel_5.1.11-1 tensorA_0.36.2.1     \n#> [52] checkmate_2.3.3       stats4_4.5.1          distributional_0.5.0 \n#> [55] generics_0.1.4        rprojroot_2.1.1       rstantools_2.5.0     \n#> [58] scales_1.4.0          xtable_1.8-4          glue_1.8.0           \n#> [61] emmeans_1.11.2-8      tools_4.5.1           mvtnorm_1.3-3        \n#> [64] grid_4.5.1            QuickJSR_1.8.0        colorspace_2.1-1     \n#> [67] nlme_3.1-168          cli_3.6.5             textshaping_1.0.3    \n#> [70] svUnit_1.0.8          Brobdingnag_1.2-9     V8_7.0.0             \n#> [73] gtable_0.3.6          digest_0.6.37         TH.data_1.1-4        \n#> [76] htmlwidgets_1.6.4     farver_2.1.2          memoise_2.0.1        \n#> [79] htmltools_0.5.8.1     lifecycle_1.0.4       MASS_7.3-65\n```\n:::\n\n\n## Bibliografia {.unnumbered}\n\n",
    "supporting": [
      "13_discr_rv_distr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}