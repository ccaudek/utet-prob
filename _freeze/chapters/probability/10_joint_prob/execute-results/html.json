{
  "hash": "940dd06c677f9055c39f2c04ac747039",
  "result": {
    "engine": "knitr",
    "markdown": "# Probabilità congiunta {#sec-prob-join-prob}\n\n::: {.epigraph}\n> “Probability is expectation founded upon partial knowledge. A perfect acquaintance with all the circumstances affecting the occurrence of an event would change expectation into certainty, and leave neither room nor demand for a theory of probabilities..”\n>\n> -- **George Boole**, An Investigation of the Laws of Thought (1854)\n:::\n\n\n## Introduzione {.unnumbered .unlisted}\n\nFino a questo momento abbiamo considerato il concetto di probabilità associato a singole variabili casuali. Tuttavia, in molte situazioni pratiche e psicologiche, è fondamentale analizzare come due o più variabili casuali interagiscono tra loro. La *distribuzione congiunta* ci permette di descrivere la probabilità che più variabili aleatorie assumano contemporaneamente specifici valori.\n\nQuesto capitolo introduce e approfondisce il concetto di distribuzione congiunta attraverso definizioni, proprietà essenziali e un esempio concreto basato sulla letteratura psicologica.\n\n\n### Panoramica del capitolo {.unnumbered .unlisted}\n\n* Definizione di distribuzione congiunta per variabili discrete e continue.\n* Le proprietà fondamentali: non-negatività e normalizzazione.\n* Come ottenere e interpretare le distribuzioni marginali da una congiunta.\n* Il concetto di indipendenza e come verificarla tramite la distribuzione congiunta.\n* Estensione al caso continuo, con esempi grafici (mappe termiche) e densità marginali/condizionali.\n\n::: {.callout-tip collapse=true}\n## Prerequisiti\n\n- Leggere il capitolo *Joint Distributions* [@kroese2025statistical].\n- Leggere il capitolo *Joint Distributions* [@blitzstein2019introduction].\n:::\n\n::: {.callout-caution collapse=true title=\"Preparazione del Notebook\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |>\n  source()\nlibrary(MASS)\nlibrary(viridis)\nlibrary(ggExtra)\n```\n:::\n\n:::\n\n\n## Cos'è la distribuzione congiunta?\n\nQuando studiamo due variabili casuali — ad esempio ansia ($Y$) e prestazione cognitiva ($X$) — non ci interessa solo il loro comportamento individuale, ma anche come si manifestano insieme. La *distribuzione congiunta* descrive proprio questo: la probabilità che $X$ e $Y$ assumano contemporaneamente determinati valori.\n\n- **Caso discreto** (variabili che possono assumere valori distinti, come categorie o conteggi)::\n\n$$\np(x, y) = P(X = x, Y = y) .\n$$\n\n- **Caso continuo** variabili misurate su scale numeriche con molti possibili valori, come punteggi o tempi di reazione):\n\n$$\nf(x, y) \n$$\nche rappresenta la densità di probabilità congiunta.\n\nGrazie a queste funzioni possiamo rispondere a domande del tipo: Qual è la probabilità che uno studente con ansia elevata ottenga una prestazione insufficiente?\n\n\n## Proprietà fondamentali\n\nPerché una distribuzione congiunta sia una corretta distribuzione di probabilità, deve rispettare due condizioni di base.\n\n::: {.callout-note}\n### 1. Non-negatività  \n$$\np(x,y) \\geq 0 \n\\quad \\text{oppure} \\quad \nf(x,y) \\geq 0 .\n$$\n:::\n\n::: {.callout-note}\n### 2. Normalizzazione  \n\n- **Caso discreto**  \n$$\n\\sum_{x}\\sum_{y} p(x,y) = 1 .\n$$\n\n- **Caso continuo**  \n$$\n\\int_{-\\infty}^{+\\infty}\\int_{-\\infty}^{+\\infty} f(x,y)\\,dx\\,dy = 1 .\n$$\n:::\n\nIn altre parole: \n\n- tutte le probabilità devono essere *positive*,  \n- e la loro somma (o integrale) deve essere *uguale a 1*.\n\n\n## Distribuzioni congiunte e inferenza bayesiana\n\nIn questo libro ci concentreremo soprattutto sull’inferenza bayesiana, che si fonda proprio sul concetto di distribuzione congiunta. In termini generali, l’inferenza bayesiana mira a descrivere la distribuzione a posteriori dei parametri del modello, cioè la probabilità dei parametri dati i dati osservati.\n\nQuesta distribuzione a posteriori deriva dalla combinazione di:\n\n1. una distribuzione a priori sui parametri, che esprime le conoscenze (o le ipotesi) disponibili prima di osservare i dati;\n2. la distribuzione di verosimiglianza, che è una distribuzione congiunta dei dati osservati, condizionata ai parametri.\n\nSe i dati costituiscono un campione casuale indipendente e identicamente distribuito (i.i.d.), allora la verosimiglianza — cioè la *distribuzione congiunta* delle osservazioni — si ottiene come prodotto delle densità di ogni singola osservazione:\n\n$$\np(y_1, y_2, \\dots, y_n \\mid \\theta) = \\prod_{i=1}^n p(y_i \\mid \\theta).\n$$\nPoiché il prodotto di molte densità può rapidamente generare numeri molto piccoli (problema di *underflow numerico*), nella pratica si lavora quasi sempre con la *log-verosimiglianza* (o *log-densità congiunta*):\n\n$$\n\\log p(y_1, \\dots, y_n \\mid \\theta) = \\sum_{i=1}^n \\log p(y_i \\mid \\theta).\n$$\nQuesto passaggio non cambia la sostanza matematica del problema, ma rende i calcoli più stabili e più facili da gestire al computer.\n\n\n## Un esempio psicologico: ansia e prestazione\n\nConsideriamo un esempio tratto dalla letteratura psicologica: la relazione tra *ansia (Y)* e *prestazione cognitiva (X)* in studenti universitari. La ricerca psicologica indica spesso una relazione negativa tra questi due fattori: livelli elevati di ansia possono associarsi a prestazioni cognitive inferiori [@eysenck2007anxiety].\n\nSupponiamo di aver valutato due variabili discrete in un gruppo di studenti:\n\n- *Ansia*: bassa, media, alta (codificata come Y = 0, 1, 2);\n- *Prestazione cognitiva*: insufficiente, sufficiente, buona (codificata come X = 0, 1, 2).\n\nLa distribuzione congiunta potrebbe essere rappresentata nella seguente tabella (i dati sono ipotetici ma coerenti con la letteratura):\n\n|               | Ansia Bassa (0) | Ansia Media (1) | Ansia Alta (2) |\n|---------------|-----------------|-----------------|----------------|\n| Insufficiente (0) | 0.05            | 0.10            | 0.15           |\n| Sufficiente (1)   | 0.15            | 0.20            | 0.10           |\n| Buona (2)         | 0.10            | 0.10            | 0.05           |\n\n\nI valori nella tabella rappresentano stime empiriche delle probabilità congiunte, ovvero le proporzioni osservate di studenti che hanno manifestato una specifica combinazione di livelli delle due variabili. Ad esempio, la cella corrispondente a \"Ansia Media\" e \"Prestazione Sufficiente\" indica che il 20% degli studenti nel campione considerato ha un livello medio di ansia ed ha ottenuto prestazioni sufficienti nel compito cognitivo.\n\nQuesta tabella ci consente di calcolare probabilità interessanti. Ad esempio, la probabilità che uno studente raggiunga almeno la sufficienza, indipendentemente dall’ansia:\n\n$$\nP(X \\geq 1) = 0.15 + 0.20 + 0.10 + 0.10 + 0.10 + 0.05 = 0.70 .\n$$\n\n## Distribuzioni marginali\n\nDalla distribuzione congiunta possiamo ricavare le distribuzioni marginali delle singole variabili, cioè la probabilità che una variabile assuma un certo valore indipendentemente dall’altra.\n\nPer l'ansia:\n\n- ansia bassa:\n$$P(Y=0)=0.05+0.15+0.10=0.30 .$$\n- ansia media:\n$$P(Y=1)=0.10+0.20+0.10=0.40 .$$\n- ansia alta:\n$$P(Y=2)=0.15+0.10+0.05=0.30 .$$\n\nQueste distribuzioni ci dicono, ad esempio, che nel campione il 40% degli studenti ha ansia media.\n\n\n## Indipendenza e dipendenza \n\nDue variabili casuali $X$ e $Y$ si dicono *indipendenti* se la loro distribuzione congiunta si fattorizza nelle rispettive distribuzioni marginali:\n\n$$p(x,y)=p(x)p(y) \\quad \\text{oppure} \\quad f(x,y)=f(x)f(y) .$$\n\nNel nostro esempio, se ansia e prestazione fossero indipendenti, dovremmo avere:\n\n$$P(X=0,Y=2)=P(X=0)P(Y=2) .$$\n\nIn realtà, come suggerisce la letteratura [@eysenck2007anxiety], ansia e prestazione tendono a essere dipendenti: più alta è l’ansia, minore è la probabilità di buone prestazioni.\n\n\n## Il caso continuo: una mappa termica\n\nSe invece misuriamo ansia e prestazione come variabili continue (es. punteggi su scale numeriche), la distribuzione congiunta è una densità. Possiamo rappresentarla come una mappa termica:\n\n- asse X = ansia,\n- asse Y = prestazione,\n- colori più caldi = combinazioni più probabili.\n\nUn esempio simulato mostra che la maggior parte degli studenti si concentra intorno a bassa ansia (30) e buona prestazione (70), mentre i punteggi più alti di ansia si associano a prestazioni più basse.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10_joint_prob_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nQuesta visualizzazione rende immediato vedere la relazione negativa tra le due variabili [@eysenck2007anxiety].\n\n### Marginali e condizionali con variabili continue\n\nQuando lavoriamo con variabili continue, non possiamo semplicemente contare le combinazioni come nel caso discreto (ad esempio, lanci di un dado). Invece, misuriamo la probabilità calcolando l'area della regione interessata sulla nostra mappa termica:\n\n- la probabilità che l'ansia sia tra 50 e 55, e la prestazione tra 30 e 50, è rappresentata dall'area della regione corrispondente nella mappa termica;\n- gli integrali (strumenti matematici per calcolare aree) sono semplicemente un modo preciso per fare questa operazione.\n\n#### Densità marginale: proiettare la mappa su un asse\n\nLa densità marginale descrive come si distribuisce una singola variabile, prescindendo completamente dall'altra. Possiamo immaginare questo processo come la proiezione della mappa termica su uno degli assi, ottenendo così un'ombra o una proiezione dell'intera distribuzione:\n\n- proiettando tutti i valori sull'asse dell'ansia, si ottiene la densità marginale dell'ansia;\n- proiettando tutti i valori sull'asse della prestazione, si ottiene la densità marginale della prestazione.\n\nQueste proiezioni rivelano la distribuzione di ciascuna variabile considerata isolatamente. Nella figura a cui si fa riferimento, le distribuzioni marginali sono state elaborate utilizzando in modo indipendente i dati relativi a ciascuna variabile, senza considerare le loro interrelazioni.\n\nInfatti, i colori più caldi nella mappa termica indicano zone con maggiore densità di osservazioni. Quando proiettiamo questi valori su un asse, otteniamo una curva di densità che rappresenta la distribuzione della variabile. Le aree dove la curva raggiunge valori più alti corrispondono ai valori più frequenti della variabile nella popolazione studiata.\n\n#### Densità condizionale: fette della mappa termica\n\nLa densità condizionale risponde alla domanda: \"Se osservo persone con un determinato punteggio di prestazione cognitiva (ad esempio 40 punti), qual è la distribuzione dell'ansia tra queste persone?\"\n\n- Immaginate di prendere una fetta verticale della mappa termica in corrispondenza della prestazione = 40 punti. Questa fetta mostra la distribuzione dell'ansia soltanto tra coloro che hanno esattamente quella prestazione cognitiva.\n- Per rendere questa distribuzione coerente, normalizziamo (cioè \"aggiustiamo\") la fetta rispetto alla probabilità complessiva della prestazione a quel livello.\n\nQuesta fetta verticale con i suoi vari colori (più caldi dove c'è maggiore densità) può essere convertita in una curva di densità che mostra come si distribuisce l'ansia specificamente per le persone con quel determinato livello di prestazione cognitiva. Il processo di normalizzazione assicura che l'area sotto questa curva di densità condizionale sia uguale a 1, consentendo confronti tra diverse condizioni.\n\n#### Perché è importante in psicologia?\n\nStudiare distribuzioni congiunte è cruciale perché\n\n- mette in luce relazioni complesse tra variabili psicologiche (lineari, curvilinee, bimodali, cluster);\n- rivela informazioni che andrebbero perse se osservassimo solo le marginali;\n- permette di analizzare fenomeni realistici, in cui costrutti come ansia, motivazione o prestazione non agiscono mai isolatamente.\n\n\n## Riflessioni conclusive {.unnumbered .unlisted}\n\nLa distribuzione congiunta rappresenta uno strumento fondamentale per l'analisi multivariata in psicologia, permettendo di studiare simultaneamente il comportamento di multiple variabili aleatorie e le loro interrelazioni. Questo approccio risulta particolarmente prezioso nella ricerca psicologica, dove i fenomeni oggetto di studio - come l'ansia, la prestazione cognitiva, la motivazione o i tratti di personalità - raramente si manifestano in isolamento, ma piuttosto attraverso complesse reti di influenze reciproche.\n\nI concetti di densità congiunta, marginale e condizionale costituiscono la base per un'analisi rigorosa delle relazioni tra variabili psicologiche continue. Questi strumenti consentono di esplorare come i costrutti psicologici interagiscono e si influenzano reciprocamente, offrendo un quadro analitico per comprendere la complessità dei fenomeni mentali e comportamentali.\n\nIl passaggio concettuale dalle variabili discrete a quelle continue, pur richiedendo l'adozione di strumenti matematici più sofisticati (integrali invece di somme), mantiene intatta la sua intuizione fondamentale. La logica appresa nel caso discreto continua a fornire una solida base interpretativa anche per i fenomeni continui, che meglio rappresentano la realtà della misurazione psicologica.\n\nQuesto framework analitico prepara il terreno per la successiva quantificazione delle relazioni tra variabili attraverso indicatori come la covarianza e la correlazione. Questi strumenti, che saranno approfonditi nel prossimo capitolo, permetteranno di misurare sistematicamente la forza e la direzione delle associazioni psicologiche, trasformando le osservazioni qualitative in relazioni quantitative verificabili.\n\nL'approccio attraverso le distribuzioni congiunte non solo fornisce un linguaggio formale per descrivere le relazioni psicologiche, ma stabilisce anche le fondamenta per modelli più avanzati di analisi dei dati, aprendo la strada a una comprensione sempre più sofisticata dei meccanismi che regolano il comportamento umano e i processi mentali.\n\n\n::: {.callout-note collapse=true title=\"Informazioni sull'ambiente di sviluppo\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] ggExtra_0.11.0        viridis_0.6.5         viridisLite_0.4.2    \n#>  [4] MASS_7.3-65           pillar_1.11.0         tinytable_0.13.0     \n#>  [7] patchwork_1.3.2       ggdist_3.3.3          tidybayes_3.0.7      \n#> [10] bayesplot_1.14.0      ggplot2_3.5.2         reliabilitydiag_0.2.1\n#> [13] priorsense_1.1.1      posterior_1.6.1       loo_2.8.0            \n#> [16] rstan_2.32.7          StanHeaders_2.32.10   brms_2.22.0          \n#> [19] Rcpp_1.1.0            sessioninfo_1.2.3     conflicted_1.2.0     \n#> [22] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        \n#> [25] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          \n#> [28] rio_1.2.3             here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#>  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#>  [7] snakecase_0.11.1      compiler_4.5.1        systemfonts_1.2.3    \n#> [10] vctrs_0.6.5           stringr_1.5.1         pkgconfig_2.0.3      \n#> [13] arrayhelpers_1.1-0    fastmap_1.2.0         backports_1.5.0      \n#> [16] labeling_0.4.3        promises_1.3.3        rmarkdown_2.29       \n#> [19] ragg_1.5.0            purrr_1.1.0           xfun_0.53            \n#> [22] cachem_1.1.0          jsonlite_2.0.0        later_1.4.4          \n#> [25] broom_1.0.9           parallel_4.5.1        R6_2.6.1             \n#> [28] stringi_1.8.7         RColorBrewer_1.1-3    lubridate_1.9.4      \n#> [31] estimability_1.5.1    knitr_1.50            zoo_1.8-14           \n#> [34] httpuv_1.6.16         Matrix_1.7-4          splines_4.5.1        \n#> [37] timechange_0.3.0      tidyselect_1.2.1      abind_1.4-8          \n#> [40] yaml_2.3.10           codetools_0.2-20      miniUI_0.1.2         \n#> [43] curl_7.0.0            pkgbuild_1.4.8        lattice_0.22-7       \n#> [46] shiny_1.11.1          withr_3.0.2           bridgesampling_1.1-2 \n#> [49] coda_0.19-4.1         evaluate_1.0.5        survival_3.8-3       \n#> [52] isoband_0.2.7         RcppParallel_5.1.11-1 tensorA_0.36.2.1     \n#> [55] checkmate_2.3.3       stats4_4.5.1          distributional_0.5.0 \n#> [58] generics_0.1.4        rprojroot_2.1.1       rstantools_2.5.0     \n#> [61] scales_1.4.0          xtable_1.8-4          glue_1.8.0           \n#> [64] emmeans_1.11.2-8      tools_4.5.1           mvtnorm_1.3-3        \n#> [67] grid_4.5.1            QuickJSR_1.8.0        colorspace_2.1-1     \n#> [70] nlme_3.1-168          cli_3.6.5             textshaping_1.0.3    \n#> [73] svUnit_1.0.8          Brobdingnag_1.2-9     V8_7.0.0             \n#> [76] gtable_0.3.6          digest_0.6.37         TH.data_1.1-4        \n#> [79] htmlwidgets_1.6.4     farver_2.1.2          memoise_2.0.1        \n#> [82] htmltools_0.5.8.1     lifecycle_1.0.4       mime_0.13\n```\n:::\n\n:::\n\n## Bibliografia {.unnumbered .unlisted}\n\n\n",
    "supporting": [
      "10_joint_prob_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}