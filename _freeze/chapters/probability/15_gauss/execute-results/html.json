{
  "hash": "15b47222f13a3441451df45f017e104a",
  "result": {
    "engine": "knitr",
    "markdown": "# Assunzione di gaussianità e trasformazioni dei dati {#sec-eda-gaussian-assumption}\n\n::: callout-note\n## In questo capitolo imparerai a\n\n- valutare la normalità dei dati usando metodi grafici e statistici;\n- applicare trasformazioni per normalizzare i dati.\n:::\n\n::: callout-tip\n## Prerequisiti\n\n- Leggere \"Assumption-checking rather than (just) testing: The importance of visualization and effect size in statistical diagnostics\" [@shatz2024assumption].\n:::\n\n::: callout-important\n## Preparazione del Notebook\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(datawizard, MASS)\n```\n:::\n\n:::\n\n## Introduzione\n\nNell’analisi dei dati numerici, un aspetto cruciale da affrontare è l’imprecisione delle misurazioni, una caratteristica intrinseca dei dati reali. Anche in condizioni ottimali, le misure sono soggette a incertezze: i dati rappresentano sempre una stima approssimativa della realtà, accurata solo entro un certo margine (ad esempio, pochi punti percentuali). Inoltre, in molti contesti, come quelli demografici, commerciali o sociali, i dati possono essere arrotondati intenzionalmente o risultare imprecisi a causa di stime indirette, incompletezza delle informazioni o altri fattori.  \n\nRiconoscere e gestire questa incertezza è una componente essenziale del processo analitico. Gli strumenti statistici forniscono un framework formale per descrivere e quantificare l’incertezza, consentendo di trarre inferenze robuste dai dati. Tra le distribuzioni di probabilità, la **distribuzione normale** (o gaussiana) occupa un posto centrale per la sua ubiquità e versatilità. Spesso rappresentata dalla caratteristica \"curva a campana,\" questa distribuzione è utilizzata per descrivere molte variabili naturali. Quando i dati approssimano una distribuzione normale, gran parte dei valori si concentra intorno alla media, con una diminuzione progressiva della probabilità di valori estremi. \n\nUna delle proprietà più utili della distribuzione normale è la possibilità di esprimere affermazioni quantitative rigorose. Ad esempio, si può calcolare la probabilità che un valore cada entro un determinato intervallo dalla media utilizzando parametri semplici come media ($\\mu$) e deviazione standard ($\\sigma$):\n\n- Circa il **68.3%** dei valori cade entro una deviazione standard dalla media:\n  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npnorm(1) - pnorm(-1)\n#> [1] 0.683\n```\n:::\n\n\n- Circa il **99.7%** cade entro tre deviazioni standard:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npnorm(3) - pnorm(-3)\n#> [1] 0.997\n```\n:::\n\n\nQuesti calcoli costituiscono la base della “regola delle tre sigma,” una strategia utilizzata per identificare valori anomali (outlier), come discusso nel @sec-eda-outlier. Tuttavia, tale regola può risultare fuorviante se i dati non seguono effettivamente una distribuzione normale.\n\nLa densità della distribuzione normale è definita dalla formula:\n\n$$\np(x) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right),\n$$\n\ndove $\\mu$ rappresenta la media e $\\sigma$ la deviazione standard. Conoscere questi due parametri permette di calcolare proprietà fondamentali della distribuzione e di stimare probabilità associate a intervalli specifici.\n\n### Gaussianità e Inferenza Statistica  \n\nLa distribuzione normale è particolarmente rilevante per molti metodi statistici, in particolare nell’approccio frequentista. Gran parte dei test di ipotesi e delle procedure inferenziali assume che i dati siano distribuiti normalmente, una condizione necessaria per derivare formalmente i risultati di molti test. Ad esempio, i test t di Student e l’ANOVA richiedono la normalità delle variabili o dei residui. Quando questa assunzione è soddisfatta, tali strumenti offrono inferenze precise e affidabili.\n\nTuttavia, se i dati non sono gaussiani, molti test perdono validità. Sebbene si siano proposti approcci che dimostrano la robustezza di alcuni test a deviazioni moderate dalla normalità [@shatz2024assumption], tale robustezza non è garantita in tutte le situazioni. Inoltre, l’enfasi sui valori-p complica la questione, poiché violazioni dell’assunzione di normalità possono compromettere l’interpretazione di questi indicatori.\n\nUna strategia comune per affrontare la non-gaussianità è l’applicazione di trasformazioni dei dati, come la trasformazione logaritmica o quella della radice quadrata, per avvicinare i dati alla distribuzione normale [@osborne2002notes]. Ad esempio, distribuzioni asimmetriche come quelle dei tempi di reazione possono essere rese più gaussiane attraverso trasformazioni adeguate, rendendo applicabili i test frequentisti standard. Tuttavia, l’uso delle trasformazioni ha un costo: la **perdita di interpretabilità**. Se i dati originali avevano un significato chiaro e intuitivo, la trasformazione può rendere i risultati più difficili da collegare al fenomeno studiato.\n\nIn questo capitolo, esploreremo come verificare se i dati seguono una distribuzione normale, discuteremo l’impatto di questa assunzione sui metodi frequentisti e valuteremo il ruolo delle trasformazioni. Il nostro obiettivo è fornire al data analyst una guida pratica per decidere come trattare i dati non gaussiani, considerando sia i vantaggi che i limiti di ciascun approccio.\n\n### L’assunzione di Gaussianità: Quando è valida?\n\nSebbene la distribuzione normale sia spesso un buon modello per i dati numerici, non è sempre una rappresentazione adeguata. Questo può dipendere da caratteristiche intrinseche dei dati, come asimmetrie, code lunghe o la presenza di valori anomali. Valutare l’appropriatezza dell’assunzione di normalità è un passaggio critico in qualsiasi analisi statistica.\n\nPer diagnosticare la normalità, presenteremo tre strumenti grafici:\n\n- **Istogrammi**, una visualizzazione semplice ma spesso limitata.\n- **Grafici di densità**, che forniscono un confronto più fluido rispetto agli istogrammi.\n- **QQ-plot (Quantile-Quantile plot)**, uno strumento visivo particolarmente efficace per rilevare deviazioni dalla normalità. \n\nQuesti strumenti possono anche essere affiancati da test formali per consentire una diagnosi robusta e guidare le decisioni sul trattamento dei dati.\n\n## Istogramma\n\nPer illustrare il concetto, utilizziamo un set di dati simulati che hanno proprietà simili a quelle dei tempi di reazione. Creeremo un istogramma e vi sovrapporremo la curva di densità normale calcolata in base ai dati.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Dati simulati di tempi di reazione\nset.seed(123)\nrt <- c(rexp(100, rate = 0.2), 50, 60) # Aggiunti valori estremi\n\n# Calcolare la media e la deviazione standard per sovrapporre la densità normale\nmean_rt <- mean(rt, na.rm = TRUE)\nsd_rt <- sd(rt, na.rm = TRUE)\n\n# Creare l'istogramma e sovrapporre la densità normale\nggplot(tibble(rt=rt), aes(x = rt)) +\n  geom_histogram(\n    aes(y = ..density..),\n    bins = 30, color = \"black\"\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean_rt, sd = sd_rt),\n    size = 1\n  ) +\n  labs(\n    x = \"Tempi di Reazione\",\n    y = \"Densità\"\n  )\n```\n\n::: {.cell-output-display}\n![](15_gauss_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nL’istogramma mostra la distribuzione empirica dei dati, mentre la curva rossa rappresenta la densità normale con la stessa media e deviazione standard. Nel nostro caso, è evidente una discrepanza tra la distribuzione empirica e la densità normale, indicando che l’assunzione di normalità non è appropriata.\n\n## Grafico di densità\n\nUn grafico di densità è una versione lisciata dell’istogramma che facilita il confronto con la distribuzione normale. Utilizzando il dataset precedente, possiamo creare un grafico di densità sovrapposto alla curva gaussiana.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(tibble(rt=rt), aes(x = rt)) +\n  geom_density(alpha = 0.5) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean_rt, sd = sd_rt),\n    size = 1\n  ) +\n  labs(\n    x = \"Peso\",\n    y = \"Densità\"\n  )\n```\n\n::: {.cell-output-display}\n![](15_gauss_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nAnche questa rappresentazione rende chiaro come l’assunzione di normalità non sia appropriata.\n\n## Diagramma quantile-quantile\n\nIl *diagramma quantile-quantile* (QQ-plot) è lo strumento più utile per analizzare visivamente la conformità di un dataset a una distribuzione teorica, in particolare alla distribuzione normale. Il QQ-plot è una tecnica essenziale per chi lavora con dati che si presume seguano una distribuzione specifica, e rappresenta un passaggio cruciale in molte analisi statistiche, soprattutto per verificare l'assunto di normalità.\n\nUn QQ-plot permette di:\n\n- *Valutare graficamente la normalità dei dati*: Se i punti nel diagramma seguono approssimativamente una linea retta, i dati possono essere considerati normalmente distribuiti. In caso contrario, il QQ-plot rivela deviazioni dalla normalità, come code pesanti o asimmetrie.\n- *Confrontare distribuzioni*: Il QQ-plot non si limita solo alla distribuzione normale, ma può essere utilizzato per confrontare la distribuzione del campione con qualsiasi distribuzione teorica, facilitando l'analisi di dati con forme di distribuzione complesse.\n- *Identificare outlier*: Gli outlier nei dati saranno visibili come punti che si discostano significativamente dalla linea retta del QQ-plot.\n\nIl QQ-plot è costruito tracciando i *quantili del campione* contro i *quantili teorici* di una distribuzione di riferimento. L’interpretazione è piuttosto semplice:\n\n- Se il campione segue la distribuzione teorica, i punti nel QQ-plot si allineano lungo una linea retta di pendenza 1 (e intercetta 0 nel caso di distribuzione normale standardizzata).\n- La deviazione dalla linea retta indica differenze nella distribuzione del campione rispetto alla distribuzione teorica:\n  - *Intercetta diversa da 0*: indica che la media del campione differisce dalla media della distribuzione teorica.\n  - *Pendenza diversa da 1*: indica una differenza nella varianza tra il campione e la distribuzione teorica.\n  - *Curve*: indicano deviazioni sistematiche, come code pesanti o distribuzioni asimmetriche.\n\nNella discussione seguente, costruiremo e analizzeremo QQ-plot per tre casi tipici:\n\n1. *Campione con stessa media e varianza della distribuzione teorica*.\n2. *Campione con media diversa ma stessa varianza*.\n3. *Campione con media e varianza diverse*.\n\nSimuleremo i dati, li ordineremo, calcoleremo manualmente i quantili teorici e infine utilizzeremo librerie specializzate per replicare e confrontare i risultati. Questo approccio pratico ci permetterà di comprendere a fondo l'utilità e il funzionamento del QQ-plot.\n\n### Comprendere e Costruire un QQ-Plot (Distribuzione Normale)\n\nUn *QQ-plot* (Quantile-Quantile plot) è uno strumento grafico utilizzato per confrontare la distribuzione di un campione con una distribuzione teorica, spesso la distribuzione normale. Il QQ-plot aiuta a visualizzare se un dataset segue una distribuzione specifica, tracciando i quantili del campione contro i quantili della distribuzione teorica.\n\n### Passi per Costruire un QQ-Plot\n\n1. **Ordinare i Dati**: Disporre i dati del campione in ordine crescente.\n2. **Determinare i Quantili Teorici**: Per una distribuzione normale, i quantili corrispondono all'inverso della funzione di distribuzione cumulativa (CDF) della distribuzione normale.\n3. **Confrontare i Quantili**: Tracciare i quantili del campione rispetto ai quantili della distribuzione teorica. Se il campione proviene dalla distribuzione teorica, i punti dovrebbero trovarsi approssimativamente su una linea retta.\n\n### Caso 1: Campione con Stessa Media e Varianza della Distribuzione Normale\n\nSupponiamo che il campione provenga da una distribuzione normale $N(\\mu = 0, \\sigma^2 = 1)$, esattamente come la distribuzione teorica.\n\n#### Simulazione dei Dati\n\nIniziamo simulando un piccolo dataset da $N(0, 1)$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Generiamo 20 punti dati da N(0, 1)\nset.seed(42)  # Per garantire la riproducibilità\ndati_campione <- rnorm(20, mean = 0, sd = 1)\n\n# Ordiniamo i dati del campione\ncampione_ordinato <- sort(dati_campione)\n\n# Calcoliamo i quantili teorici da N(0, 1)\nquantili_teorici <- qnorm((seq(1, 20) - 0.5) / 20)\n\n# Tracciamo il QQ-plot\nplot(quantili_teorici, campione_ordinato,\n     xlab = \"Quantili Teorici\", ylab = \"Quantili del Campione\",\n     main = \"QQ-Plot: Stessa Media e Varianza\", pch = 16)\nabline(0, 1, lwd = 2)  # Linea y = x\n```\n\n::: {.cell-output-display}\n![](15_gauss_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIn questo caso, i punti del QQ-plot dovrebbero allinearsi alla linea rossa, indicando che la distribuzione del campione corrisponde a quella teorica.\n\n---\n\n### Caso 2: Campione con Media Diversa (Intercetta ≠ 0)\n\nSimuliamo un campione da $N(2, 1)$, con una media diversa ma la stessa varianza:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Generiamo 20 punti dati da N(2, 1)\ndati_campione_media_spostata <- rnorm(20, mean = 2, sd = 1)\n\n# Ordiniamo i dati del campione\ncampione_ordinato_media_spostata <- sort(dati_campione_media_spostata)\n\n# Tracciamo il QQ-plot\nplot(quantili_teorici, campione_ordinato_media_spostata,\n     xlab = \"Quantili Teorici\", ylab = \"Quantili del Campione\",\n     main = \"QQ-Plot: Media Diversa (Intercetta ≠ 0)\", pch = 16)\nabline(0, 1, lwd = 2)  # Linea y = x\n```\n\n::: {.cell-output-display}\n![](15_gauss_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIn questo caso, i punti dovrebbero seguire una linea retta ma essere spostati verticalmente, indicando una media diversa (intercetta ≠ 0).\n\n---\n\n### Caso 3: Campione con Media e Varianza Diverse (Pendenza ≠ 1)\n\nSimuliamo un campione da $N(2, 2^2)$, con una media e una varianza diverse:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Generiamo 20 punti dati da N(2, 2^2)\ndati_campione_varianza_spostata <- rnorm(20, mean = 2, sd = 2)\n\n# Ordiniamo i dati del campione\ncampione_ordinato_varianza_spostata <- sort(dati_campione_varianza_spostata)\n\n# Tracciamo il QQ-plot\nplot(quantili_teorici, campione_ordinato_varianza_spostata,\n     xlab = \"Quantili Teorici\", ylab = \"Quantili del Campione\",\n     main = \"QQ-Plot: Media e Varianza Diverse (Pendenza ≠ 1)\", pch = 16)\nabline(0, 1, lwd = 2)  # Linea y = x\n```\n\n::: {.cell-output-display}\n![](15_gauss_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIn questo caso, i punti si discosteranno sia verticalmente (per la media diversa) sia rispetto alla pendenza della linea (per la varianza diversa).\n\n---\n\n### Calcolo Manuale del QQ-Plot\n\nPer ciascun caso sopra, i passaggi sono i seguenti:\n\n1. **Ordinamento dei dati del campione**: Questo fornisce i quantili del campione.\n2. **Calcolo dei quantili teorici**: Utilizzando la funzione inversa della CDF per la distribuzione normale.\n\nEsempio in R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo manuale dei quantili teorici\nquantili_teorici_manuali <- function(n) {\n  sapply(1:n, function(i) qnorm((i - 0.5) / n))\n}\n\nn <- length(dati_campione)\nquantili_teorici_calcolati <- quantili_teorici_manuali(n)\nquantili_teorici_calcolati\n#>  [1] -1.9600 -1.4395 -1.1503 -0.9346 -0.7554 -0.5978 -0.4538 -0.3186 -0.1891\n#> [10] -0.0627  0.0627  0.1891  0.3186  0.4538  0.5978  0.7554  0.9346  1.1503\n#> [19]  1.4395  1.9600\n```\n:::\n\n\n---\n\n### Utilizzo di Funzioni Specializzate\n\nIn R, il pacchetto base offre la funzione `qqnorm()` per generare QQ-plot. Ad esempio:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Generazione del QQ-plot con qqnorm\nqqnorm(dati_campione, main = \"QQ-Plot: Stessa Media e Varianza\")\nqqline(dati_campione, lwd = 2)  # Linea di riferimento\n```\n\n::: {.cell-output-display}\n![](15_gauss_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nPossiamo ripetere lo stesso per i campioni con media e varianza spostate.\n\n--- \n\nQuesto approccio fornisce un'analisi completa della corrispondenza tra distribuzioni teoriche e campioni simulati utilizzando QQ-plot.\n\nPer concludere, esaminiamo la distribuzione dei tempi di reazione simulati con il qq-plot.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqqnorm(rt, main = \"Tempi di Reazione\")\n```\n\n::: {.cell-output-display}\n![](15_gauss_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIl diagramma quantile-quantile rende molto chiaro che la distribuzione del peso dei pulcini non è gaussiana.\n\n## Valutare la Normalità: Test Statistici \n\nSebbene esistano numerosi test statistici formali per valutare la conformità dei dati alla distribuzione normale, questi sono spesso troppo conservativi o sensibili a lievi deviazioni, e nella pratica sono frequentemente sostituiti da metodi visivi più flessibili ed efficaci.\n\nIn R sono disponibili diversi test per verificare la normalità dei dati. Di seguito presentiamo i più comuni, insieme a un esempio pratico basato sul dataset `ChickWeight`.\n\n### Test di Shapiro-Wilk\n\nIl test di Shapiro-Wilk è uno dei test più utilizzati per verificare la normalità. Valuta l'ipotesi nulla che i dati seguano una distribuzione normale.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshapiro_test <- shapiro.test(rt)\nshapiro_test\n#> \n#> \tShapiro-Wilk normality test\n#> \n#> data:  rt\n#> W = 0.6, p-value = 5e-16\n```\n:::\n\n\n- Il p-value è inferiore a 0.05 → Rifiutiamo l'ipotesi nulla, i dati non sono normali.\n- Il p-value è maggiore di 0.05 → Non rifiutiamo l'ipotesi nulla, i dati possono essere considerati normali.\n\n### Test di Kolmogorov-Smirnov\n\nQuesto test confronta la distribuzione cumulativa dei dati con una distribuzione teorica, come la normale. Tuttavia, è meno sensibile rispetto al test di Shapiro-Wilk.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nks_test <- ks.test(\n  rt, \n  \"pnorm\", \n  mean = mean(rt), \n  sd = sd(rt)\n)\nks_test\n#> \n#> \tAsymptotic one-sample Kolmogorov-Smirnov test\n#> \n#> data:  rt\n#> D = 0.3, p-value = 0.000005\n#> alternative hypothesis: two-sided\n```\n:::\n\n\nIl test di Kolmogorov-Smirnov è più adatto per grandi dataset, ma è noto per essere eccessivamente conservativo.\n\n### Limitazioni dei test statistici\n\nNonostante la loro precisione formale, i test statistici per la normalità notevoli  limitazioni:\n\n- **Eccessiva sensibilità ai grandi campioni:** Quando il campione è ampio, anche lievi deviazioni dalla normalità, non rilevanti per l’analisi, possono portare a un risultato di non-normalità.\n\n- **Mancanza di sensibilità nei piccoli campioni:** Con campioni ridotti, i test possono mancare di potere statistico, portando a falsi negativi (ovvero, non rilevare deviazioni significative dalla normalità).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\nshapiro.test(rchisq(20, 4))\n#> \n#> \tShapiro-Wilk normality test\n#> \n#> data:  rchisq(20, 4)\n#> W = 0.9, p-value = 0.3\n```\n:::\n\n\nIn questo esempio, vediamo come, con un campione di 20 osservazioni da una distribuzione $\\chi^2_4$ si produca un falso negativo.\n\n- **Difficoltà interpretative:** Un p-value elevato non implica che i dati siano esattamente normali; semplicemente, non c'è evidenza sufficiente per rifiutare l'ipotesi di normalità.\n\nI metodi visivi, sebbene meno formali, sono spesso più pratici ed efficaci per diagnosticare deviazioni dalla normalità.I metodi visivi sono preferibili sono preferibili perché\n\n- forniscono una diagnosi immediata, che consente di identificare deviazioni rilevanti senza dipendere da un p-value.\n- rivelano non solo se i dati non sono normali, ma anche *come* e *dove* differiscono dalla normalità (ad esempio, asimmetria o code pesanti).\n- offrono indicazioni utili anche in presenza di grandi campioni, dove i test statistici possono risultare eccessivamente conservativi.\n\n---\n\n## Trasformazione dei dati: affrontare la non-normalità\n\nQuando i dati non rispettano l'assunzione di normalità, è possibile utilizzare diverse strategie per affrontare questa violazione. Una delle più comuni è l'uso di trasformazioni dei dati, che permettono di adattare la distribuzione dei dati a una forma più vicina a quella normale, mantenendo comunque la validità dell'analisi che richiede l'assunzione di normalità. Due approcci comuni sono *Winsorizing* e *trimming*.\n\n### Winsorizing e Trimming\n\nQuesti metodi si concentrano sulla gestione degli outlier, ossia valori estremi che possono distorcere la distribuzione dei dati. Entrambi gli approcci presumono che la non-normalità sia dovuta a dati contaminanti e agiscono in modo differente:\n\n- **Winsorizing**: Sostituisce i valori estremi con valori meno estremi, come i percentili limite della distribuzione.\n- **Trimming**: Rimuove completamente i valori estremi dalla distribuzione.\n\nConsideriamo i seguenti dati di esempio:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndati <- c(\n  1.0, 2.2, 3.0, 3.1, 4.0, 4.0, 4.1, 5.3, 6.5, 8.3,\n  10.9, 20.4, 21.4, 34.\n)\n\n# Winsorizing al 20%\ndati_winsorized <- winsorize(\n  dati,\n  method = \"percentile\", percentile = 20\n)\nprint(dati_winsorized)\n#>  [1]  3.0  3.0  3.0  3.1  4.0  4.0  4.1  5.3  6.5  8.3 10.9 20.4 20.4 20.4\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Trimming al 20%\ndati_trimmed <- dati[\n  dati >= quantile(dati, 0.2) & dati <= quantile(dati, 0.8)\n]\nprint(dati_trimmed)\n#> [1]  3.1  4.0  4.0  4.1  5.3  6.5  8.3 10.9\n```\n:::\n\n\n- **Dati Winsorized**: I valori estremi (inferiori e superiori ai percentili 20° e 80°) sono sostituiti dai valori limite.\n- **Dati Trimmed**: I valori fuori dai percentili 20° e 80° sono completamente rimossi.\n\nQuesti metodi riducono l'impatto degli outlier, ma possono introdurre bias se i valori estremi sono effettivamente parte della popolazione target.\n\n---\n\n## Trasformazioni comuni\n\nQuando i dati non rispettano l'assunzione di normalità, è possibile applicare trasformazioni matematiche per modificarne la forma e migliorare l'adattamento a una distribuzione normale. Di seguito, presentiamo le trasformazioni più utilizzate.\n\n**Trasformazione logaritmica.**  \nLa trasformazione logaritmica è particolarmente utile per variabili con asimmetria positiva (code lunghe a destra), come i tempi di reazione.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Trasformazione logaritmica\ndati_log <- log(rt)\nplot(density(dati_log), main = \"Densità dei dati log-transformati\")\n```\n\n::: {.cell-output-display}\n![](15_gauss_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**Trasformazione radice quadrata.**  \nAdatta per variabili di conteggio o proporzioni con valori vicini a zero.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndati_sqrt <- sqrt(rt)\nplot(density(dati_sqrt), main = \"Densità dei dati trasformati con radice quadrata\")\n```\n\n::: {.cell-output-display}\n![](15_gauss_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**Trasformazione inversa.**  \nEfficace per dati con forte asimmetria positiva, ma può complicare l'interpretazione.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndati_inv <- 1 / rt\nplot(density(dati_inv), main = \"Densità dei dati trasformati inversamente\")\n```\n\n::: {.cell-output-display}\n![](15_gauss_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**Trasformazione Box-Cox.**  \nLa trasformazione Box-Cox è una tecnica parametrica che generalizza le precedenti. Utilizza il massimo della verosimiglianza per determinare la trasformazione ottimale per normalizzare i dati. La funzione di trasformazione dipende da un parametro $\\lambda$:\n\n$$\ny(\\lambda) =\n\\begin{cases} \n\\frac{y^\\lambda - 1}{\\lambda} & \\text{se } \\lambda \\neq 0, \\\\\n\\log(y) & \\text{se } \\lambda = 0.\n\\end{cases}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nb <- boxcox(lm(rt ~ 1))\n```\n\n::: {.cell-output-display}\n![](15_gauss_files/figure-html/unnamed-chunk-20-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Exact lambda\nlambda <- b$x[which.max(b$y)]\nlambda\n#> [1] 0.182\n```\n:::\n\n\nTrasformiamo i dati utilizzando `lambda`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrt_boxcox <- (rt^lambda - 1) / lambda\nrt_boxcox |> head()\n#> [1]  1.645  1.168  2.261 -1.568 -1.133  0.479\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(\n  density(rt_boxcox), \n  main = \"Densità dei dati trasformati con Box-Cox\"\n)\n```\n\n::: {.cell-output-display}\n![](15_gauss_files/figure-html/unnamed-chunk-23-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Generazione del QQ-plot con qqnorm\nqqnorm(rt_boxcox, main = \"QQ-Plot\")\nqqline(rt_boxcox, lwd = 2) \n```\n\n::: {.cell-output-display}\n![](15_gauss_files/figure-html/unnamed-chunk-24-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n---\n\n### Pro e contro delle trasformazioni\n\n**Vantaggi:**  \nLe trasformazioni dei dati offrono molteplici benefici nell'analisi statistica. In primo luogo, possono migliorare l'aderenza alla normalità, un requisito fondamentale per l'applicazione di molti test statistici. Inoltre, contribuiscono a stabilizzare la varianza e a mitigare l'impatto dei valori estremi, riducendo il rischio che questi ultimi influenzino eccessivamente i risultati. Tali vantaggi migliorano la robustezza e l'accuratezza delle analisi.\n\n**Svantaggi:**  \nNonostante i benefici, le trasformazioni presentano limitazioni rilevanti. La **perdita di interpretabilità** è uno degli aspetti più critici: i risultati su dati trasformati possono risultare meno intuitivi. Ad esempio, in un modello di regressione applicato a dati trasformati con il logaritmo, il coefficiente rappresenta un cambiamento percentuale anziché assoluto, rendendo l'interpretazione meno diretta per i non esperti.  \n\nInoltre, l'applicazione di una trasformazione deve essere attentamente motivata in base alla natura dei dati e agli obiettivi dell'analisi. Una trasformazione inappropriata può introdurre **distorsioni indesiderate**, compromettendo la validità dei risultati e portando a conclusioni fuorvianti. Per questo motivo, è essenziale valutare attentamente i costi e i benefici della trasformazione nel contesto specifico della ricerca.\n\n## Riflessioni Conclusive\n\nLa verifica della normalità dei dati e l’eventuale utilizzo di trasformazioni matematiche costituiscono fasi fondamentali nell’analisi statistica. Sebbene i test formali (come Shapiro-Wilk o Kolmogorov-Smirnov) offrano una valutazione strutturata, essi possono risultare troppo sensibili, portando a rigettare l’assunto di normalità anche in presenza di lievi deviazioni non rilevanti dal punto di vista pratico. In questi casi, l’impiego di strumenti visivi—come istogrammi, grafici di densità o QQ-plot—si rivela spesso più informativo e flessibile, consentendo di individuare la natura e l’entità delle deviazioni.\n\nLe trasformazioni dei dati rappresentano una strategia utile per normalizzare la distribuzione, ma richiedono una scelta oculata. È essenziale verificare che la trasformazione adottata non comprometta il significato teorico della variabile in esame. Quando l’interpretabilità risulta compromessa, potrebbe essere preferibile ricorrere a metodi robusti o modelli alternativi (ad esempio, approcci bayesiani) che non presuppongono la normalità. In definitiva, la decisione finale dipenderà dal contesto di ricerca, dalla natura dei dati e dagli obiettivi analitici, privilegiando sempre un equilibrio tra rigore statistico e interpretabilità dei risultati.\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] MASS_7.3-65           datawizard_1.2.0      pillar_1.11.0        \n#>  [4] tinytable_0.13.0      patchwork_1.3.2       ggdist_3.3.3         \n#>  [7] tidybayes_3.0.7       bayesplot_1.14.0      ggplot2_3.5.2        \n#> [10] reliabilitydiag_0.2.1 priorsense_1.1.1      posterior_1.6.1      \n#> [13] loo_2.8.0             rstan_2.32.7          StanHeaders_2.32.10  \n#> [16] brms_2.22.0           Rcpp_1.1.0            sessioninfo_1.2.3    \n#> [19] conflicted_1.2.0      janitor_2.2.1         matrixStats_1.5.0    \n#> [22] modelr_0.1.11         tibble_3.3.0          dplyr_1.1.4          \n#> [25] tidyr_1.3.1           rio_1.2.3             here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#>  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#>  [7] snakecase_0.11.1      compiler_4.5.1        systemfonts_1.2.3    \n#> [10] vctrs_0.6.5           stringr_1.5.1         pkgconfig_2.0.3      \n#> [13] arrayhelpers_1.1-0    fastmap_1.2.0         backports_1.5.0      \n#> [16] labeling_0.4.3        rmarkdown_2.29        ragg_1.5.0           \n#> [19] purrr_1.1.0           xfun_0.53             cachem_1.1.0         \n#> [22] jsonlite_2.0.0        broom_1.0.9           parallel_4.5.1       \n#> [25] R6_2.6.1              stringi_1.8.7         RColorBrewer_1.1-3   \n#> [28] lubridate_1.9.4       estimability_1.5.1    knitr_1.50           \n#> [31] zoo_1.8-14            pacman_0.5.1          Matrix_1.7-4         \n#> [34] splines_4.5.1         timechange_0.3.0      tidyselect_1.2.1     \n#> [37] abind_1.4-8           yaml_2.3.10           codetools_0.2-20     \n#> [40] curl_7.0.0            pkgbuild_1.4.8        lattice_0.22-7       \n#> [43] withr_3.0.2           bridgesampling_1.1-2  coda_0.19-4.1        \n#> [46] evaluate_1.0.5        survival_3.8-3        RcppParallel_5.1.11-1\n#> [49] tensorA_0.36.2.1      checkmate_2.3.3       stats4_4.5.1         \n#> [52] insight_1.4.2         distributional_0.5.0  generics_0.1.4       \n#> [55] rprojroot_2.1.1       rstantools_2.5.0      scales_1.4.0         \n#> [58] xtable_1.8-4          glue_1.8.0            emmeans_1.11.2-8     \n#> [61] tools_4.5.1           mvtnorm_1.3-3         grid_4.5.1           \n#> [64] QuickJSR_1.8.0        colorspace_2.1-1      nlme_3.1-168         \n#> [67] cli_3.6.5             textshaping_1.0.3     svUnit_1.0.8         \n#> [70] Brobdingnag_1.2-9     V8_7.0.0              gtable_0.3.6         \n#> [73] digest_0.6.37         TH.data_1.1-4         htmlwidgets_1.6.4    \n#> [76] farver_2.1.2          memoise_2.0.1         htmltools_0.5.8.1    \n#> [79] lifecycle_1.0.4\n```\n:::\n\n\n## Bibliografia {.unnumbered}\n\n",
    "supporting": [
      "15_gauss_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}