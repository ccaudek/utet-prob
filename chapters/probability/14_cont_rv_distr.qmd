# Distribuzioni di v.c. continue {#sec-prob-cont-prob-distr}


::: callout-important
## In questo capitolo imparerai a:

- comprendere le principali distribuzioni di densit√† di probabilit√†;
- utilizzare R per manipolare e analizzare queste distribuzioni.
::: 

::: callout-tip
## Prerequisiti

- Leggere il capitolo *Continuous random variables* [@blitzstein2019introduction].
- Leggere il capitolo *Special Distributions* [@schervish2014probability].
- Leggere il capitolo *Random Variables and Probability Distributions* [@schervish2014probability].
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()
```
:::


## Introduzione

Proprio come per le variabili casuali discrete, anche per le variabili casuali continue √® possibile rappresentare la variabilit√† di una popolazione attraverso un modello statistico. Tuttavia, mentre le distribuzioni discrete si applicano a fenomeni con un numero finito o numerabile di esiti, le variabili casuali continue richiedono l'uso di **funzioni di densit√† di probabilit√†** (pdf), che descrivono fenomeni in cui i valori possono assumere un continuum di possibilit√†. Queste funzioni ci permettono di modellare e analizzare situazioni in cui i risultati non sono discreti, ma possono variare in modo continuo.

La funzione di densit√† di probabilit√† $f(x)$ associata a una variabile casuale continua $X$ rappresenta la distribuzione della probabilit√† all'interno della popolazione. A differenza delle distribuzioni discrete, dove la probabilit√† √® assegnata direttamente a singoli valori, la pdf non fornisce la probabilit√† di un singolo punto, ma descrive la probabilit√† che $X$ assuma valori all'interno di un intervallo specifico. Questo approccio consente di costruire un modello matematico della popolazione, utile per fare previsioni e comprendere meglio i fenomeni aleatori continui.

## La Distribuzione Uniforme Continua

La **distribuzione uniforme continua** rappresenta un pilastro della teoria delle probabilit√†, caratterizzata da una densit√† di probabilit√† costante su un intervallo definito. Questo modello √® particolarmente utile per descrivere fenomeni casuali dove ogni esito possibile ha identica probabilit√† di verificarsi, come nel caso di uno spinner perfettamente bilanciato o di un generatore di numeri casuali ideale.

### Un esempio intuitivo: lo spinner  
Consideriamo uno spinner circolare con valori angolari compresi tra 0¬∞ e 360¬∞. Se il dispositivo √® **perfettamente equilibrato**, ogni angolo ha la stessa probabilit√† di essere selezionato dopo una rotazione. Questo esperimento costituisce un‚Äôimplementazione concreta della distribuzione uniforme sull‚Äôintervallo $[0, 360)$.

### Simulazione della distribuzione: dal campione piccolo alla convergenza teorica  

Per illustrare il comportamento della distribuzione, analizziamo due scenari distinti attraverso simulazioni numeriche.

**Caso 1: Campione piccolo (n = 20)**  

Generiamo 20 valori casuali e visualizziamoli con un istogramma:  

```{r}
set.seed(123)  # Riproducibilit√† dei risultati
spinner_results <- runif(20, min = 0, max = 360)
print(spinner_results)  # Output dei dati

# Creazione dell'istogramma
ggplot(data.frame(Valori = spinner_results), aes(x = Valori)) +
  geom_histogram(
    binwidth = 10, 
    fill = "skyblue", 
    color = "black", 
    alpha = 0.5
  ) +
  labs(
    x = "Angolo (gradi)", 
    y = "Frequenza relativa"
  ) 
```

L‚Äôistogramma mostra un andamento **irregolare**, riflettendo la variabilit√† intrinseca dei piccoli campioni. Questa disomogeneit√† √® attesa e diminuisce all‚Äôaumentare della dimensione campionaria.

**Caso 2: Campione grande (n = 100,000)**  

Ripetiamo la simulazione con un campione esteso:  

```{r}
spinner_results_large <- runif(100000, min = 0, max = 360)

ggplot(data.frame(Valori = spinner_results_large), aes(x = Valori)) +
  geom_histogram(
    binwidth = 10, 
    fill = "skyblue", 
    color = "black", 
    alpha = 0.5
  ) +
  labs(
    x = "Angolo (gradi)", 
    y = "Frequenza relativa"
  ) 
```

L‚Äôistogramma ora rivela un profilo **piatto e regolare**, in accordo con la forma teorica della distribuzione. Questo risultato dimostra empiricamente la Legge dei Grandi Numeri, dove all‚Äôaumentare delle osservazioni la distribuzione empirica converge a quella teorica.

### La Funzione di Densit√† di Probabilit√† (PDF) 

Per una variabile casuale $X \sim \mathcal{U}(a, b)$, la PDF √® definita come:

$$
f(x) = 
\begin{cases} 
  \displaystyle \frac{1}{b - a} & \text{se } x \in [a, b], \\
  0 & \text{altrimenti}.
\end{cases}
$$

**Propriet√† chiave:**  

- l‚Äôarea totale sotto la curva √® unitaria: $\int_{a}^{b} \frac{1}{b - a} \, dx = 1$;  
- la densit√† √® nulla al di fuori dell‚Äôintervallo $[a, b]$.  

**Applicazione allo spinner:**  

$$
f(x) = \frac{1}{360} \quad \text{per } x \in [0, 360].
$$

**Visualizzazione grafica in R:**  

```{r}
x <- seq(-50, 410, length.out = 500)  
density_uniform <- dunif(x, min = 0, max = 360)

ggplot(data.frame(x = x, y = density_uniform), aes(x = x, y = y)) +
  geom_line(linewidth = 1.2, color = "blue") +
  geom_vline(xintercept = c(0, 360), linetype = "dashed", color = "red") +
  labs(
    x = "x (gradi)", 
    y = "Densit√† f(x)"
  ) +
  xlim(-50, 410) 
```

Il grafico evidenzia la densit√† costante nell‚Äôintervallo $[0, 360]$ e l‚Äôassenza di probabilit√† al di fuori di esso.

### Calcolo delle Probabilit√†: Metodo Geometrico e Funzionale  

La probabilit√† che $X$ assuma valori in un sottointervallo $[c, d] \subseteq [a, b]$ √® data da:  

$$
P(c \leq X \leq d) = \frac{d - c}{b - a}.
$$

**Esempio applicativo:**  

Calcoliamo la probabilit√† che lo spinner si fermi tra 150¬∞ e 250¬∞:  

$$
P(150 \leq X \leq 250) = \frac{250 - 150}{360} = \frac{100}{360} = \frac{5}{18} \approx 0.2778.
$$

**Conferma numerica in R:**  

```{r}
# Approccio manuale
prob_manuale <- (250 - 150) / 360  

# Utilizzo della funzione cumulativa (CDF)
prob_cdf <- punif(250, min = 0, max = 360) - punif(150, min = 0, max = 360)  
```

**Rappresentazione grafica dell‚Äôarea di probabilit√†:**  

```{r}
ggplot(data.frame(x = x, fx = density_uniform), aes(x = x, y = fx)) +
  geom_line(linewidth = 1.2, color = "blue") +
  geom_area(
    data = subset(data.frame(x, fx = density_uniform), x >= 150 & x <= 250),
    aes(x = x, y = fx), 
    fill = "gray", 
    alpha = 0.5
  ) +
  labs(
    x = "x (gradi)", 
    y = "Densit√† f(x)"
  ) 
```

L‚Äôarea grigia corrisponde esattamente al valore di probabilit√† calcolato, illustrando visivamente il concetto di integrazione della PDF.

### Propriet√† Fondamentali: Media e Varianza  

Per $X \sim \mathcal{U}(a, b)$ valgono le seguenti relazioni:  

1. **Valore atteso (centro della distribuzione):**  

   $$
   E(X) = \frac{a + b}{2}.
   $$  
   
   *Esempio:* Per lo spinner, $E(X) = (0 + 360)/2 = 180$ gradi.

2. **Varianza (misura di dispersione):**  

   $$
   \text{Var}(X) = \frac{(b - a)^2}{12}.
   $$  
   
   *Esempio:* Per lo spinner, $\text{Var}(X) = (360 - 0)^2 / 12 = 10,\!800$ gradi¬≤.

### Implementazione in R: Funzioni Principali  

R fornisce quattro funzioni per lavorare con la distribuzione uniforme:  

| Funzione  | Descrizione                           | Esempio d‚Äôuso                   |
|-----------|---------------------------------------|----------------------------------|
| `runif()` | Genera valori casuali                 | `runif(5, min=0, max=1)`        |
| `dunif()` | Calcola la densit√† $f(x)$             | `dunif(180, min=0, max=360)`    |
| `punif()` | Calcola la CDF $P(X \leq x)$          | `punif(250, min=0, max=360)`    |
| `qunif()` | Determina il quantile per una probabilit√† | `qunif(0.9, min=0, max=360)`    |

**Esempi operativi:**  

```{r}
# 1. Generazione di 5 numeri casuali in [0, 1]
runif(5, min = 0, max = 1)  

# 2. Valore della densit√† in x = 0.5 per U(0,1)
dunif(0.5, min = 0, max = 1)  

# 3. Probabilit√† cumulativa fino a x = 0.8 per U(0,1)
punif(0.8, min = 0, max = 1)  

# 4. Calcolo del quantile corrispondente al 50¬∞ percentile (mediana)
qunif(0.5, min = 0, max = 360)  
```

## La Distribuzione Esponenziale  

La **distribuzione esponenziale** √® una distribuzione continua fondamentale per modellare il **tempo di attesa** fino al verificarsi di un evento casuale. La sua caratteristica distintiva √® la **propriet√† di assenza di memoria**, che la rende unica nel panorama delle distribuzioni probabilistiche.  

### La propriet√† di assenza di memoria: un concetto chiave  

L‚Äô**assenza di memoria** implica che la probabilit√† che un evento si verifichi in un intervallo futuro √® indipendente dal tempo gi√† trascorso.  

**Esempio intuitivo:**  
immaginiamo una persona che sperimenta **attacchi di ansia improvvisi**, il cui tempo tra un episodio e il successivo segue una distribuzione esponenziale. Se l‚Äôindividuo non ha avuto un attacco nelle ultime 2 settimane, la probabilit√† che ne sperimenti uno nei prossimi 3 giorni √® **identica** a quella di una persona appena uscita da un episodio, nel medesimo intervallo di 3 giorni.  

Questa analogia illustra la **propriet√† di assenza di memoria**: il "tempo trascorso dall‚Äôultimo evento" (in questo caso, un attacco di ansia) non influenza la probabilit√† futura. Il sistema non "accumula stress" n√© riduce il rischio col passare del tempo senza episodi, riflettendo dinamiche tipiche di processi psicologici **non legati a meccanismi di apprendimento o adattamento**.  

**Parametri chiave nell‚Äôesempio:** 

- **$\lambda$ (tasso):** Frequenza media degli attacchi (es. 0.1 episodi/giorno). 
- **$\mu = 1/\lambda$ (media):** Tempo medio tra due episodi (es. 10 giorni).  

La distribuzione esponenziale modellizza cos√¨ situazioni in cui il comportamento √® **puramente stocastico** e non influenzato dalla storia precedente, come certi pattern di ansia, impulsivit√† o reazioni fisiologiche a stimoli neutri.

### Struttura matematica: PDF e parametri  

La **funzione di densit√† di probabilit√† (PDF)** di una variabile $X \sim \text{Exp}(\lambda)$ √®:  

$$
f(x) = 
\begin{cases} 
\lambda e^{-\lambda x} & \text{se } x \geq 0, \\
0 & \text{altrimenti},
\end{cases}
$$  

dove:  

- **$\lambda$ (tasso):** numero medio di eventi per unit√† di tempo (es. 0.25 episodi/ora).  
- **$\mu = 1/\lambda$ (media):** tempo medio di attesa per l‚Äôevento (es. 4 ore/episodio).  

**Forma alternativa con $\mu$:**  

$$
f(x) = \frac{1}{\mu} e^{-x/\mu} \quad \text{per } x \geq 0.
$$  

### Propriet√† fondamentali  

Per $X \sim \text{Exp}(\lambda)$:  

| Propriet√†               | Formula                  | Interpretazione                          |  
|-------------------------|--------------------------|------------------------------------------|  
| **Valore atteso (Œº)**   | $E(X) = \frac{1}{\lambda}$ | Tempo medio di attesa per l‚Äôevento.      |  
| **Varianza**            | $\text{Var}(X) = \frac{1}{\lambda^2}$ | Dispersione cresce col quadrato di 1/Œª. |  
| **Deviazione standard** | $\sigma_X = \frac{1}{\lambda}$       | Spread lineare attorno alla media.       |  

**Esempio applicato.**  
Se il tempo medio di pubblicazione dei voti di un esame universitario √® $\mu = 4$ giorni ($\lambda = 0.25$), la PDF √®:  

$$
f(x) = \frac{1}{4} e^{-x/4} \quad (x \geq 0).
$$  

### Visualizzazione della densit√† in R  

```{r}
# Definizione dei parametri
mu <- 4
lambda <- 1 / mu  # 0.25

# Generazione dei punti per il grafico
x <- seq(0, 20, by = 0.1)
pdf <- dexp(x, rate = lambda)

# Creazione del grafico
ggplot(data.frame(x = x, y = pdf), aes(x = x, y = y)) +
  geom_line(linewidth = 1.2, color = "darkblue") +
  labs(
    x = "Tempo di attesa (giorni)", 
    y = "Densit√† f(x)",
    title = paste("PDF esponenziale (Œº =", mu, "giorni)")
  ) 
``` 

Il grafico mostra un **decadimento esponenziale**: la probabilit√† decresce rapidamente all‚Äôaumentare del tempo.  

### Calcolo delle Probabilit√†: Tre Scenari   

**1. Probabilit√† cumulativa: $P(X \leq 1.5)$** -- qual √® la probabilit√† che il voto venga pubblicato entro un giorno e mezzo?

Utilizziamo la **funzione di ripartizione (CDF)**:  

$$
P(X \leq 1.5) = 1 - e^{-\lambda \cdot 1.5} = 1 - e^{-0.25 \cdot 1.5} \approx 0.312.
$$  

**Codice R:**  

```{r}
pexp(1.5, rate = lambda)  # Restituisce 0.312
```  

**Visualizzazione:** 

```{r}
# Area sotto la curva per X <= 1.5
ggplot(data.frame(x = x, y = pdf), aes(x = x, y = y)) +
  geom_line(linewidth = 1.2, color = "darkblue") +
  geom_area(
    data = subset(data.frame(x, y = pdf), x <= 1.5),
    aes(x = x, y = y), 
    fill = "gray", 
    alpha = 0.5
  ) +
  labs(
    x = "Tempo (giorni)", 
    y = "Densit√†",
    title = "Probabilit√† P(X ‚â§ 1.5)"
  )
```  

**2. Probabilit√† intervallo: $P(1 \leq X \leq 6)$** -- qual √® la probabilit√† che il voto venga pubblicato in un intervallo compreso tra 1 e 6 giorni dopo l'esame?

Calcoliamo la differenza tra due CDF:  

$$
P(1 \leq X \leq 6) = F(6) - F(1) = e^{-0.25 \cdot 1} - e^{-0.25 \cdot 6} \approx 0.491.
$$  

**Codice R:**  

```{r}
pexp(6, rate = lambda) - pexp(1, rate = lambda)  # 0.491
```  

**Visualizzazione:**  

```{r}
# Area per 1 <= X <= 6
ggplot(data.frame(x = x, y = pdf), aes(x = x, y = y)) +
  geom_line(linewidth = 1.2, color = "darkblue") +
  geom_area(
    data = subset(data.frame(x, y = pdf), x >= 1 & x <= 6),
    aes(x = x, y = y), 
    fill = "gray", 
    alpha = 0.5
  ) +
  labs(
    x = "Tempo (giorni)", 
    y = "Densit√†",
    title = "Probabilit√† P(1 ‚â§ X ‚â§ 6)"
  )
```  

**3. Probabilit√† della coda: $P(X \geq 5.5)$** -- qual √® la probabilit√† di un ritardo nella pubblicazione del voto superiore a 5.5 giorni dall'esame?

Usiamo il complemento della CDF:  

$$
P(X \geq 5.5) = 1 - P(X \leq 5.5) = e^{-0.25 \cdot 5.5} \approx 0.252.
$$  

**Codice R:** 

```{r}
1 - pexp(5.5, rate = lambda)  # 0.252
# Alternativa equivalente:
pexp(5.5, rate = lambda, lower.tail = FALSE)
```  

**Visualizzazione:**  

```{r}
# Area per X >= 5.5
ggplot(data.frame(x = x, y = pdf), aes(x = x, y = y)) +
  geom_line(linewidth = 1.2, color = "darkblue") +
  geom_area(
    data = subset(data.frame(x, y = pdf), x >= 5.5),
    aes(x = x, y = y), 
    fill = "gray", 
    alpha = 0.5
  ) +
  labs(
    x = "Tempo (giorni)", 
    y = "Densit√†",
    title = "Probabilit√† P(X ‚â• 5.5)"
  )
```  

### Simulazione e convergenza alla teoria  

Generiamo 1,000,000 di osservazioni da $\text{Exp}(\lambda = 0.25)$ e confrontiamo l‚Äôistogramma con la PDF teorica:  

```{r}
set.seed(123)
simulated_data <- rexp(1e6, rate = lambda)

ggplot(data.frame(x = simulated_data), aes(x = x)) +
  geom_histogram(
    aes(y = after_stat(density)), 
    bins = 100, 
    fill = "skyblue", 
    color = "black",
    alpha = 0.6
  ) +
  geom_line(
    data = data.frame(x = x, y = pdf),
    aes(x = x, y = y), 
    color = "red", 
    linewidth = 1.2
  ) +
  coord_cartesian(xlim = c(0, 20)) +  # Escludiamo code estreme
  labs(
    x = "Tempo di attesa (giorni)", 
    y = "Densit√†",
    title = "Dati simulati e PDF teorica"
  ) 
```  

L‚Äôistogramma si allinea perfettamente alla curva rossa, dimostrando la **Legge dei Grandi Numeri**.  

### Funzioni R per la distribuzione esponenziale  

R offre quattro funzioni essenziali:  

| Funzione  | Descrizione                           | Esempio d‚Äôuso                    | Output Esempio      |  
|-----------|---------------------------------------|-----------------------------------|---------------------|  
| `dexp()`  | Calcola la densit√† $f(x)$             | `dexp(2, rate = 0.25)`           | 0.1516              |  
| `pexp()`  | Calcola la CDF $P(X \leq x)$          | `pexp(4, rate = 0.25)`           | 0.632 (‚âà1 - e‚Åª¬π)    |  
| `qexp()`  | Trova il quantile $x$ per una probabilit√† | `qexp(0.5, rate = 0.25)`       | ~2.773 (mediana)    |  
| `rexp()`  | Genera valori casuali                 | `rexp(5, rate = 0.25)`           | [3.1, 0.8, 5.2, ...] |  


## Distribuzione Normale  

La **distribuzione normale** (o gaussiana) √® fondamentale in statistica per modellare fenomeni naturali, sociali e psicologici. La sua importanza deriva dal **Teorema del Limite Centrale**, che garantisce la convergenza alla normalit√† per somme di variabili casuali indipendenti.  

### La Famiglia delle Distribuzioni Normali  

Ogni distribuzione normale √® definita da due parametri:  

- **$\mu$ (media):** centro della distribuzione;  
- **$\sigma$ (deviazione standard):** dispersione dei dati attorno alla media.  

La funzione di densit√† √®:  

$$
f(y; \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(y - \mu)^2}{2\sigma^2}}.
$$ {#eq-gaussian}

### Distribuzione Normale Standardizzata  

La **normale standardizzata** √® un caso speciale con $\mu = 0$ e $\sigma = 1$. Qualsiasi variabile $Y \sim \mathcal{N}(\mu, \sigma)$ pu√≤ essere standardizzata tramite:  

$$
Z = \frac{Y - \mu}{\sigma}.
$$ {#eq-z-score}

Questa trasformazione preserva la forma della distribuzione ma riporta i valori in unit√† di deviazione standard (**Z-score**), permettendo confronti universali.  

#### Relazione tra Deviazione Standard e Distribuzione  

**La regola empirica 68-95-99.7 vale per tutte le distribuzioni normali**, indipendentemente da $\mu$ e $\sigma$:  

- **68.3%** dei dati cade entro $\pm 1\sigma$ dalla media;  
- **95.4%** entro $\pm 2\sigma$;  
- **99.7%** entro $\pm 3\sigma$.  

Per intervalli specifici legati a test statistici:  

- **$\pm 1.96\sigma$** copre il **95%** dei dati (intervallo di confidenza al 95%);
- **$\pm 2.576\sigma$** copre il **99%** (intervallo al 99%).  


### Origini storiche e connessione alla binomiale  

Abraham de Moivre osserv√≤ che distribuzioni binomiali con $n$ elevato approssimano una normale. Ad esempio:  

- con $n=10$ e $p=0.9$, la distribuzione √® asimmetrica; 
- con $n=1000$, la forma diventa simmetrica e campanulare.  

### Simulazione di Passeggiate Casuali  

La distribuzione normale emerge naturalmente come risultato della **somma di un gran numero di effetti casuali indipendenti**, un principio formalizzato dal **Teorema del Limite Centrale**. Questo la rende ideale per modellare:  

- **errori di misurazione**, dove piccole fluttuazioni casuali (strumentali, ambientali, umane) si combinano;  
- **fenomeni biologici multifattoriali** come altezza, peso o QI, influenzati da decine di fattori genetici, ambientali e nutrizionali che interagiscono in modo additivo;  
- **processi sociali** come i punteggi dei test, dove il risultato finale √® il prodotto cumulativo di abilit√† innate, studio, stato emotivo e altro.  

**Simulazione con passeggiate casuali**

Per visualizzare concretamente questo fenomeno, consideriamo una **passeggiata casuale unidimensionale** semplificata:  

1. **Impostazione:**  
   - 1,000 partecipanti partono dalla posizione 0;  
   - ogni partecipante compie **16 passi consecutivi**;  
   - ogni passo √® determinato da un generatore casuale che assegna uno spostamento compreso tra -1 e +1 unit√† (*simulando l'effetto di piccole perturbazioni indipendenti*).  

2. **Dinamica:**  
   la posizione finale di ciascun partecipante √® la **somma algebrica** degli spostamenti casuali. Nonostante ogni passo individuale segua una distribuzione uniforme, la posizione finale aggregata di tutti i partecipanti mostrer√† una distribuzione a campana tipica della normale.  
   
```{r}
# Parametri
numero_passi <- 16
ripetizioni <- 1000

# Generazione di passeggiate casuali
set.seed(123)
x <- matrix(0, nrow = numero_passi + 1, ncol = ripetizioni)

for (i in 1:ripetizioni) {
  passi <- runif(numero_passi, min = -1, max = 1)
  x[-1, i] <- cumsum(passi)
}

# Grafico delle passeggiate casuali
df <- data.frame(
  Passo = rep(0:numero_passi, times = ripetizioni), 
  Distanza = as.vector(x)
)

ggplot(
  df, 
  aes(
    x = Passo, 
    y = Distanza, 
    group = rep(1:ripetizioni, each = numero_passi + 1))
  ) +
  geom_line(color = "blue", alpha = 0.05) +
  labs(
    title = "Passeggiate Casuali", 
    x = "Numero di Passi", y = "Distanza dall'Origine"
  )
```

```{r}  
# Codice di simulazione (esempio concettuale)  
set.seed(123)  
n_partecipanti <- 1000  
n_passi <- 16  

# Genera spostamenti casuali (-1 a +1)  
spostamenti <- matrix(runif(n_partecipanti * n_passi, min = -1, max = 1), ncol = n_passi)  

# Calcola le posizioni finali  
posizioni_finali <- rowSums(spostamenti)  

# Visualizzazione  
ggplot(data.frame(Posizione = posizioni_finali), aes(x = Posizione)) +  
  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "lightblue", alpha = 0.7) +  
  stat_function(fun = dnorm, args = list(mean = mean(posizioni_finali), sd = sd(posizioni_finali)), color = "red", linewidth = 1) +  
  labs(title = "Distribuzione delle posizioni finali", x = "Posizione", y = "Densit√†")  
```  

**Risultato atteso:**  
l'istogramma delle posizioni finali aderir√† alla curva rossa (normale teorica), dimostrando come la **combinazione di piccole variazioni casuali** produca una distribuzione gaussiana, anche partendo da passi non-normali. Questo esperimento illustra l‚Äôonnipresenza della normale in contesti reali governati da molteplici fattori indipendenti.

**Perch√© 16 passi?**  
La scelta di 16 passi non √® arbitraria:  

- un numero ridotto di passi (es. 3-5) produrrebbe una distribuzione ancora vicina all‚Äôuniforme; 
- con 16 passi, la **simmetria** e la **curvatura tipica della gaussiana** diventano chiaramente riconoscibili senza richiedere simulazioni massicce.

### Propriet√† fondamentali  

- **Media**: $\mathbb{E}(Y) = \mu$;  
- **Varianza**: $\mathbb{V}(Y) = \sigma^2$.  

### Funzioni R per la Normale  

| Funzione  | Descrizione                          | Esempio                       |  
|-----------|--------------------------------------|-------------------------------|  
| `dnorm()` | Densit√† a un punto $y$               | `dnorm(115, mean=100, sd=15)` |  
| `pnorm()` | Probabilit√† cumulativa $P(Y \leq y)$ | `pnorm(115, mean=100, sd=15)` |  
| `qnorm()` | Quantile per una probabilit√† $p$     | `qnorm(0.975, mean=100, sd=15)` |  
| `rnorm()` | Genera valori casuali                | `rnorm(10, mean=100, sd=15)`  |  


### Visualizzazione delle aree critiche  

Le aree sotto la curva corrispondenti a $\pm 1\sigma$, $\pm 1.96\sigma$, e $\pm 3\sigma$ possono essere visualizzate in R:  

```{r}  
# Esempio per ¬±1.96œÉ (95% di confidenza)  
mu <- 100  
sigma <- 15  
x <- seq(mu - 4*sigma, mu + 4*sigma, length.out=1000)  
df <- data.frame(x=x, pdf=dnorm(x, mu, sigma))  

ggplot(df, aes(x=x, y=pdf)) +  
  geom_line(color="blue") +  
  geom_area(data=subset(df, x >= mu - 1.96*sigma & x <= mu + 1.96*sigma),  
            fill="gray", alpha=0.5) +  
  labs(title="95% dei dati entro ¬±1.96œÉ", x="Valori", y="Densit√†")  
```  

**In sintesi**, la distribuzione normale standardizzata permette di standardizzare qualsiasi fenomeno Gaussiano, rendendo confrontabili dati eterogenei. La relazione tra deviazioni standard e aree sottese √® universale: **indipendentemente dalla media e varianza originale, il 68-95-99.7% dei dati cadr√† sempre entro 1-2-3œÉ**. 



::: {.callout-important title="Esercizio" collapse="true"}
Una psicologa vuole studiare i **livelli di ansia** tra gli studenti universitari durante la settimana degli esami. Dalle ricerche precedenti si sa che nella **popolazione universitaria**:

- il **punteggio medio di ansia** √® di **50 punti** su una scala da 0 a 100;
- la **deviazione standard** dei punteggi di ansia √® **10 punti**.

La psicologa decide di **estrarre un campione casuale di 25 studenti**.

Vogliamo usare la **distribuzione campionaria della media** per rispondere a due domande:

1. Qual √® la probabilit√† di ottenere una media campionaria maggiore di 54 punti?
2. Quale media campionaria rappresenta il **95¬∞ percentile** della distribuzione campionaria?

üìò **Concetti chiave.**

La **distribuzione campionaria della media** ha:

- la **stessa media** della popolazione ($\mu$),
- una **deviazione standard pi√π piccola**, detta *errore standard della media* (SE):

$$
SE = \frac{\sigma}{\sqrt{n}} = \frac{10}{\sqrt{25}} = 2 .
$$

Useremo due funzioni importanti in R:

- `dnorm(x, mean, sd)`: calcola la **densit√†** della normale in un punto $x$.
- `qnorm(p, mean, sd)`: calcola il valore di $x$ corrispondente a una certa **probabilit√† cumulativa** $p$.


‚úÖ **Codice base.**

```{r}
# Parametri della popolazione e del campione
mu <- 50       # media della popolazione
sigma <- 10    # deviazione standard
n <- 25        # dimensione campione

# Errore standard della media
SE <- sigma / sqrt(n)
SE
```

üîç **Domanda 1: Probabilit√† di ottenere una media > 54.**

```{r}
# Probabilit√† che la media campionaria sia maggiore di 54
p_oltre_54 <- pnorm(54, mean = mu, sd = SE, lower.tail = FALSE)
p_oltre_54
```

La probabilit√† √® molto bassa. Questo vuol dire che, se la vera media della popolazione fosse 50, ottenere una media campionaria superiore a 54 sarebbe raro.

üîç **Domanda 2: Media al 95¬∞ percentile.**

```{r}
# Calcolo del valore soglia al 95¬∞ percentile
q_95 <- qnorm(0.95, mean = mu, sd = SE)
q_95
```

All'interno della distribuzione campionaria, solo il 5% dei campioni ha una media superiore a questo valore.

üìä Grafico 1: Probabilit√† di media > 54

```{r}
# Dati per la distribuzione normale
x_vals <- seq(44, 56, length.out = 300)
dens_vals <- dnorm(x_vals, mean = mu, sd = SE)
df <- data.frame(x = x_vals, y = dens_vals)

# Grafico
ggplot(df, aes(x, y)) +
  geom_line(color = "black") +
  geom_area(data = subset(df, x >= 54), aes(x, y), fill = "red", alpha = 0.4) +
  geom_vline(xintercept = 54, color = "red", linetype = "dashed") +
  labs(
    title = "Distribuzione campionaria della media (n = 25)",
    subtitle = "Area rossa = P(media > 54)",
    x = "Media campionaria",
    y = "Densit√†"
  ) 
```


üìä **Grafico 2: Valore al 95¬∞ percentile**

```{r}
# Grafico con il 95¬∞ percentile evidenziato
ggplot(df, aes(x, y)) +
  geom_line(color = "black") +
  geom_area(data = subset(df, x <= q_95), aes(x, y), fill = "blue", alpha = 0.4) +
  geom_vline(xintercept = q_95, color = "blue", linetype = "dashed") +
  labs(
    title = "Distribuzione campionaria della media (n = 25)",
    subtitle = "Area blu = 95% dei campioni (valore critico ‚âà 53.29)",
    x = "Media campionaria",
    y = "Densit√†"
  ) 
```

**Domande di approfondimento.**

1. Perch√© l'errore standard della media √® pi√π piccolo della deviazione standard della popolazione?
2. Se la dimensione del campione aumentasse a 100, come cambierebbe l'errore standard?
3. Che cosa rappresenta `pnorm(54, ...)` nel nostro contesto?
4. In quali casi, in psicologia, potresti voler calcolare il 95¬∞ percentile di una distribuzione campionaria?

**Simulazione Monte Carlo.**

Simuliamo 10.000 campioni casuali, ciascuno di **25 studenti**, estratti da una popolazione normale con media = 50 e deviazione standard = 10. Per ogni campione calcoliamo la media. Alla fine, visualizziamo la distribuzione di queste medie.

```{r}
set.seed(123)  # per rendere la simulazione replicabile

# Parametri
mu <- 50
sigma <- 10
n <- 25
n_sim <- 10000  # numero di campioni

# Simulazione: 10.000 medie campionarie
campioni <- replicate(n_sim, mean(rnorm(n, mean = mu, sd = sigma)))

# Visualizza le prime 5 medie
head(campioni)
```

üìä **Istogramma delle medie campionarie.**

```{r}

df_sim <- data.frame(media_campionaria = campioni)

ggplot(df_sim, aes(x = media_campionaria)) +
  geom_histogram(aes(y = ..density..), bins = 40, fill = "lightblue", color = "black") +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma / sqrt(n)),
                color = "red", size = 1) +
  labs(
    title = "Distribuzione delle medie campionarie",
    subtitle = "Istogramma di 10.000 medie di campioni di 25 studenti",
    x = "Media campionaria",
    y = "Densit√†"
  ) 
```

Cosa si osserva?

- Le medie **non sono tutte uguali**, ma si distribuiscono **intorno alla media vera** (50).
- La forma della distribuzione delle medie √® **normale**, anche se i dati originali **non devono necessariamente esserlo** (grazie al Teorema del Limite Centrale).
- La **deviazione standard** della distribuzione simulata √® vicina all'**errore standard teorico**:

```{r}
# Confronto tra errore standard teorico e osservato
SE_teorico <- sigma / sqrt(n)
SE_osservato <- sd(campioni)

c(SE_teorico = SE_teorico, SE_osservato = SE_osservato)
```

**Domande di approfondimento.**

1. Perch√© la forma dell‚Äôistogramma √® simile a una curva normale?
2. Cosa succederebbe alla **larghezza della distribuzione** se aumentassimo la dimensione del campione?
3. Se la media osservata in un esperimento reale fosse **fuori dalla zona centrale**, come potremmo interpretarla?
:::

::: {.callout-important title="Esercizio" collapse="true"}
Consideriamo un esercizio in cui si utilizza la **distribuzione normale** e si consultano le **tavole della normale standard** (z) per risolvere il problema dopo la **standardizzazione**.

In uno studio su un campione di 600 studenti universitari, i punteggi ottenuti a un test di ansia da esame seguono una distribuzione normale con **media** $\mu = 50$ e **deviazione standard** $\sigma = 10$.

1. Qual √® la **probabilit√†** che uno studente scelto a caso ottenga un punteggio **inferiore a 65**?

2. Qual √® la **percentuale di studenti** che ottengono un punteggio **compreso tra 45 e 60**?

3. Qual √® il punteggio minimo che uno studente deve ottenere per rientrare nel **10% superiore** della distribuzione?

**1. Probabilit√† che $X < 65$.**

Standardizziamo:

$$
Z = \frac{X - \mu}{\sigma} = \frac{65 - 50}{10} = \frac{15}{10} = 1.5 .
$$

Cerchiamo $P(Z < 1.5)$ nella **tavola della normale standard**:

$$
P(Z < 1.5) \approx 0.9332 .
$$

**Risposta**: La probabilit√† che uno studente ottenga meno di 65 √® circa **93.32%**.

**2. Probabilit√† che $45 < X < 60$.**

Calcoliamo gli z-score:

$$
Z_1 = \frac{45 - 50}{10} = -0.5 \quad ; \quad Z_2 = \frac{60 - 50}{10} = 1.0 .
$$

Cerchiamo nelle tavole:

- $P(Z < 1.0) \approx 0.8413$
- $P(Z < -0.5) \approx 0.3085$

Quindi:

$$
P(45 < X < 60) = P(Z < 1.0) - P(Z < -0.5) = 0.8413 - 0.3085 = 0.5328
$$

**Risposta**: Circa il **53.28%** degli studenti ha un punteggio tra 45 e 60.

**3. Punteggio minimo per rientrare nel 10% superiore.**

Il 10% superiore corrisponde a:

$$
P(Z > z) = 0.10 \Rightarrow P(Z < z) = 0.90 .
$$

Dalla tavola:  
$P(Z < 1.28) \approx 0.8997$,  
$P(Z < 1.29) \approx 0.9015$

Prendiamo $z = 1.28$

Ora risolviamo per $X$:

$$
X = z \cdot \sigma + \mu = 1.28 \cdot 10 + 50 = 62.8
$$

**Risposta**: Il punteggio minimo per rientrare nel 10% superiore √® circa **62.8**.
:::

## Distribuzione Chi-Quadrato

La distribuzione **$\chi^2$** deriva dalla distribuzione normale e descrive la somma dei quadrati di $k$ variabili casuali indipendenti e identicamente distribuite (i.i.d.) che seguono la distribuzione normale standard $\mathcal{N}(0, 1)$. Una variabile casuale $\chi^2_{~k}$ con $k$ gradi di libert√† √® definita come:

$$
Z_1^2 + Z_2^2 + \dots + Z_k^2,
$$ {#eq-chisq-def}

dove $Z_1, Z_2, \dots, Z_k \sim \mathcal{N}(0, 1)$. Il parametro $k$, detto **gradi di libert√†** ($\nu$), determina la forma della distribuzione.

### Funzione di densit√†

La densit√† di probabilit√† della distribuzione $\chi^2_{~\nu}$ √® data da:

$$
f(x) = C_{\nu} x^{\nu/2 - 1} \exp(-x/2), \quad \text{per } x > 0,
$$ {#eq-chisq-def2}

dove $C_{\nu}$ √® una costante di normalizzazione. 

### Simulazione della Distribuzione Chi-Quadrato

Utilizziamo la definizione per simulare la distribuzione $\chi^2$ con 3 gradi di libert√†.

```{r}
# Impostare il seed per la riproducibilit√†
set.seed(1234)

# Generare 1000 valori casuali per 3 variabili gaussiane standard
n <- 1000
var1 <- rnorm(n, mean = 0, sd = 1)
var2 <- rnorm(n, mean = 0, sd = 1)
var3 <- rnorm(n, mean = 0, sd = 1)

# Calcolare la somma dei quadrati
chi_sq_values <- var1^2 + var2^2 + var3^2

# Creare un dataframe per il grafico
data <- data.frame(chi_sq_values = chi_sq_values)

# Istogramma e densit√† teorica
ggplot(data, aes(x = chi_sq_values)) +
  geom_histogram(
    aes(y = after_stat(density)), 
    bins = 30, fill = "lightblue", color = "black", alpha = 0.7
    ) +
  stat_function(fun = dchisq, args = list(df = 3), color = "red", size = 1) +
  labs(
    title = "Distribuzione Chi-Quadrato (df = 3)",
    x = "Valore",
    y = "Densit√†"
  )
```

- L'istogramma rappresenta i valori empirici simulati;
- la curva rossa rappresenta la densit√† teorica della distribuzione $\chi^2_{~3}$.

### Media e Varianza Empiriche

Calcoliamo la media e la varianza dei valori simulati:

```{r}
# Media empirica
mean(chi_sq_values)

# Varianza empirica
var(chi_sq_values)
```

Questi valori possono essere confrontati con le propriet√† teoriche della distribuzione $\chi^2$:

- **media**: $\nu = 3$;
- **varianza**: $2\nu = 6$.

### Grafico per Diversi Gradi di Libert√†

Confrontiamo le distribuzioni $\chi^2$ per diversi valori di $\nu$.

```{r}
# Intervallo di x
x <- seq(0, 40, by = 0.1)

# Gradi di libert√†
nus <- c(2, 4, 8, 16)

# Creare un dataframe
data <- do.call(rbind, lapply(nus, function(nu) {
  data.frame(x = x, f_x = dchisq(x, df = nu), nu = as.factor(nu))
}))

# Grafico
ggplot(data, aes(x = x, y = f_x, color = nu)) +
  geom_line(size = 1) +
  labs(
    x = "x",
    y = "f(x)",
    color = expression(nu)
  ) 
```

### Propriet√† della Distribuzione Chi-Quadrato

1. **Asimmetria**: La distribuzione $\chi^2_{\nu}$ √® asimmetrica, ma diventa pi√π simmetrica al crescere di $\nu$.
2. **Media**: $\mathbb{E}[\chi^2_{\nu}] = \nu$.
3. **Varianza**: $\mathbb{V}[\chi^2_{\nu}] = 2\nu$.
4. **Convergenza**: Per $\nu \to \infty$, $\chi^2_{\nu} \to \mathcal{N}(\nu, 2\nu)$.
5. **Somma**: La somma di variabili $\chi^2$ indipendenti con gradi di libert√† $\nu_1, \nu_2, \dots, \nu_k$ segue una distribuzione $\chi^2$ con $\nu = \sum_{i=1}^k \nu_i$.

### Applicazioni

La distribuzione $\chi^2$ √® utilizzata in molteplici ambiti statistici, tra cui:

- **test di indipendenza**: per verificare se due variabili categoriche sono indipendenti;
- **test di adattamento**: per confrontare una distribuzione empirica con una teorica.

## Distribuzione $t$ di Student

La **distribuzione $t$ di Student** √® una delle distribuzioni fondamentali della statistica inferenziale. Deriva dalle distribuzioni Normale e Chi-quadrato ed √® particolarmente utile per analizzare campioni di piccole dimensioni o situazioni in cui la varianza della popolazione √® sconosciuta.

### Definizione Formale

Se:

- $Z \sim \mathcal{N}(0, 1)$ (distribuzione Normale standard),
- $W \sim \chi^2_{\nu}$ (distribuzione Chi-quadrato con $\nu$ gradi di libert√†),

e $Z$ e $W$ sono indipendenti, allora la variabile casuale

$$
T = \frac{Z}{\sqrt{\frac{W}{\nu}}}
$$ {#eq-t-def}

segue una **distribuzione $t$ di Student** con $\nu$ gradi di libert√†. Si indica come $T \sim t_{\nu}$.

### Propriet√† della Distribuzione $t$ di Student

1. **Forma della distribuzione**:

   - la distribuzione $t$ √® simmetrica rispetto a zero, come la Normale standard ($\mathcal{N}(0, 1)$);
   - presenta **code pi√π pesanti** rispetto alla Normale, riflettendo una maggiore probabilit√† di osservare valori estremi.

2. **Code pesanti e gradi di libert√†**:

   - la pesantezza delle code diminuisce con l'aumentare dei gradi di libert√† ($\nu$);
   - per $\nu \to \infty$, la distribuzione $t$ converge alla distribuzione Normale standard.

3. **Media e varianza**:

   - la **media** √® $0$ per $\nu > 1$;
   - la **varianza** √®:
   
     $$
     \text{Var}(T) = \frac{\nu}{\nu - 2}, \quad \text{per } \nu > 2.
     $$
     
     Per $\nu \leq 2$, la varianza non √® definita.

4. **Applicazioni principali**:

   - **test t di Student**: Confronto delle medie di due gruppi o test per una singola media;
   - **intervalli di confidenza**: Stima dell'intervallo per la media quando la varianza √® sconosciuta.

### Differenze tra la Distribuzione $t$ e la Normale

| **Caratteristica**            | **Distribuzione Normale**     | **Distribuzione $t$ di Student**  |
|-------------------------------|-------------------------------|--------------------------------------|
| Forma                         | Simmetrica, a campana         | Simmetrica, a campana               |
| Code                          | Sottili                       | Pesanti                             |
| Dipendenza dai gradi di libert√† | No                            | S√¨                                  |
| Convergenza                   | Non varia                     | Con $\nu \to \infty$, converge alla Normale |

### Visualizzazione della Distribuzione $t$

Confrontiamo graficamente la distribuzione $t$ con diversi gradi di libert√† e la distribuzione Normale standard:

```{r}
# Creazione dei dati
x <- seq(-4, 4, length.out = 1000)
df <- c(1, 2, 5, 10)  # Gradi di libert√†

# Dataframe con curve di densit√†
data <- data.frame(
  x = rep(x, length(df) + 1),
  density = c(
    dnorm(x),
    dt(x, df[1]),
    dt(x, df[2]),
    dt(x, df[3]),
    dt(x, df[4])
  ),
  distribution = rep(c("Normale", paste("t (df =", df, ")")), each = length(x))
)

# Plot
ggplot(data, aes(x = x, y = density, color = distribution)) +
  geom_line(size = 1) +
  labs(
    x = "Valore",
    y = "Densit√†",
    color = "Distribuzione"
  ) 
```

### Simulazione della Distribuzione $t$

Simuliamo una distribuzione $t$ con 10 gradi di libert√† e confrontiamola con la densit√† teorica.

```{r}
# Impostare il seed per la riproducibilit√†
set.seed(123)

# Simulare 1000 valori da una distribuzione t
n <- 1000
df <- 10  # Gradi di libert√†
t_values <- rt(n, df = df)

# Creare un dataframe per il grafico
data <- data.frame(t_values = t_values)

# Istogramma con densit√† teorica
ggplot(data, aes(x = t_values)) +
  geom_histogram(
    aes(y = after_stat(density)), 
    bins = 30, fill = "lightblue", color = "black", alpha = 0.7
  ) +
  stat_function(fun = dt, args = list(df = df), color = "red", size = 1) +
  labs(
    x = "Valore",
    y = "Densit√†"
  ) 
```

### Propriet√† Teoriche della Distribuzione $t$

1. **Media**:
   $$
   \mathbb{E}[T] = 0, \quad \text{per } \nu > 1.
   $$

2. **Varianza**:
   $$
   \mathbb{V}[T] = \frac{\nu}{\nu - 2}, \quad \text{per } \nu > 2.
   $$

3. **Simmetria**:
   - la distribuzione √® simmetrica rispetto a zero, come la Normale.

4. **Code**:
   - le code sono pi√π pesanti rispetto alla Normale, riflettendo una maggiore incertezza per piccoli campioni.

In conclusione, la distribuzione $t$ di Student √® uno strumento versatile nell'inferenza statistica, trovando applicazione in contesti sia frequentisti che bayesiani. √à particolarmente utile in situazioni in cui la conoscenza della varianza √® limitata o i campioni sono di dimensioni ridotte. Grazie alla sua forma simmetrica e alle code pi√π pesanti rispetto alla distribuzione Normale, la distribuzione $t$ pu√≤ modellare meglio l'incertezza, includendo una maggiore probabilit√† per valori estremi.

Nel contesto bayesiano, la distribuzione $t$ viene utilizzata come:

- **prior informativo robusto**, per modellare parametri con valori plausibili lontani dalla media ma con una penalizzazione graduale per valori estremi.
- **distribuzione predittiva** per sintetizzare l'incertezza derivante da campioni piccoli o con variabilit√† elevata.

In entrambi i paradigmi, la distribuzione $t$ rappresenta una scelta robusta, capace di riflettere in modo flessibile la natura dei dati. Inoltre, per valori elevati dei gradi di libert√†, la distribuzione $t$ converge alla distribuzione Normale, un caso limite che ne estende ulteriormente l‚Äôutilit√† in vari contesti analitici.

## Funzione Beta di Eulero

La **funzione Beta di Eulero** √® una funzione matematica, non una densit√† di probabilit√†, ma √® strettamente collegata alla distribuzione Beta, poich√© appare nella sua definizione. Indicata comunemente con il simbolo $\mathcal{B}(\alpha, \beta)$, la funzione Beta pu√≤ essere espressa in vari modi. Per i nostri scopi, utilizziamo la seguente definizione:

$$
\mathcal{B}(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}\,,
$$ {#eq-eulero-beta-def}

dove $\Gamma(x)$ rappresenta la funzione Gamma, una generalizzazione del fattoriale definita per numeri reali positivi. Quando $x$ √® un numero intero, la funzione Gamma si riduce al fattoriale traslato:

$$
\Gamma(x) = (x-1)!.
$$

::: {#exm-}
Supponiamo di voler calcolare $\mathcal{B}(3, 9)$. Utilizzando la definizione, abbiamo:

$$
\mathcal{B}(3, 9) = \frac{\Gamma(3) \cdot \Gamma(9)}{\Gamma(3 + 9)}.
$$

In R, possiamo calcolarla in tre modi diversi.

1. Utilizzando la definizione con la funzione `gamma()`:

```{r}
alpha <- 3
beta <- 9

beta_function <- gamma(alpha) * gamma(beta) / gamma(alpha + beta)
beta_function
```

2. Utilizzando direttamente la funzione `beta()` di R:

```{r}
beta(alpha, beta)
```

3. Calcolo manuale con fattoriali:

```{r}
(factorial(alpha - 1) * factorial(beta - 1)) / factorial(alpha + beta - 1)
```

Tutti e tre i metodi restituiscono lo stesso risultato, confermando la correttezza della definizione.
:::

La funzione Beta √® utilizzata nella definizione della **densit√† di probabilit√† Beta**. Essa serve a normalizzare la densit√†, garantendo che l'area sotto la curva sia pari a $1$. 

## Distribuzione Beta

La **distribuzione Beta**, indicata come $\mathcal{Beta}(\alpha, \beta)$, √® una distribuzione di probabilit√† continua definita sull‚Äôintervallo $(0, 1)$. √à particolarmente utile per modellare proporzioni, probabilit√†, o in generale qualsiasi fenomeno che assume valori compresi tra 0 e 1.

Questa distribuzione √® molto flessibile: a seconda dei valori dei parametri $\alpha$ e $\beta$, pu√≤ assumere forme simmetriche, asimmetriche, concave, convesse, ecc. √à frequentemente utilizzata come distribuzione a priori nei modelli bayesiani per parametri che rappresentano probabilit√†.

### Definizione

:::: {.definition #def-beta}
Sia $\theta$ una variabile casuale continua. Se $\theta$ segue una distribuzione Beta con parametri $\alpha > 0$ e $\beta > 0$, scriviamo:

$$
\theta \sim \mathcal{Beta}(\alpha, \beta),
$$

e la sua **funzione di densit√† di probabilit√†** (pdf) √® data da:

$$
\mathcal{Beta}(\theta \mid \alpha, \beta) = \frac{1}{\mathcal{B}(\alpha, \beta)} \, \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}, \quad \text{per } \theta \in (0, 1),
$$ {#eq-beta-distr-def}

dove $\mathcal{B}(\alpha, \beta)$ √® la funzione Beta (o funzione beta di Eulero).
::::

### Rappresentazione alternativa

Un‚Äôespressione equivalente della densit√†, che mette in evidenza il legame con la funzione Gamma, √®:

$$
\mathcal{Beta}(\theta \mid \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \, \Gamma(\beta)} \, \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}, \quad \text{per } \theta \in (0, 1),
$$ {#eq-beta-distr-def2}

dove $\Gamma(\cdot)$ √® la funzione Gamma, che generalizza il fattoriale: $\Gamma(n) = (n - 1)!$ per ogni intero positivo $n$.


### Ruolo dei Parametri $\alpha$ e $\beta$

I parametri $\alpha$ e $\beta$ determinano la forma della distribuzione:

- **$\alpha > 1$**: favorisce valori di $\theta$ vicini a 1.
- **$\beta > 1$**: favorisce valori di $\theta$ vicini a 0.
- **$\alpha = \beta = 1$**: corrisponde alla distribuzione uniforme sull'intervallo $[0, 1]$.
- **$\alpha, \beta < 1$**: la distribuzione √® bimodale, concentrandosi agli estremi (vicino a 0 e 1).

### Propriet√† della Distribuzione Beta

1. **Valore atteso**:
   $$
   \mathbb{E}(\theta) = \frac{\alpha}{\alpha + \beta}.
   $$

2. **Varianza**:
   $$
   \mathbb{V}(\theta) = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.
   $$

3. **Moda** (se $\alpha, \beta > 1$):
   $$
   \text{Moda}(\theta) = \frac{\alpha - 1}{\alpha + \beta - 2}.
   $$

Queste propriet√† evidenziano come $\alpha$ e $\beta$ possano essere interpretati come "successi" e "fallimenti" in una serie di prove, fornendo un collegamento intuitivo con la distribuzione binomiale.

### Relazione con la Distribuzione Binomiale

La **distribuzione Beta** pu√≤ essere interpretata come una generalizzazione continua della distribuzione binomiale. Mentre la distribuzione binomiale descrive la **probabilit√† di osservare un certo numero di successi** in un numero fissato di prove ($n$), la distribuzione Beta descrive l‚Äô**incertezza sulla probabilit√† di successo** $\theta$ stessa, trattandola come una variabile casuale.

#### Contesto bayesiano

In un contesto di **inferenza bayesiana**, la distribuzione Beta viene comunemente utilizzata come **distribuzione a priori coniugata** per il modello binomiale. Ci√≤ significa che, se si assume una distribuzione Beta come prior per $\theta$, anche la distribuzione a posteriori (dopo aver osservato i dati) sar√† una Beta, ma con parametri aggiornati.

Supponiamo:

- che $\theta$ sia la probabilit√† di successo in un compito con esiti binari (es. risposta corretta o errata),
- di assumere una distribuzione a priori:  
  $$
  \theta \sim \mathcal{Beta}(\alpha, \beta),
  $$
- e di osservare $y$ successi su $n$ prove, con modello di verosimiglianza:
  $$
  y \sim \mathcal{Binom}(n, \theta).
  $$

Allora, per il teorema di Bayes, la distribuzione a posteriori di $\theta$ sar√†:

$$
\theta \mid \text{dati} \sim \mathcal{Beta}(\alpha + y, \beta + n - y).
$$

#### Vantaggi della coniugatezza

Questo aggiornamento √® particolarmente comodo perch√©:

- si ottiene in forma chiusa (senza dover ricorrere a metodi numerici),
- i parametri $\alpha$ e $\beta$ possono essere interpretati come **conteggi fittizi di successi e insuccessi** prima dell‚Äôosservazione dei dati,
- l‚Äôinformazione a priori e quella empirica si combinano sommando i rispettivi ‚Äúconteggi.‚Äù

Questa propriet√† rende la distribuzione Beta una scelta naturale nei modelli bayesiani con dati binomiali, come in contesti psicologici in cui si vogliono modellare incertezze sulla probabilit√† di una risposta corretta, sull‚Äôesito di una scelta, o sul successo di un comportamento.

### Visualizzazione della Distribuzione Beta

Di seguito mostriamo come la forma della distribuzione Beta varia al variare dei parametri $\alpha$ e $\beta$:

```{r}
# Parametri
x <- seq(0, 1, length.out = 200)
alphas <- c(0.5, 5.0, 1.0, 2.0, 2.0)
betas <- c(0.5, 1.0, 3.0, 2.0, 5.0)

# Creare un dataframe
df <- do.call(rbind, lapply(1:length(alphas), function(i) {
  data.frame(
    x = x,
    density = dbeta(x, alphas[i], betas[i]),
    label = paste0("Œ± = ", alphas[i], ", Œ≤ = ", betas[i])
  )
}))

# Plot
ggplot(df, aes(x = x, y = density, color = label)) +
  geom_line(size = 1) +
  labs(
    x = "x",
    y = "f(x)"
  ) +
  theme(legend.title = element_blank())
```

### Costante di Normalizzazione

La costante di normalizzazione della distribuzione Beta √® il reciproco della funzione Beta di Eulero, $B(\alpha, \beta)$. Questa garantisce che:

$$
\int_0^1 \mathcal{Beta}(\theta \mid \alpha, \beta) \, d\theta = 1.
$$

::: {#exm-}
Di seguito viene proposto un esempio in R per calcolare l'area sottesa alla distribuzione Beta non normalizzata e, con gli stessi parametri, ottenere il valore della funzione Beta di Eulero. L'obiettivo √® mostrare come la costante di normalizzazione, pari al reciproco di $B(\alpha, \beta)$, garantisca che l'integrale della densit√† normalizzata su $[0,1]$ sia pari a 1.

Supponiamo di voler utilizzare i parametri:

- $\alpha = 2$
- $\beta = 5$

```{r}
# Parametri della distribuzione Beta
alpha <- 2
beta  <- 5

# Definiamo la funzione non normalizzata della distribuzione Beta
unnormalized_beta <- function(theta) {
  theta^(alpha - 1) * (1 - theta)^(beta - 1)
}

# Calcoliamo l'integrale della funzione non normalizzata su [0, 1]
integrale <- integrate(unnormalized_beta, lower = 0, upper = 1)$value
cat("Integrale della funzione non normalizzata:", integrale, "\n")

# Calcoliamo il valore della funzione Beta usando la funzione beta() di R
valore_beta <- beta(alpha, beta)
cat("Valore della funzione Beta B(alpha, beta):", valore_beta, "\n")
```

Spiegazione del Codice

1. **Definizione dei Parametri e della Funzione**  
   Impostiamo $\alpha = 2$ e $\beta = 5$ e definiamo la funzione non normalizzata:
   $$
   f(\theta) = \theta^{\alpha-1}(1-\theta)^{\beta-1}.
   $$

2. **Calcolo dell'Integrale**  
   Utilizzando la funzione `integrate()`, calcoliamo l'area sottesa a $f(\theta)$ nell'intervallo $[0,1]$, che corrisponde a $\mathcal{B}(\alpha, \beta)$.

3. **Verifica con la Funzione Beta**  
   La funzione `beta(alpha, beta)` di R restituisce direttamente il valore di $\mathcal{B}(\alpha, \beta)$. La stampa dei due valori conferma che l'integrale calcolato e il valore della funzione Beta coincidono.

4. **Costante di Normalizzazione**  
   Il reciproco di $\mathcal{B}(\alpha, \beta)$ √® calcolato e utilizzato per definire la densit√† normalizzata della distribuzione Beta. L'integrazione della densit√† normalizzata su $[0,1]$ restituisce 1, confermando la corretta normalizzazione.

Questo esempio in R mostra in modo pratico come la costante di normalizzazione derivi dalla funzione Beta di Eulero e come essa venga applicata per ottenere una densit√† di probabilit√† correttamente normalizzata.
:::

In conclusione, la distribuzione Beta si rivela particolarmente utile per modellare variabili continue comprese nell'intervallo [0, 1]. Grazie alla sua parametrizzazione tramite $\alpha$ e $\beta$, consente di adattare la forma della densit√† in modo specifico alle caratteristiche osservate dei dati, facilitando la stima di proporzioni. Inoltre, essendo il coniugato della distribuzione binomiale, permette un aggiornamento analitico nei modelli bayesiani, semplificando l'inferenza quando si raccolgono dati incrementali, come nella stima della probabilit√† di successo in esperimenti o studi psicologici.

## Distribuzione di Cauchy

La **distribuzione di Cauchy** √® un caso speciale della distribuzione $t$ di Student con un solo grado di libert√† ($t_1$). Questa distribuzione √® caratterizzata da code molto pesanti e da una media e varianza non definite, rendendola particolarmente utile in contesti dove valori estremi possono avere un'influenza importante.

::: {#def-}
La funzione di densit√† di probabilit√† della distribuzione di Cauchy √® definita da due parametri:

- **$\alpha$**: posizione (location), che determina il centro della distribuzione.
- **$\beta > 0$**: scala (scale), che controlla la larghezza della distribuzione.

La densit√† √® data da:

$$
f(x \mid \alpha, \beta) = \frac{1}{\pi \beta \left[1 + \left( \frac{x - \alpha}{\beta} \right)^2\right]} ,
$$

dove:

- $x \in \mathbb{R}$,
- $\alpha \in \mathbb{R}$,
- $\beta > 0$.

Questa funzione descrive una distribuzione simmetrica attorno a $\alpha$, con code pi√π pesanti rispetto alla distribuzione Normale.
:::

### Propriet√† della Distribuzione di Cauchy

1. **Simmetria**: La distribuzione √® simmetrica rispetto a $\alpha$.
2. **Code Pesanti**: Le code sono significativamente pi√π pesanti rispetto alla distribuzione Normale, con una decrescita pi√π lenta ($\propto x^{-2}$).
3. **Media e Varianza**: La distribuzione non ha una media n√© una varianza definita.
4. **Relazione con $t_1$**: La distribuzione di Cauchy √® equivalente a una distribuzione $t$ di Student con 1 grado di libert√†.
5. **Caratteristiche Estreme**: I valori estremi hanno una probabilit√† pi√π alta rispetto ad altre distribuzioni comuni, rendendola utile per modellare fenomeni con outlier significativi.

### Visualizzazione della Distribuzione di Cauchy

Per comprendere l'effetto dei parametri $\alpha$ e $\beta$ sulla forma della distribuzione, consideriamo alcuni esempi con:

- $\alpha = 0.0, 0.0, 0.0, -2.0$,
- $\beta = 0.5, 1.0, 2.0, 1.0$.

```{r}
# Definire i parametri
x <- seq(-5, 5, length.out = 500)
alphas <- c(0.0, 0.0, 0.0, -2.0)
betas <- c(0.5, 1.0, 2.0, 1.0)

# Creare un data frame per i risultati
df <- do.call(rbind, lapply(1:length(alphas), function(i) {
  data.frame(
    x = x,
    density = dcauchy(x, location = alphas[i], scale = betas[i]),
    label = paste0("Œ± = ", alphas[i], ", Œ≤ = ", betas[i])
  )
}))

# Grafico
ggplot(df, aes(x = x, y = density, color = label)) +
  geom_line(size = 1) +
  labs(
    x = "x",
    y = "f(x)"
  ) +
  theme(
    legend.title = element_blank()
  )
```

### Applicazioni della Distribuzione di Cauchy

1. **Inferenza Bayesiana**:

   - Utilizzata come prior **robusto** in modelli bayesiani, particolarmente quando si vuole attribuire una probabilit√† maggiore a valori estremi rispetto a una distribuzione Normale.

2. **Modellazione di Fenomeni con Outlier**:

   - La distribuzione di Cauchy √® adatta per descrivere dati con valori estremi significativi che possono influenzare fortemente altre distribuzioni.

In conclusione, la distribuzione di Cauchy, con le sue propriet√† uniche come code pesanti e l'assenza di media e varianza definite, √® uno strumento fondamentale per modellare fenomeni in cui i valori estremi giocano un ruolo importante. La sua relazione con la distribuzione $t$ di Student e la sua utilit√† nei modelli bayesiani ne ampliano ulteriormente le applicazioni in contesti statistici e probabilistici avanzati.

## Distribuzione Gamma

La **distribuzione Gamma** √® una distribuzione di probabilit√† continua utilizzata principalmente per modellare variabili strettamente positive, come tassi, varianze, o tempi di attesa. √à usata nella statistica bayesiana come distribuzione a priori per parametri positivi e trova applicazione in ambiti come la modellazione di eventi rari.

::: {#def-}
La distribuzione Gamma √® caratterizzata da due parametri principali:

- **Parametro di forma** ($\alpha$): determina la forma generale della distribuzione.
- **Parametro di scala** ($\theta$) o, alternativamente, il **parametro di tasso** ($\beta = 1/\theta$): regola la larghezza o la dispersione della distribuzione.

La funzione di densit√† di probabilit√† (PDF) √® data da:

$$
f(x \mid \alpha, \theta) = \frac{x^{\alpha-1} e^{-x/\theta}}{\theta^\alpha \Gamma(\alpha)}, \quad x > 0,
$$

dove:

- $x$ √® la variabile casuale continua,
- $\Gamma(\alpha)$ √® la funzione Gamma di Eulero, definita come:

$$
\Gamma(\alpha) = \int_0^\infty t^{\alpha-1} e^{-t} dt.
$$

Se utilizziamo il parametro di tasso $\beta = 1/\theta$, la PDF pu√≤ essere scritta come:

$$
f(x \mid \alpha, \beta) = \frac{\beta^\alpha x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)}, \quad x > 0.
$$
:::

### Propriet√† della Distribuzione Gamma

1. **Media**:
   $$
   \mathbb{E}[X] = \alpha \cdot \theta = \frac{\alpha}{\beta}.
   $$

2. **Varianza**:
   $$
   \text{Var}(X) = \alpha \cdot \theta^2 = \frac{\alpha}{\beta^2}.
   $$

3. **Moda** (per $\alpha > 1$):
   $$
   \text{Moda}(X) = (\alpha - 1) \cdot \theta.
   $$


Di seguito, mostriamo un esempio per $\alpha = 3$ e $\beta = 5/3$, calcolando e rappresentando graficamente la distribuzione.

1. **Calcolo della Media e della Deviazione Standard**:
   ```{r}
   # Parametri
   alpha <- 3
   beta <- 5 / 3

   # Calcolo
   mean <- alpha / beta
   sigma <- sqrt(alpha / beta^2)

   cat("Media:", mean, "\n")
   cat("Deviazione Standard:", sigma, "\n")
   ```

2. **Generazione e Plot dei Dati**:
   ```{r}
   # Generazione di dati
   set.seed(123)
   data <- rgamma(100000, shape = alpha, rate = beta)

   # Data frame per ggplot
   df <- data.frame(values = data)

   # Plot
   ggplot(df, aes(x = values)) +
     geom_histogram(aes(y = ..density..), bins = 30, fill = "green", alpha = 0.6) +
     stat_function(fun = function(x) dgamma(x, shape = alpha, rate = beta),
                   color = "red", size = 1) +
     labs(
       x = "Valore",
       y = "Densit√† di probabilit√†"
     ) 
   ```

### Applicazioni della Distribuzione Gamma

1. **Modellazione del Tempo di Attesa**:  
   La distribuzione Gamma √® ideale per modellare tempi di attesa, ad esempio, il tempo necessario affinch√© si verifichino $n$ eventi in un processo di Poisson.

2. **Inferenza Bayesiana**:  
   - Utilizzata come prior per parametri positivi, come tassi ($\lambda$) o varianze ($\sigma^2$).
   - Ad esempio, nella modellazione bayesiana dei processi di Poisson, una distribuzione Gamma √® una scelta naturale per il prior su $\lambda$.


## Riflessioni conclusive

Le distribuzioni di probabilit√† costituiscono il cuore dell'inferenza statistica, sia bayesiana che frequentista. In questo capitolo, abbiamo esplorato come R offre un insieme completo di strumenti per lavorare con diverse distribuzioni, permettendo di modellare e analizzare una vasta gamma di fenomeni.

### Principali Applicazioni

1. **Inferenza Bayesiana**:  
   Le distribuzioni di probabilit√†, come la Beta, la Gamma e la Normale, sono essenziali per definire priors, calcolare posteriori e quantificare l'incertezza nei modelli bayesiani. Ad esempio:
   - La distribuzione Beta √® ideale per modellare credenze a priori su proporzioni o probabilit√†.
   - La distribuzione Gamma √® ampiamente usata per modellare parametri positivi come tassi o varianze.

2. **Analisi Statistica e Modellazione**:  
   Le distribuzioni, come la $t$ di Student, sono fondamentali per il confronto tra campioni, mentre la Normale √® indispensabile per modellare fenomeni che seguono la legge del limite centrale.

3. **Generazione e Simulazione di Dati**:  
   R permette di generare campioni casuali da distribuzioni comuni, utili per simulazioni, bootstrap e validazione di modelli.

### Funzionalit√† di R

Con poche funzioni, R consente di:

- **Generare campioni casuali**: con funzioni come `rnorm`, `rgamma`, `rbeta`, possiamo simulare dati da distribuzioni specifiche.
- **Calcolare densit√†**: ad esempio, con `dnorm`, `dgamma`, `dbeta`, possiamo visualizzare le funzioni di densit√†.
- **Calcolare probabilit√† cumulate**: con funzioni come `pnorm`, `pbeta`, possiamo determinare probabilit√† su intervalli specifici.
- **Determinare quantili**: con funzioni come `qnorm`, `qgamma`, possiamo calcolare i punti corrispondenti a specifici livelli di probabilit√†.

### Versatilit√† delle Distribuzioni

Le distribuzioni esplorate non solo descrivono fenomeni naturali, ma sono anche i "mattoncini" per costruire modelli statistici complessi. Le loro propriet√†, come la media, la varianza, la simmetria o le code pesanti, consentono di adattare il modello al fenomeno studiato.

In conclusione, il linguaggio R, con la sua flessibilit√† e ricchezza di strumenti, permette di padroneggiare le distribuzioni di probabilit√†, non solo come oggetti matematici, ma anche come strumenti pratici per rispondere a domande complesse. La comprensione e l'uso delle distribuzioni presentate costituiscono le fondamenta per avanzare verso tecniche pi√π sofisticate, come l'inferenza bayesiana avanzata o la modellazione gerarchica. 

## Esercizi {.unnumbered}

::: {.callout-important title="Problemi" collapse="true"}
Esercizi sulla distribuzione normale, risolvibili usando R, sono disponibili sulla seguente [pagina web](https://mathcenter.oxford.emory.edu/site/math117/probSetNormalDistribution/).
:::


## Informazioni sull'Ambiente di Sviluppo {.unnumbered}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}
