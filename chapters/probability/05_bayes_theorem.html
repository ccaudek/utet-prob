<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.34">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>6&nbsp; Il teorema di Bayes – Probabilità per la psicologia — Modulo di richiamo</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/probability/06_random_var.html" rel="next">
<link href="../../chapters/probability/04_conditional_prob.html" rel="prev">
<link href="../../style/gauss.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0a72236910a44089af39cd28873f322e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-9908c7b05874059c2106d454ac00f1d0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QT5S3P9D31"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QT5S3P9D31', { 'anonymize_ip': true});
</script><style>html{ scroll-behavior: smooth; }</style>
<script>
window.MathJax = {
  tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
  svg: { scale: 1, mtextInheritFont: true, fontCache: 'none', minScale: 1 },
  options: { renderActions: { addMenu: [0, '', ''] } },
  loader: { load: ['input/tex','output/svg'] }
};
</script><script>
window.MathJax = {
  tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
  svg: { scale: 1, mtextInheritFont: true, fontCache: 'none', minScale: 1 },
  options: { renderActions: { addMenu: [0, '', ''] } },
  loader: { load: ['input/tex','output/svg'] }
};
// Suggerimento CSS: vedi sezione 3 per gli spazi attorno a display math
</script><script>
window.MathJax = {
  tex: {
    packages: {'[+]': ['boldsymbol']},
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: { fontCache: 'global' }
};
</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="../../style/_typography-extras.css">
<link rel="stylesheet" href="../../style/_code-extras.css">
<link rel="stylesheet" href="../../style/_math-extras.css">
<link rel="stylesheet" href="../../style/styles.css">
</head>
<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/probability/introduction_probability.html">Probabilità</a></li><li class="breadcrumb-item"><a href="../../chapters/probability/05_bayes_theorem.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Probabilità per la psicologia — Modulo di richiamo</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/utet-prob/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Percorso e obiettivi</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Prefazione</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/probability/introduction_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probabilità</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/01_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Interpretazione della probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/02_probability_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Modelli probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/03_sigma-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Dal Discreto al Continuo: la <span class="math inline">\(\sigma\)</span>-algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/04_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/05_bayes_theorem.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/06_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/07_prob_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Distribuzioni di massa e di densità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/08_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Proprietà delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/09_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/10_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11_cov_cor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Covarianza e correlazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12_intro_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Introduzione alle distribuzioni di probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/13_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/14_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/15_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Assunzione di gaussianità e trasformazioni dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/16_likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Sommario</h2>
   
  <ul class="collapse">
<li><a href="#la-regola-di-bayes-e-linferenza-probabilistica" id="toc-la-regola-di-bayes-e-linferenza-probabilistica" class="nav-link active" data-scroll-target="#la-regola-di-bayes-e-linferenza-probabilistica"><span class="header-section-number">6.1</span> La regola di Bayes e l’inferenza probabilistica</a></li>
  <li><a href="#test-medici" id="toc-test-medici" class="nav-link" data-scroll-target="#test-medici"><span class="header-section-number">6.2</span> Test medici</a></li>
  <li><a href="#la-fallacia-del-procuratore" id="toc-la-fallacia-del-procuratore" class="nav-link" data-scroll-target="#la-fallacia-del-procuratore"><span class="header-section-number">6.3</span> La fallacia del procuratore</a></li>
  <li><a href="#la-probabilit%C3%A0-inversa" id="toc-la-probabilità-inversa" class="nav-link" data-scroll-target="#la-probabilit%C3%A0-inversa"><span class="header-section-number">6.4</span> La probabilità inversa</a></li>
  </ul><div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/ccaudek/utet-prob/blob/main/chapters/probability/05_bayes_theorem.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/utet-prob/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/probability/introduction_probability.html">Probabilità</a></li><li class="breadcrumb-item"><a href="../../chapters/probability/05_bayes_theorem.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-prob-bayes-theorem" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="epigraph">
<blockquote class="blockquote">
<p>“It is, without exaggeration, perhaps the most important single equation in history.”</p>
<p>– <strong>Tom Chivers</strong> (2024)</p>
</blockquote>
</div>
<section id="introduzione" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="introduzione">Introduzione</h2>
<p>Il teorema di Bayes costituisce un metodo matematico ottimale per risolvere problemi di <em>inferenza induttiva</em>, ovvero situazioni in cui si deducono cause sottostanti, principi generali o strutture complesse a partire da dati parziali e incerti. Trova applicazione in scenari disparati: dalla ricostruzione della percezione tridimensionale basata su segnali retinici all’interpretazione degli stati mentali altrui attraverso il comportamento osservabile, fino alla stima di parametri fisici in condizioni sperimentali rumorose <span class="citation" data-cites="ma2023bayesian baker2011bayesian">(<a href="#ref-baker2011bayesian" role="doc-biblioref">Baker et al., 2011</a>; <a href="#ref-ma2023bayesian" role="doc-biblioref">Ma et al., 2023</a>)</span>. La sua efficacia emerge soprattutto in contesti dove le evidenze disponibili non permettono di discriminare univocamente tra ipotesi concorrenti.</p>
<section id="panoramica-del-capitolo" class="level3 unnumbered unlisted"><h3 class="unnumbered unlisted anchored" data-anchor-id="panoramica-del-capitolo">Panoramica del capitolo</h3>
<ul>
<li>L’importanza dell teorema di Bayes.</li>
<li>l’utilizzo del teorema di Bayes per analizzare e interpretare i test diagnostici, tenendo in considerazione la prevalenza della malattia in questione.</li>
<li>Soluzione di problemi di probabilità discreta che necessitano dell’applicazione del teorema di Bayes.</li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prerequisiti
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Leggere <em>Everything is Predictable: How Bayesian Statistics Explain Our World</em> <span class="citation" data-cites="chivers2024everything">(<a href="#ref-chivers2024everything" role="doc-biblioref">Chivers, 2024</a>)</span>. Questo libro offre una descrizione chiara e accessibile dell’impatto che il teorema di Bayes ha avuto sulla vita moderna.</li>
<li>Leggere <a href="https://oecs.mit.edu/pub/lwxmte1p/release/2">Bayesian Models of Cognition</a> di Thomas L. Griffiths, una voce della <a href="https://oecs.mit.edu">Open Encyclopedia of Cognitive Science</a>.</li>
<li>Leggere il capitolo <em>Conditional Probability</em> <span class="citation" data-cites="schervish2014probability">(<a href="#ref-schervish2014probability" role="doc-biblioref">Schervish &amp; DeGroot, 2014</a>)</span>.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-caution no-icon callout-titled" title="Preparazione del Notebook">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Preparazione del Notebook
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">here</span><span class="fu">::</span><span class="fu"><a href="https://here.r-lib.org/reference/here.html">here</a></span><span class="op">(</span><span class="st">"code"</span>, <span class="st">"_common.R"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section><section id="incertezza-come-fondamento-dellinferenza" class="level3" data-number="6.0.1"><h3 data-number="6.0.1" class="anchored" data-anchor-id="incertezza-come-fondamento-dellinferenza">
<span class="header-section-number">6.0.1</span> Incertezza come fondamento dell’inferenza</h3>
<p>Un principio cardine del ragionamento bayesiano è il riconoscimento dell’<em>incertezza intrinseca</em> a qualsiasi processo conoscitivo. Anche in un universo deterministico, la complessità dei sistemi e i limiti dei nostri sensi rendono impossibile una conoscenza completa. Ad esempio, non possiamo determinare con esattezza infiniti dettagli (come posizione e stato di ogni neurone nel cervello di un interlocutore) né accedere direttamente a variabili latenti (come emozioni o intenzioni). Di conseguenza, ogni inferenza conserva un margine probabilistico, che Bayes quantifica e trasforma in uno strumento operativo.</p>
</section><section id="dinamica-bayesiana-aggiornare-le-credenze" class="level3" data-number="6.0.2"><h3 data-number="6.0.2" class="anchored" data-anchor-id="dinamica-bayesiana-aggiornare-le-credenze">
<span class="header-section-number">6.0.2</span> Dinamica bayesiana: aggiornare le credenze</h3>
<p>La realtà può essere paragonata a una partita di poker più che a una di scacchi: operiamo sempre in condizioni di <em>informazione imperfetta</em>. Le decisioni si basano su un bilanciamento tra conoscenze pregresse (<em>prior</em>) e nuovi indizi (<em>likelihood</em>), in un processo dinamico formalizzato dall’equazione:</p>
<p><span class="math display">\[
P(H \mid E) = \frac{P(E \mid H) \cdot P(H)}{P(E)} ,
\]</span><br>
dove:</p>
<ul>
<li>
<span class="math inline">\(P(H \mid E)\)</span> (<em>posterior</em>): plausibilità rivista dell’ipotesi <span class="math inline">\(H\)</span> dopo aver osservato l’evidenza <span class="math inline">\(E\)</span>;<br>
</li>
<li>
<span class="math inline">\(P(E \mid H)\)</span> (<em>likelihood</em>): probabilità di osservare <span class="math inline">\(E\)</span> se <span class="math inline">\(H\)</span> fosse vera;<br>
</li>
<li>
<span class="math inline">\(P(H)\)</span> (<em>prior</em>): fiducia iniziale in <span class="math inline">\(H\)</span>;<br>
</li>
<li>
<span class="math inline">\(P(E)\)</span>: fattore di normalizzazione.</li>
</ul>
<p>Questo meccanismo permette di ricalibrare razionalmente le convinzioni, riducendo l’incertezza man mano che nuovi dati vengono integrati.</p>
</section><section id="inferenza-induttiva-e-razionalità-adattiva" class="level3" data-number="6.0.3"><h3 data-number="6.0.3" class="anchored" data-anchor-id="inferenza-induttiva-e-razionalità-adattiva">
<span class="header-section-number">6.0.3</span> Inferenza induttiva e razionalità adattiva</h3>
<p>L’<em>inferenza induttiva bayesiana</em> rappresenta un pilastro della razionalità scientifica e quotidiana. A differenza della logica deduttiva (dove le conclusioni derivano necessariamente dalle premesse), Bayes riconcilia teoria ed evidenza empirica, consentendo previsioni robuste nonostante dati incompleti. Le applicazioni spaziano:</p>
<ul>
<li>in <em>psicologia cognitiva</em>, modellando come il cervello interpreta segnali ambigui <span class="citation" data-cites="domini20033 caudek2024fenomeni">(<a href="#ref-caudek2024fenomeni" role="doc-biblioref">Caudek &amp; Bruno, 2024</a>; <a href="#ref-domini20033" role="doc-biblioref">Domini &amp; Caudek, 2003</a>)</span>;<br>
</li>
<li>nell’<em>intelligenza artificiale</em>, guidando algoritmi di apprendimento automatico <span class="citation" data-cites="chivers2024everything">(<a href="#ref-chivers2024everything" role="doc-biblioref">Chivers, 2024</a>)</span>;<br>
</li>
<li>nelle <em>scienze sociali</em>, per stimare preferenze nascoste da comportamenti osservati.</li>
</ul>
<p>Il teorema non elimina l’incertezza, ma fornisce un protocollo formale per gestirla, trasformando l’induzione da atto intuitivo a procedura rigorosa. In questo senso, incarna un principio di <em>razionalità adattiva</em>, dove l’ottimalità non richiede onniscienza, bensì un aggiornamento coerente delle credenze in risposta all’esperienza.</p>
<!-- ## Il teorema di Bayes nella psicologia: il modello Rescorla-Wagner come esempio di adattamento all'ambiente {#sec-bayes-rescorla-wagner} -->
<!-- Per illustrare concretamente il ruolo del teorema di Bayes in psicologia, possiamo considerare un fenomeno molto comune nella vita quotidiana: l'apprendimento associativo. Questa capacità permette a esseri umani e altri organismi viventi di prevedere eventi futuri sulla base delle esperienze passate, adattando continuamente il proprio comportamento a un ambiente in costante mutamento. -->
<!-- Un modello psicologico particolarmente influente che formalizza questo processo è il modello Rescorla-Wagner. Nato inizialmente per spiegare come gli animali apprendano ad associare segnali ambientali a specifiche conseguenze, questo modello si è rivelato utile anche per comprendere l'apprendimento umano. Esso mostra chiaramente come le persone aggiornino le proprie aspettative ogni volta che si trovano davanti a nuove informazioni o situazioni inattese, evidenziando un processo continuo di adattamento alle condizioni mutevoli dell'ambiente circostante. -->
<!-- ### Il modello Rescorla-Wagner e l'adattamento all’ambiente   -->
<!-- Secondo il modello Rescorla-Wagner, quando ci troviamo di fronte a situazioni nuove o imprevedibili, sviluppiamo aspettative su ciò che potrebbe accadere. Se queste aspettative vengono disattese (ad esempio, ricevendo una ricompensa diversa da quella prevista), sperimentiamo quello che il modello definisce "errore di previsione". Questo errore, fondamentale per l’apprendimento, funge da meccanismo chiave per aggiornare rapidamente la nostra comprensione della realtà.   -->
<!-- Facciamo un esempio: immaginiamo un agente che, in un contesto in cui premere un pulsante genera solitamente una ricompensa (una moneta, una caramella, ecc.), si trovi improvvisamente a non riceverla più. L’errore di previsione generato da questa discrepanza modifica le aspettative dell’agente. Di conseguenza, nelle occasioni successive, le sue previsioni si adatteranno alla nuova realtà, riducendo la probabilità attesa della ricompensa.   -->
<!-- Il principale vantaggio del modello Rescorla-Wagner risiede nella sua essenzialità: basandosi su pochi principi fondamentali, riesce a descrivere in modo efficace come gli individui regolino le proprie aspettative in risposta a cambiamenti ambientali, garantendo un adattamento rapido e dinamico.   -->
<!-- ### Una prospettiva bayesiana sull'apprendimento -->
<!-- Anche se il modello di Rescorla-Wagner non nasce direttamente dal teorema di Bayes, può essere interpretato facilmente come un caso speciale di aggiornamento bayesiano. Il teorema di Bayes, infatti, descrive come dovremmo modificare razionalmente le nostre credenze alla luce di nuove evidenze. In psicologia, questa "evidenza" può essere pensata come la differenza tra ciò che ci aspettavamo e ciò che realmente accade—proprio come fa il modello Rescorla-Wagner. -->
<!-- Nella prospettiva bayesiana, le aspettative di una persona vengono considerate come "credenze" che vengono aggiornate costantemente sulla base delle nuove informazioni che emergono. Ogni volta che riceviamo un'informazione che contraddice le nostre aspettative iniziali, la nostra credenza viene rivista. Questo processo bayesiano permette di adattarsi in modo ottimale alle situazioni nuove o incerte, proprio come avviene con il modello Rescorla-Wagner (si veda l'esempio presentato nella @sec-bayes-coin-hypotheses). -->
<!-- In sintesi, il modello Rescorla-Wagner non solo rappresenta efficacemente come avviene l'apprendimento associativo nella vita quotidiana, ma fornisce anche un esempio pratico e intuitivo del ruolo del teorema di Bayes in psicologia: aiuta a capire come la nostra mente sia continuamente impegnata ad adattarsi razionalmente e rapidamente ai cambiamenti di un ambiente imprevedibile. -->
</section><section id="una-rivoluzione-nel-pensiero-probabilistico" class="level3" data-number="6.0.4"><h3 data-number="6.0.4" class="anchored" data-anchor-id="una-rivoluzione-nel-pensiero-probabilistico">
<span class="header-section-number">6.0.4</span> Una rivoluzione nel pensiero probabilistico</h3>
<p>Per comprendere appieno il teorema di Bayes, è necessario delineare le sue origini storiche. Nel XVIII secolo, <em>Thomas Bayes</em> (1701-1761), ecclesiastico presbiteriano e matematico britannico, pose le basi di una rivoluzione concettuale nel campo della probabilità e della statistica. Il suo contributo teorico, passato alla storia come <em>teorema di Bayes</em>, ha plasmato in modo decisivo lo sviluppo scientifico e tecnologico dei secoli successivi, influenzando discipline che spaziano dalla medicina all’intelligenza artificiale <span class="citation" data-cites="chivers2024everything">(<a href="#ref-chivers2024everything" role="doc-biblioref">Chivers, 2024</a>)</span>.</p>
<section id="la-figura-di-thomas-bayes" class="level4" data-number="6.0.4.1"><h4 data-number="6.0.4.1" class="anchored" data-anchor-id="la-figura-di-thomas-bayes">
<span class="header-section-number">6.0.4.1</span> La figura di Thomas Bayes</h4>
<p>Bayes proveniva da una famiglia benestante e studiò teologia a Edimburgo, preparandosi al ministero religioso. Come ricorda il biografo David Bellhouse, Bayes non era un accademico nel senso moderno del termine, ma un erudito libero, interessato alla conoscenza per passione personale <span class="citation" data-cites="bellhouse2004">(<a href="#ref-bellhouse2004" role="doc-biblioref">Bellhouse, 2004</a>)</span>.</p>
<p>Durante la sua vita, Bayes pubblicò due testi:</p>
<ol type="1">
<li>
<em>Un trattato di teologia</em>: <em>Divine Benevolence: Or, an Attempt to Prove that the Principal End of the Divine Providence and Government is the Happiness of His Creatures</em> (1731), una teodicea che cerca di spiegare come la legge naturale possa ottimizzare il benessere universale.<br>
</li>
<li>
<em>Una difesa del calcolo infinitesimale</em>: <em>An Introduction to the Doctrine of Fluxions</em> (1736), in risposta alle critiche di George Berkeley sugli infinitesimi e i concetti fondamentali del calcolo newtoniano <span class="citation" data-cites="jesseph1993berkeley">(<a href="#ref-jesseph1993berkeley" role="doc-biblioref">Jesseph, 1993</a>)</span>.</li>
</ol>
<p>Il lavoro che segnò la svolta nella teoria della probabilità fu però pubblicato postumo, nel 1763, sulle <em>Philosophical Transactions of the Royal Society</em>: <em>An Essay towards Solving a Problem in the Doctrine of Chances</em>. Per la prima volta, si formalizzava un metodo per aggiornare le ipotesi probabilistiche alla luce di nuove evidenze, ponendo le fondamenta dell’inferenza bayesiana <span class="citation" data-cites="stigler1990history">(<a href="#ref-stigler1990history" role="doc-biblioref">Stigler, 1990</a>)</span>.</p>
</section><section id="bayes-e-il-ruolo-culturale-della-scienza" class="level4" data-number="6.0.4.2"><h4 data-number="6.0.4.2" class="anchored" data-anchor-id="bayes-e-il-ruolo-culturale-della-scienza">
<span class="header-section-number">6.0.4.2</span> Bayes e il ruolo culturale della scienza</h4>
<p>Come sottolinea ancora Bellhouse, nel XVIII secolo era comune, tra le élite colte, dedicarsi allo studio di discipline scientifiche per prestigio sociale. Per Bayes, la matematica era dunque una passione coltivata con spirito libero. Il suo merito straordinario fu di spingere l’interpretazione della probabilità verso una prospettiva <em>epistemologica</em> innovativa, dove la probabilità diventa espressione quantitativa della nostra ignoranza sul mondo.</p>
<p>In contrapposizione alla visione “classica”, che vedeva la probabilità come frequenza osservabile in eventi ripetuti, Bayes propose che essa potesse rappresentare il <em>grado di fiducia</em> di un osservatore, inevitabilmente influenzato da conoscenze pregresse e da pregiudizi individuali. In questo senso, la probabilità assume un carattere dinamico e soggettivo, configurandosi come uno <em>strumento di conoscenza</em> che si aggiorna di continuo al variare dei dati <span class="citation" data-cites="spiegelhalter2019art">(<a href="#ref-spiegelhalter2019art" role="doc-biblioref">Spiegelhalter, 2019</a>)</span>.</p>
</section><section id="un-esperimento-mentale-illuminante" class="level4" data-number="6.0.4.3"><h4 data-number="6.0.4.3" class="anchored" data-anchor-id="un-esperimento-mentale-illuminante">
<span class="header-section-number">6.0.4.3</span> Un esperimento mentale illuminante</h4>
<p>Per illustrare la sua idea, Bayes propose un semplice esempio: immagina di lanciare alcune palline su un tavolo da biliardo. Dopo aver segnato con una linea il punto in cui si ferma una pallina bianca (e averla poi rimossa), si lanciano altre palline rosse e si conta quante cadono a destra e quante a sinistra di quella linea. Sulla base di queste osservazioni, come si può “indovinare” la posizione della linea? E con quale probabilità la prossima pallina rossa cadrà a sinistra di essa?</p>
<p>La soluzione di Bayes combina i <em>dati osservati</em> (numero di palline cadute a sinistra o a destra) con le <em>convinzioni iniziali</em> dell’osservatore (il cosiddetto “prior”), delineando un processo di apprendimento graduale che guida la revisione critica delle ipotesi.</p>
</section><section id="il-ruolo-di-richard-price" class="level4" data-number="6.0.4.4"><h4 data-number="6.0.4.4" class="anchored" data-anchor-id="il-ruolo-di-richard-price">
<span class="header-section-number">6.0.4.4</span> Il ruolo di Richard Price</h4>
<p>Dopo la morte di Bayes, fu un altro ecclesiastico, <em>Richard Price (1723-1791)</em>, a dare impulso alla diffusione del saggio bayesiano. Price aveva un’ottima reputazione negli ambienti intellettuali dell’epoca, grazie anche alle sue relazioni con figure di spicco come <em>Benjamin Franklin</em>, <em>Thomas Jefferson</em> e <em>John Adams</em>.</p>
<p>Price prese in carico il manoscritto di Bayes, lo sottopose al fisico <em>John Canton</em> e ne curò la pubblicazione postuma, operando modifiche significative. Rispetto alla versione originale di Bayes, concentrata quasi esclusivamente sugli aspetti teorici, Price aggiunse una parte dedicata alle applicazioni pratiche, rendendo il testo più fruibile a un pubblico più ampio. Per questo motivo, lo storico Stephen Stigler lo definisce «il primo bayesiano della storia».</p>
</section><section id="dal-silenzio-alla-riscoperta" class="level4" data-number="6.0.4.5"><h4 data-number="6.0.4.5" class="anchored" data-anchor-id="dal-silenzio-alla-riscoperta">
<span class="header-section-number">6.0.4.5</span> Dal silenzio alla riscoperta</h4>
<p>Per oltre cinquant’anni, il lavoro di Bayes rimase in ombra, oscurato dall’opera pionieristica di <em>Pierre-Simon Laplace</em>. Già nel 1774, Laplace pervenne indipendentemente a principi analoghi, e successivamente li sistematizzò nella monumentale <em>Théorie analytique des probabilités</em> (1812). Solo in tempi più recenti, con l’avvento dei metodi di calcolo moderno e dell’informatica, la statura del teorema di Bayes è emersa in tutta la sua importanza.</p>
<p>Oggi, il teorema di Bayes è considerato un cardine della statistica moderna: formalizza il modo in cui aggiorniamo le nostre credenze alla luce di nuovi dati. Questo schema è cruciale in ogni disciplina scientifica e tecnologica che debba fare i conti con incertezza e dati incompleti. Dalla genomica all’econometria, dalla fisica delle particelle alle scienze cognitive, il paradigma bayesiano risulta prezioso per gestire e interpretare informazioni in continuo aggiornamento.</p>
</section><section id="leredità-di-bayes-nellera-digitale" class="level4" data-number="6.0.4.6"><h4 data-number="6.0.4.6" class="anchored" data-anchor-id="leredità-di-bayes-nellera-digitale">
<span class="header-section-number">6.0.4.6</span> L’eredità di Bayes nell’era digitale</h4>
<p>Nell’intelligenza artificiale, le idee bayesiane sono alla base di sistemi di apprendimento automatico e modelli probabilistici complessi. Strumenti come i moderni modelli linguistici (ad esempio ChatGPT e Claude) sfruttano strategie di inferenza bayesiana – anche se in forme estremamente avanzate – per generare risposte, fare previsioni e adattarsi costantemente agli input degli utenti.</p>
<p>La parabola storica di questo teorema, nato dalle speculazioni di un pastore presbiteriano del Settecento, mostra chiaramente il potenziale trasformativo delle idee matematiche. Come sottolinea Tom Chivers nel suo <em>Everything Is Predictable: How Bayesian Statistics Explain Our World</em>, la statistica bayesiana è diventata una sorta di <em>“grammatica universale”</em> per interpretare la realtà, permettendoci di affrontare con metodo situazioni complesse, modellare l’incertezza e formulare previsioni in contesti dove l’informazione è inevitabilmente limitata <span class="citation" data-cites="chivers2024everything">(<a href="#ref-chivers2024everything" role="doc-biblioref">Chivers, 2024</a>)</span>.</p>
<p>In sintesi, la forza del teorema di Bayes non risiede soltanto nella sua eleganza formale, ma soprattutto nella sua <em>portata epistemologica</em>: esso traduce in termini matematici la nostra naturale tendenza ad apprendere da ciò che osserviamo e a rivedere continuamente ciò che crediamo. Per questo rimane, ancora oggi, un punto di riferimento fondamentale in qualunque disciplina che affronti il problema della conoscenza in condizioni di incertezza.</p>
</section></section></section><section id="la-regola-di-bayes-e-linferenza-probabilistica" class="level2" data-number="6.1"><h2 data-number="6.1" class="anchored" data-anchor-id="la-regola-di-bayes-e-linferenza-probabilistica">
<span class="header-section-number">6.1</span> La regola di Bayes e l’inferenza probabilistica</h2>
<p>L’inferenza bayesiana utilizza un principio centrale della teoria delle probabilità noto come <em>regola di Bayes</em>. Questo principio consente di aggiornare in modo razionale le nostre credenze sulla base di nuovi dati osservati, integrandoli con conoscenze pregresse.</p>
<section id="derivazione-della-regola-di-bayes" class="level3" data-number="6.1.1"><h3 data-number="6.1.1" class="anchored" data-anchor-id="derivazione-della-regola-di-bayes">
<span class="header-section-number">6.1.1</span> Derivazione della regola di Bayes</h3>
<p>Consideriamo due eventi casuali, <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span>. La probabilità congiunta <span class="math inline">\(P(A, B)\)</span>, ossia la probabilità che entrambi gli eventi accadano simultaneamente, può essere espressa in due modi equivalenti:</p>
<ol type="1">
<li><p>Tramite la <em>regola della catena</em>, possiamo scrivere: <span class="math display">\[
P(A, B) = P(A \mid B)P(B).
\]</span> Qui, <span class="math inline">\(P(A \mid B)\)</span> è la probabilità condizionata che si verifichi l’evento <span class="math inline">\(A\)</span> sapendo che l’evento <span class="math inline">\(B\)</span> è avvenuto, mentre <span class="math inline">\(P(B)\)</span> è la probabilità marginale di <span class="math inline">\(B\)</span>, indipendente da <span class="math inline">\(A\)</span>.</p></li>
<li><p>Utilizzando la simmetria della probabilità congiunta, possiamo invertire gli eventi: <span class="math display">\[
P(A, B) = P(B \mid A)P(A).
\]</span></p></li>
</ol>
<p>Dato che entrambe le espressioni rappresentano la stessa probabilità congiunta, possiamo eguagliarle:</p>
<p><span class="math display">\[
P(A \mid B)P(B) = P(B \mid A)P(A).
\]</span></p>
<p>Risolvendo per <span class="math inline">\(P(B \mid A)\)</span> otteniamo la <em>regola di Bayes</em>:</p>
<p><span id="eq-bayes-def1"><span class="math display">\[
P(B \mid A) = \frac{P(A \mid B) P(B)}{P(A)}.
\tag{6.1}\]</span></span></p>
</section><section id="interpretazione-dei-termini-della-regola-di-bayes" class="level3" data-number="6.1.2"><h3 data-number="6.1.2" class="anchored" data-anchor-id="interpretazione-dei-termini-della-regola-di-bayes">
<span class="header-section-number">6.1.2</span> Interpretazione dei termini della regola di Bayes</h3>
<p>La regola di Bayes permette di aggiornare la nostra credenza sulla probabilità di un’ipotesi o evento (<span class="math inline">\(B\)</span>), dopo aver osservato un dato o evidenza (<span class="math inline">\(A\)</span>):</p>
<ul>
<li>
<em><span class="math inline">\(P(B)\)</span> (prior)</em>: è la probabilità iniziale assegnata all’evento <span class="math inline">\(B\)</span> prima di osservare il dato <span class="math inline">\(A\)</span>. Rappresenta la nostra conoscenza pregressa o il nostro grado iniziale di fiducia.</li>
<li>
<em><span class="math inline">\(P(A \mid B)\)</span> (verosimiglianza)</em>: è la probabilità di osservare il dato <span class="math inline">\(A\)</span> nell’ipotesi che <span class="math inline">\(B\)</span> sia vero. Indica quanto il dato sia compatibile con l’ipotesi.</li>
<li>
<em><span class="math inline">\(P(B \mid A)\)</span> (posterior)</em>: è la probabilità aggiornata, cioè la nostra nuova credenza sull’evento <span class="math inline">\(B\)</span> dopo aver osservato il dato <span class="math inline">\(A\)</span>.</li>
<li>
<em><span class="math inline">\(P(A)\)</span> (evidenza)</em>: è la probabilità marginale del dato osservato, calcolata sommando o integrando su tutte le possibili ipotesi alternative che potrebbero aver generato tale dato. Agisce da termine di normalizzazione per garantire che la somma delle probabilità a posteriori sia uguale a 1.</li>
</ul></section><section id="applicazioni-della-regola-di-bayes" class="level3" data-number="6.1.3"><h3 data-number="6.1.3" class="anchored" data-anchor-id="applicazioni-della-regola-di-bayes">
<span class="header-section-number">6.1.3</span> Applicazioni della regola di Bayes</h3>
<p>Nella pratica, l’inferenza bayesiana si svolge tipicamente nel seguente modo:</p>
<ol type="1">
<li>Si parte da uno <em>spazio delle ipotesi</em> <span class="math inline">\(\mathcal{H}\)</span>, ovvero un insieme di tutte le possibili spiegazioni o modelli che potrebbero aver generato i dati osservati <span class="math inline">\(D\)</span>.</li>
<li>A ciascuna ipotesi <span class="math inline">\(H \in \mathcal{H}\)</span> viene assegnata una <em>probabilità a priori</em> <span class="math inline">\(P(H)\)</span> che riflette la nostra fiducia iniziale.</li>
<li>Una volta raccolti i dati <span class="math inline">\(D\)</span>, aggiorniamo le probabilità delle ipotesi usando la formula:</li>
</ol>
<p><span id="eq-bayes-def2"><span class="math display">\[
P(H \mid D) = \frac{P(D \mid H) \, P(H)}{P(D)},
\tag{6.2}\]</span></span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(P(D \mid H)\)</span> è la verosimiglianza, cioè la probabilità che l’ipotesi <span class="math inline">\(H\)</span> abbia generato i dati <span class="math inline">\(D\)</span>;</li>
<li>
<span class="math inline">\(P(D)\)</span> è la probabilità marginale (evidenza), calcolata considerando tutte le possibili ipotesi:</li>
</ul>
<p><span class="math display">\[
P(D) = \sum_{H' \in \mathcal{H}} P(D \mid H')P(H'),
\]</span></p>
<p>nel caso discreto, oppure:</p>
<p><span class="math display">\[
P(D) = \int_{\mathcal{H}} P(D \mid H')P(H') \, dH',
\]</span></p>
<p>nel caso continuo.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Immagina di avere ricevuto un messaggio anonimo sul tuo cellulare con scritto solo “Ci vediamo stasera!”. Vuoi capire chi può essere stato a mandartelo. In questo esempio, il tuo “spazio delle ipotesi” sarà rappresentato da tre persone possibili: Alice, Bruno e Carla.</p>
<p>Quindi, hai un insieme di ipotesi molto semplice:</p>
<p><span class="math display">\[
\mathcal{H} = \{\text{Alice},\, \text{Bruno},\, \text{Carla}\} .
\]</span></p>
<p><em>1. Probabilità a priori (prima di guardare i dati).</em></p>
<p>Supponi che ciascuna persona abbia una probabilità diversa di scriverti:</p>
<table class="caption-top table">
<thead><tr class="header">
<th>Ipotesi</th>
<th><span class="math inline">\(P(H)\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Alice</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>Bruno</td>
<td>0.3</td>
</tr>
<tr class="odd">
<td>Carla</td>
<td>0.2</td>
</tr>
</tbody>
</table>
<p>Queste sono le tue probabilità <em>a priori</em>, basate sulla tua esperienza o conoscenza passata (ad esempio, Alice tende a scriverti spesso, Carla raramente).</p>
<p><em>2. Come le ipotesi generano i dati (informazioni aggiuntive).</em></p>
<p>Ora raccogli alcune informazioni utili (i tuoi dati <span class="math inline">\(D\)</span>):</p>
<ul>
<li>Il messaggio dice “Ci vediamo stasera!”.</li>
</ul>
<p>Rifletti sul fatto che ciascuna delle tre persone usa questa frase con frequenze diverse (sai, ad esempio, che Alice usa spesso frasi brevi come questa, mentre Bruno e Carla la usano meno spesso, ovvero tendono a scrivere messaggi più lunghi):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 85%">
</colgroup>
<thead><tr class="header">
<th>Ipotesi</th>
<th>Probabilità di inviare questa specifica frase (<span class="math inline">\(P(D \mid H)\)</span>)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Alice</td>
<td>0.7</td>
</tr>
<tr class="even">
<td>Bruno</td>
<td>0.4</td>
</tr>
<tr class="odd">
<td>Carla</td>
<td>0.1</td>
</tr>
</tbody>
</table>
<p>Queste probabilità rappresentano il <em>“meccanismo generatore dei dati”</em>, ovvero come ciascuna persona (ipotesi) potrebbe generare proprio il messaggio che hai ricevuto.</p>
<p><em>3. Aggiornamento delle probabilità a posteriori (dopo aver osservato il messaggio).</em></p>
<p>Ora applichiamo la formula di Bayes per aggiornare la nostra fiducia iniziale:</p>
<p><span class="math display">\[
P(H \mid D) = \frac{P(D \mid H) \, P(H)}{P(D)} .
\]</span></p>
<p>Prima calcoliamo la probabilità totale di ricevere quello specifico messaggio, indipendentemente da chi l’ha inviato. Usiamo il teorema della probabilità totale:</p>
<p><span class="math display">\[
P(D) = P(D\mid\text{Alice})P(\text{Alice}) + P(D\mid\text{Bruno})P(\text{Bruno}) + P(D\mid\text{Carla})P(\text{Carla}) .
\]</span></p>
<p>Cioè:</p>
<p><span class="math display">\[
P(D) = (0.7 \times 0.5) + (0.4 \times 0.3) + (0.1 \times 0.2)
= 0.35 + 0.12 + 0.02
= 0.49 .
\]</span></p>
<p>Ora aggiorniamo ciascuna ipotesi:</p>
<ul>
<li><em>Alice:</em></li>
</ul>
<p><span class="math display">\[
P(\text{Alice}\mid D) = \frac{0.7\times0.5}{0.49} = \frac{0.35}{0.49} \approx 0.714 .
\]</span></p>
<ul>
<li><em>Bruno:</em></li>
</ul>
<p><span class="math display">\[
P(\text{Bruno}\mid D) = \frac{0.4\times0.3}{0.49} = \frac{0.12}{0.49} \approx 0.245 .
\]</span></p>
<ul>
<li><em>Carla:</em></li>
</ul>
<p><span class="math display">\[
P(\text{Carla}\mid D) = \frac{0.1\times0.2}{0.49} = \frac{0.02}{0.49} \approx 0.041 .
\]</span></p>
<p><em>4. Interpretazione finale (intuizione bayesiana).</em></p>
<p>Dopo aver osservato il messaggio (“dati”), la tua fiducia si è aggiornata rispetto alle probabilità iniziali:</p>
<table class="caption-top table">
<thead><tr class="header">
<th>Ipotesi</th>
<th>Probabilità a priori</th>
<th>Probabilità a posteriori</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Alice</td>
<td>0.5</td>
<td><em>0.714</em></td>
</tr>
<tr class="even">
<td>Bruno</td>
<td>0.3</td>
<td><em>0.245</em></td>
</tr>
<tr class="odd">
<td>Carla</td>
<td>0.2</td>
<td><em>0.041</em></td>
</tr>
</tbody>
</table>
<p>Ora credi molto più fortemente che sia stata Alice a scriverti.</p>
<p>In sintesi, in questo esempio semplice, lo <em>spazio delle ipotesi</em> era costituito da tre persone possibili. Ciascuna ipotesi poteva “generare” (cioè produrre o inviare) lo specifico messaggio che hai ricevuto con una diversa probabilità (“meccanismo generatore dei dati”). Prima dei dati avevi delle credenze su chi poteva averti scritto (“probabilità a priori”), poi lo specifico messaggio osservato (“i dati”) ha modificato le tue convinzioni (“probabilità a posteriori”), secondo la logica della Regola di Bayes.</p>
<p>Questo esempio chiarisce intuitivamente il significato di:</p>
<ul>
<li>
<em>spazio delle ipotesi</em> (le possibili spiegazioni);</li>
<li>
<em>meccanismo generatore dei dati</em> (la probabilità con cui ciascuna ipotesi produce il dato osservato);</li>
<li>
<em>aggiornamento bayesiano</em> (come cambia la fiducia nelle ipotesi dopo aver visto i dati).</li>
</ul>
</div>
</div>
</div>
</section><section id="il-processo-iterativo-dellaggiornamento-bayesiano" class="level3" data-number="6.1.4"><h3 data-number="6.1.4" class="anchored" data-anchor-id="il-processo-iterativo-dellaggiornamento-bayesiano">
<span class="header-section-number">6.1.4</span> Il processo iterativo dell’aggiornamento bayesiano</h3>
<p>L’inferenza bayesiana è intrinsecamente iterativa. Ogni volta che emergono nuovi dati, la distribuzione a posteriori <span class="math inline">\(P(H \mid D)\)</span> ottenuta diventa il nuovo prior per aggiornamenti successivi. Questo permette un affinamento continuo delle credenze, adattando la nostra comprensione del mondo in modo dinamico e coerente con le nuove evidenze.</p>
<section id="considerazioni-pratiche" class="level4" data-number="6.1.4.1"><h4 data-number="6.1.4.1" class="anchored" data-anchor-id="considerazioni-pratiche">
<span class="header-section-number">6.1.4.1</span> Considerazioni pratiche</h4>
<p>Spesso, il calcolo diretto della <em>probabilità marginale</em> <span class="math inline">\(P(D)\)</span> — corrispondente, nell’esempio precedente, alla probabilità di osservare lo specifico messaggio ricevuto sul dispositivo mobile — risulta computazionalmente oneroso, in particolare quando lo spazio delle ipotesi è discreto o continuo di alta dimensionalità. Per ovviare a questa limitazione, vengono impiegati <em>metodi numerici approssimativi</em> come il <em>Campionamento Monte Carlo</em> o le <em>inferenze variazionali</em>, tecniche che permettono di stimare in modo efficiente tali grandezze probabilistiche anche in scenari reali complessi, senza ricorrere a calcoli analitici esatti.</p>
<p>In sintesi, la regola di Bayes fornisce uno schema formale e razionale per integrare informazioni pregresse con nuove osservazioni. Questa capacità di aggiornare continuamente le nostre credenze rappresenta il cuore del ragionamento probabilistico e rende l’approccio bayesiano uno strumento fondamentale in molte discipline scientifiche e applicazioni pratiche.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Immaginiamo questo scenario: sospettiamo che una moneta possa essere truccata e vogliamo verificarlo attraverso due lanci. Utilizzeremo il ragionamento bayesiano per combinare le nostre convinzioni iniziali con i dati osservati.</p>
<p><em>Le Due Ipotesi.</em></p>
<p>Supponiamo che la moneta possa essere:</p>
<ul>
<li>
<em>bilanciata</em> (pari probabilità di Testa e Croce: 50% ciascuna);</li>
<li>
<em>truccata</em> (sbilanciata, con probabilità di Testa del 80% e Croce del 20%).</li>
</ul>
<p>Il nostro obiettivo è capire quale ipotesi sia più plausibile dopo ogni lancio.</p>
<p><em>Fase 1: credenze iniziali (prior).</em></p>
<p>Prima di lanciare la moneta, abbiamo una certa idea di quanto sia probabile ciascuna ipotesi:</p>
<p><span class="math display">\[
P(\text{Bilanciata}) = 0.85 \quad\text{e}\quad P(\text{Truccata}) = 0.15.
\]</span></p>
<p>Queste probabilità rappresentano il <em>prior</em>, ovvero le nostre convinzioni iniziali prima di osservare qualunque risultato.</p>
<p><em>Fase 2: primo lancio - esce Testa.</em></p>
<p>Lanciamo la moneta una volta e osserviamo il risultato: esce <em>Testa</em>.</p>
<p>Ci chiediamo: “Quanto è probabile osservare Testa se ciascuna delle due ipotesi fosse vera?”</p>
<ul>
<li>Se la moneta è bilanciata, la probabilità di osservare Testa è 0.5 (50%).</li>
<li>Se la moneta è truccata, la probabilità di osservare Testa è 0.8 (80%).</li>
</ul>
<p>Queste due probabilità rappresentano la <em>verosimiglianza</em>:</p>
<p><span class="math display">\[
P(\text{Testa} \mid \text{Bilanciata}) = 0.5 \quad\text{e}\quad P(\text{Testa} \mid \text{Truccata}) = 0.8.
\]</span></p>
<p><em>Evidenza: Probabilità Complessiva dell’Evento Osservato.</em></p>
<p>Vogliamo ora sapere quanto sia probabile osservare Testa in generale, considerando entrambe le ipotesi possibili. Per calcolarlo, usiamo la <em>probabilità totale</em>, che tiene conto di tutte le possibili ipotesi:</p>
<p><span class="math display">\[
P(\text{Testa}) = P(\text{Testa} \mid \text{Bilanciata}) \times P(\text{Bilanciata}) + P(\text{Testa} \mid \text{Truccata}) \times P(\text{Truccata}).
\]</span></p>
<p>Sostituiamo i valori numerici:</p>
<p><span class="math display">\[
P(\text{Testa}) = (0.5 \times 0.85) + (0.8 \times 0.15) = 0.425 + 0.12 = 0.545.
\]</span></p>
<p>Questa è la <em>probabilità marginale</em> o <em>evidenza</em> del risultato osservato.</p>
<p><em>Posterior: Aggiornamento delle Credenze dopo l’Evidenza.</em></p>
<p>Ora possiamo usare il Teorema di Bayes per aggiornare le nostre credenze iniziali alla luce dell’evento osservato (Testa):</p>
<p><span class="math display">\[
\begin{aligned}
P(\text{Bilanciata} \mid \text{Testa}) &amp;= \frac{P(\text{Testa} \mid \text{Bilanciata}) \times P(\text{Bilanciata})}{P(\text{Testa})}\notag\\
&amp;= \frac{0.5 \times 0.85}{0.545} \notag\\
&amp;= 0.7798 \quad (77.98\%).
\end{aligned} \notag
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
P(\text{Truccata} \mid \text{Testa}) &amp;= \frac{P(\text{Testa} \mid \text{Truccata}) \times P(\text{Truccata})}{P(\text{Testa})} \notag\\
&amp;= \frac{0.8 \times 0.15}{0.545} \notag\\
&amp;= 0.2202 \quad (22.02\%). \notag
\end{aligned}
\]</span></p>
<p><em>Interpretazione Intuitiva.</em></p>
<p>Prima del lancio, eravamo abbastanza sicuri (85%) che la moneta fosse bilanciata. Dopo aver osservato un singolo lancio che mostra Testa, questa certezza diminuisce (passa a circa 77.98%), mentre la probabilità che la moneta sia truccata aumenta (passa da 15% a circa 22.02%).</p>
<p>Questo esempio mostra come il <em>prior</em>, la <em>verosimiglianza</em> e l’<em>evidenza</em> si combinino nel ragionamento bayesiano per produrre un aggiornamento razionale e coerente delle credenze.</p>
<p><em>Fase 3: secondo Lancio - esce Testa.</em></p>
<p>Supponiamo ora di lanciare la moneta una seconda volta, osservando ancora <em>Testa</em>. Usiamo le nuove probabilità ottenute (posterior) come prior aggiornati:</p>
<p><span class="math display">\[
P(\text{Bilanciata}) = 0.7798 \quad\text{e}\quad P(\text{Truccata}) = 0.2202.
\]</span></p>
<p>Calcoliamo nuovamente l’evidenza:</p>
<p><span class="math display">\[
P(\text{Testa}) = (0.5 \times 0.7798) + (0.8 \times 0.2202) = 0.3899 + 0.1762 = 0.5661.
\]</span></p>
<p>Aggiorniamo quindi le credenze con il teorema di Bayes:</p>
<p><span class="math display">\[
P(\text{Bilanciata} \mid \text{Testa}) = \frac{0.5 \times 0.7798}{0.5661} = 0.6887 \quad (68.87\%).
\]</span></p>
<p><span class="math display">\[
P(\text{Truccata} \mid \text{Testa}) = \frac{0.8 \times 0.2202}{0.5661} = 0.3113 \quad (31.13\%).
\]</span></p>
<p><em>Interpretazione del Secondo Aggiornamento.</em></p>
<p>Dopo il secondo lancio che mostra ancora Testa, la probabilità che la moneta sia bilanciata scende ulteriormente da 0.7798 a 0.6887, mentre la probabilità che la moneta sia truccata sale a 0.3113. Questo esempio mostra come l’<em>aggiornamento bayesiano</em> consenta di modificare progressivamente le nostre credenze, adattandole coerentemente a ogni nuova evidenza osservata.</p>
</div>
</div>
</div>
</section></section><section id="applicazioni-in-psicologia" class="level3" data-number="6.1.5"><h3 data-number="6.1.5" class="anchored" data-anchor-id="applicazioni-in-psicologia">
<span class="header-section-number">6.1.5</span> Applicazioni in psicologia</h3>
<p>Negli ultimi anni, i <em>modelli bayesiani</em> hanno acquisito un ruolo centrale nello studio della cognizione umana, fornendo una struttura formale per comprendere come il cervello costruisca rappresentazioni del mondo e prenda decisioni sulla base di dati incerti. Come discusso da <span class="citation" data-cites="griffiths2024bayesian">Griffiths et al. (<a href="#ref-griffiths2024bayesian" role="doc-biblioref">2024</a>)</span>, questi modelli sono stati applicati a una vasta gamma di processi cognitivi, tra cui:</p>
<ul>
<li>
<em>Apprendimento e generalizzazione</em>: i modelli bayesiani descrivono come gli individui apprendano nuove categorie e concetti sulla base di dati limitati e rumorosi (Tenenbaum, Griffiths, &amp; Kemp, 2006).</li>
<li>
<em>Percezione e interpretazione sensoriale</em>: la percezione visiva e il riconoscimento di oggetti possono essere spiegati come un’inferenza bayesiana sulla base di segnali sensoriali ambigui <span class="citation" data-cites="yuille2006vision domini20033">(<a href="#ref-domini20033" role="doc-biblioref">Domini &amp; Caudek, 2003</a>; <a href="#ref-yuille2006vision" role="doc-biblioref">Yuille &amp; Kersten, 2006</a>)</span>.</li>
<li>
<em>Controllo motorio</em>: il sistema motorio umano sembra ottimizzare i movimenti attraverso una combinazione di modelli interni e aggiornamenti bayesiani (Kording &amp; Wolpert, 2006).</li>
<li>
<em>Memoria e recupero delle informazioni</em>: i processi mnemonici, come il richiamo della memoria semantica, possono essere modellati come inferenze bayesiane basate su conoscenze pregresse (Steyvers, Griffiths, &amp; Dennis, 2006).</li>
<li>
<em>Acquisizione del linguaggio</em>: l’apprendimento del linguaggio nei bambini può essere descritto attraverso processi probabilistici che permettono di inferire le strutture grammaticali sulla base di dati linguistici limitati (Chater &amp; Manning, 2006; Xu &amp; Tenenbaum, in press).</li>
<li>
<em>Apprendimento causale</em>: la capacità di inferire relazioni causali dagli eventi osservati è coerente con un modello bayesiano, in cui la mente valuta la probabilità di una relazione causale sulla base dell’evidenza disponibile (Griffiths &amp; Tenenbaum, 2005, 2007).</li>
<li>
<em>Ragionamento e decisione</em>: il ragionamento simbolico e il processo decisionale possono essere formalizzati come un aggiornamento bayesiano delle credenze sulla base di nuove informazioni (Oaksford &amp; Chater, 2001).</li>
<li>
<em>Cognizione sociale</em>: le inferenze sulle intenzioni e credenze altrui possono essere modellate attraverso processi bayesiani, permettendo di spiegare come le persone comprendano il comportamento altrui (Baker, Tenenbaum, &amp; Saxe, 2007).</li>
</ul>
<section id="linferenza-bayesiana-nella-cognizione-umana" class="level4" data-number="6.1.5.1"><h4 data-number="6.1.5.1" class="anchored" data-anchor-id="linferenza-bayesiana-nella-cognizione-umana">
<span class="header-section-number">6.1.5.1</span> L’inferenza bayesiana nella cognizione umana</h4>
<p>Un tema centrale che emerge da questi programmi di ricerca è la seguente domanda: <em>come fa la mente umana ad andare oltre i dati dell’esperienza?</em> In altre parole, come riesce il cervello a costruire modelli complessi del mondo a partire da informazioni limitate e spesso ambigue?</p>
<p>L’approccio bayesiano propone che il cervello utilizzi un processo di <em>inferenza probabilistica</em> per aggiornare continuamente le proprie credenze, combinando informazioni pregresse con nuove osservazioni per affinare le proprie rappresentazioni mentali. Questo meccanismo consente di spiegare molte delle capacità cognitive umane, dall’apprendimento rapido di nuove categorie alla capacità di adattarsi a un ambiente mutevole, fino alla formulazione di inferenze sociali e alla presa di decisioni in condizioni di incertezza.</p>
<p>L’adozione dei modelli bayesiani nella psicologia cognitiva ha portato a una nuova comprensione della mente come <em>sistema predittivo</em>, in grado di formulare ipotesi probabilistiche sugli eventi futuri e di correggerle dinamicamente sulla base dell’esperienza. Questo approccio ha profonde implicazioni per lo studio del comportamento umano e per lo sviluppo di nuove tecniche di modellizzazione nei campi della psicologia, delle neuroscienze e dell’intelligenza artificiale.</p>
</section></section></section><section id="test-medici" class="level2" data-number="6.2"><h2 data-number="6.2" class="anchored" data-anchor-id="test-medici">
<span class="header-section-number">6.2</span> Test medici</h2>
<p>Uno degli esempi più comuni per comprendere il teorema di Bayes riguarda i test diagnostici.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consideriamo un test di mammografia utilizzato per diagnosticare il cancro al seno che abbiamo già discusso nel <a href="04_conditional_prob.html" class="quarto-xref"><span>Capitolo 5</span></a>. Definiamo le seguenti ipotesi:</p>
<ul>
<li>
<em><span class="math inline">\(M^+\)</span></em>: la persona ha il cancro al seno;</li>
<li>
<em><span class="math inline">\(M^-\)</span></em>: la persona non ha il cancro al seno.</li>
</ul>
<p>L’evidenza è il risultato positivo del test, indicato con <span class="math inline">\(T^+\)</span>. Il nostro obiettivo è calcolare la probabilità che una persona abbia il cancro al seno, dato un risultato positivo al test, ovvero <span class="math inline">\(P(M^+ \mid T^+)\)</span>.</p>
<p><em>Definizione dei termini nella regola di Bayes.</em><br>
Il teorema di Bayes afferma che:</p>
<p><span class="math display">\[
P(M^+ \mid T^+) = \frac{P(T^+ \mid M^+) P(M^+)}{P(T^+)} ,
\]</span></p>
<p>dove:</p>
<ul>
<li>
<em><span class="math inline">\(P(T^+ \mid M^+)\)</span></em> è la <em>sensibilità</em> del test, cioè la probabilità che il test risulti positivo se la persona ha effettivamente il cancro. Nel nostro caso, <span class="math inline">\(P(T^+ \mid M^+) = 0.90\)</span>.</li>
<li>
<em><span class="math inline">\(P(M^+)\)</span></em> è la <em>probabilità a priori</em> di avere il cancro al seno, ovvero la prevalenza della malattia nella popolazione. Supponiamo che sia <span class="math inline">\(P(M^+) = 0.01\)</span> (1%).</li>
<li>
<em><span class="math inline">\(P(T^+ \mid M^-)\)</span></em> è la <em>probabilità di un falso positivo</em>, cioè la probabilità che il test risulti positivo anche in assenza di malattia. Questa è complementare alla specificità del test:</li>
</ul>
<p><span class="math display">\[
  P(T^+ \mid M^-) = 1 - \text{Specificità} = 1 - 0.90 = 0.10.
\]</span></p>
<ul>
<li>
<em><span class="math inline">\(P(M^-)\)</span></em> è la probabilità a priori che una persona non abbia il cancro, ovvero:</li>
</ul>
<p><span class="math display">\[
  P(M^-) = 1 - P(M^+) = 1 - 0.01 = 0.99.
\]</span></p>
<ul>
<li>
<em><span class="math inline">\(P(T^+)\)</span></em> è la probabilità marginale che il test risulti positivo, calcolata considerando entrambe le possibilità (cioè che la persona abbia o non abbia il cancro):</li>
</ul>
<p><span class="math display">\[
  P(T^+) = P(T^+ \mid M^+) P(M^+) + P(T^+ \mid M^-) P(M^-).
\]</span></p>
<p>Sostituendo i valori numerici:</p>
<p><span class="math display">\[
  P(T^+) = (0.90 \cdot 0.01) + (0.10 \cdot 0.99) = 0.009 + 0.099 = 0.108.
\]</span></p>
<p><em>Applicazione della Regola di Bayes.</em><br>
Ora possiamo calcolare la probabilità a posteriori <span class="math inline">\(P(M^+ \mid T^+)\)</span>:</p>
<p><span class="math display">\[
P(M^+ \mid T^+) = \frac{0.90 \cdot 0.01}{0.108} = \frac{0.009}{0.108} = 0.0833.
\]</span></p>
<p><em>Interpretazione del Risultato.</em><br>
Questo risultato indica che, nonostante il test abbia una sensibilità e una specificità del 90%, la probabilità che una persona con un test positivo abbia effettivamente il cancro è solo dell’<em>8.3%</em>. Questo effetto è dovuto alla bassa prevalenza della malattia: anche se il test è relativamente accurato, il numero di falsi positivi è ancora alto rispetto ai veri positivi. Tale risultato conferma quanto precedentemente ottenuto nel <a href="04_conditional_prob.html" class="quarto-xref"><span>Capitolo 5</span></a>, attraverso un metodo di calcolo alternativo.</p>
<p>Questa formulazione mostra come la regola di Bayes permetta di aggiornare la probabilità di avere la malattia dopo aver osservato il risultato del test, combinando la sensibilità, la specificità e la prevalenza della malattia nella popolazione.</p>
</div>
</div>
</div>
<p>In un secondo esempio, vogliamo valutare l’affidabilità di un test per l’HIV e capire come la nostra stima di infezione <em>cambia dopo due test consecutivi positivi</em>. Utilizzeremo la <em>regola di Bayes</em> per aggiornare la probabilità di avere l’HIV man mano che otteniamo nuovi risultati.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Immaginiamo che una persona esegua due volte un test per l’HIV.</p>
<p><em>Notazione e dati iniziali.</em></p>
<p>Indichiamo con:</p>
<ul>
<li>
<span class="math inline">\(M^+\)</span>: la persona ha l’HIV;</li>
<li>
<span class="math inline">\(M^-\)</span>: la persona non ha l’HIV;</li>
<li>
<span class="math inline">\(T^+\)</span>: il test è positivo;</li>
<li>
<span class="math inline">\(T^-\)</span>: il test è negativo.</li>
</ul>
<p>Abbiamo inoltre i seguenti dati:</p>
<ul>
<li><p><em>Prevalenza</em> (probabilità a priori di avere l’HIV):<br><span class="math display">\[
P(M^+) = 0.003 \quad (0.3\%).
\]</span></p></li>
<li><p><em>Sensibilità</em> del test (probabilità che il test sia positivo se la persona è malata):<br><span class="math display">\[
P(T^+ \mid M^+) = 0.95.
\]</span></p></li>
<li><p><em>Specificità</em> del test (probabilità che il test sia negativo se la persona è sana):<br><span class="math display">\[
P(T^- \mid M^-) = 0.9928 \quad \Longrightarrow \quad P(T^+ \mid M^-) = 0.0072.
\]</span></p></li>
</ul>
<p><em>Passo 1: dopo il primo test positivo.</em></p>
<p>Usiamo la regola di Bayes per aggiornare la probabilità di essere malati, dopo un primo risultato positivo:</p>
<p><span class="math display">\[
P(M^+ \mid T^+) = \frac{P(T^+ \mid M^+)P(M^+)}{P(T^+)}.
\]</span></p>
<p>Calcoliamo la probabilità marginale di un test positivo, considerando entrambe le ipotesi:</p>
<p><span class="math display">\[
P(T^+) = P(T^+ \mid M^+)P(M^+) + P(T^+ \mid M^-)P(M^-).
\]</span></p>
<p>Sostituendo i valori noti, otteniamo:</p>
<p><span class="math display">\[
P(T^+) = (0.95 \times 0.003) + (0.0072 \times 0.997) = 0.00285 + 0.00718 = 0.01003.
\]</span></p>
<p>La probabilità aggiornata (posterior) diventa quindi:</p>
<p><span class="math display">\[
P(M^+ \mid T^+) = \frac{0.00285}{0.01003} \approx 0.2844 \quad (28.44\%).
\]</span></p>
<p>Dopo un primo test positivo, la probabilità che la persona sia effettivamente HIV-positiva sale da un valore iniziale molto basso (0.3%) a 28.44%, aumentando notevolmente ma senza ancora garantire la certezza.</p>
<p><em>Passo 2: aggiornamento dopo un secondo test positivo.</em></p>
<p>Adesso immaginiamo di ripetere il test e ottenere nuovamente un risultato positivo. La nuova probabilità si calcola applicando ancora la regola di Bayes, utilizzando come prior il risultato appena trovato:</p>
<p><span class="math display">\[
P(M^+ \mid T_1^+, T_2^+) = \frac{P(T_2^+ \mid M^+, T_1^+)P(M^+ \mid T_1^+)}{P(T_2^+ \mid T_1^+)}.
\]</span></p>
<p>Assumendo che i risultati dei test siano indipendenti dato lo stato di malattia o meno, possiamo semplificare:</p>
<ul>
<li><span class="math inline">\(P(T_2^+ \mid M^+, T_1^+) = P(T^+ \mid M^+) = 0.95\)</span></li>
<li><span class="math inline">\(P(T_2^+ \mid M^-, T_1^+) = P(T^+ \mid M^-) = 0.0072\)</span></li>
</ul>
<p>La probabilità di ottenere un secondo test positivo diventa quindi:</p>
<p><span class="math display">\[
P(T_2^+ \mid T_1^+) = P(T^+ \mid M^+)P(M^+ \mid T_1^+) + P(T^+ \mid M^-)P(M^- \mid T_1^+).
\]</span></p>
<p>Sostituendo i valori numerici calcolati in precedenza:</p>
<p><span class="math display">\[
P(T_2^+ \mid T_1^+) = (0.95 \times 0.2844) + (0.0072 \times 0.7156) = 0.2702 + 0.00515 = 0.27535.
\]</span></p>
<p>Ora calcoliamo la nuova probabilità a posteriori dopo due test positivi:</p>
<p><span class="math display">\[
P(M^+ \mid T_1^+, T_2^+) = \frac{0.95 \times 0.2844}{0.27535} \approx 0.981 \quad (98.1\%).
\]</span></p>
<p><em>Interpretazione finale.</em></p>
<ul>
<li>Dopo il <em>primo test positivo</em>, la probabilità passa dallo 0.3% iniziale a circa il 28.44%, aumentando notevolmente ma restando incerta.</li>
<li>Dopo il <em>secondo test positivo</em>, la probabilità sale drasticamente al 98.1%, rendendo quasi certa la diagnosi.</li>
</ul>
<p>Questo esempio dimostra chiaramente il valore dell’aggiornamento bayesiano: un singolo risultato positivo incrementa la probabilità, ma in presenza di una bassa prevalenza non basta per una diagnosi certa. Ripetere il test e ottenere conferme successive permette invece di raggiungere una certezza diagnostica molto elevata.</p>
</div>
</div>
</div>
</section><section id="la-fallacia-del-procuratore" class="level2" data-number="6.3"><h2 data-number="6.3" class="anchored" data-anchor-id="la-fallacia-del-procuratore">
<span class="header-section-number">6.3</span> La fallacia del procuratore</h2>
<p>Il <em>teorema di Bayes</em> non trova applicazione solo in campo medico, ma è essenziale anche nei procedimenti giudiziari. Infatti, fraintendimenti nell’interpretazione di probabilità e statistiche possono portare a <em>gravi errori di giudizio</em>. Uno degli errori più comuni in questo contesto è la <em>fallacia del procuratore</em>.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Che cos’è la fallacia del procuratore?</strong></p>
<p>La fallacia del procuratore consiste nel confondere la probabilità di osservare una certa evidenza se una persona è innocente, <em><span class="math inline">\(P(T^+ \mid I)\)</span></em>, con la probabilità che una persona sia innocente dopo aver osservato quella evidenza, <em><span class="math inline">\(P(I \mid T^+)\)</span></em>.</p>
<ul>
<li>In termini giudiziari, questo equivale a dire: “Poiché è estremamente improbabile ottenere un certo riscontro (ad es. un test positivo) se la persona è innocente, allora è estremamente improbabile che la persona sia innocente se si è ottenuto un esito positivo”.</li>
<li>In realtà, per stabilire se la persona è innocente o colpevole <em>dopo</em> aver visto il risultato, occorre considerare sia la bassa frequenza delle persone effettivamente colpevoli nella popolazione (<em>prevalenza</em>) sia la possibilità di <em>falsi positivi</em>. Il <em>teorema di Bayes</em> fornisce lo strumento formale per integrare questi elementi.</li>
</ul>
<p>Consideriamo il seguente esempio. Supponiamo di utilizzare un test del DNA per identificare un sospetto tra <em>65 milioni</em> di persone. Il test ha:</p>
<ul>
<li>
<em>Sensibilità</em> (<span class="math inline">\(P(T^+ \mid C)\)</span>) = 99%<br><span class="math inline">\(\rightarrow\)</span> Se la persona è effettivamente colpevole, il test risulta positivo il 99% delle volte.</li>
<li>
<em>Specificità</em> (<span class="math inline">\(P(T^- \mid I)\)</span>) = 99.99997%<br><span class="math inline">\(\rightarrow\)</span> Se la persona è innocente, il test risulta negativo il 99.99997% delle volte.<br>
Da cui segue che il tasso di falso positivo è <span class="math inline">\(1 - 0.9999997 = 0.0000003 = 0.00003\%\)</span>.</li>
<li>
<em>Prevalenza</em> (<span class="math inline">\(P(C)\)</span>) = <span class="math inline">\(1/65{,}000{,}000 \approx 1.54 \times 10^{-8}\)</span><br><span class="math inline">\(\rightarrow\)</span> Un individuo scelto a caso ha una probabilità di circa <span class="math inline">\(1.54 \times 10^{-8}\)</span> (cioè 1 su 65 milioni) di essere il vero colpevole.</li>
</ul>
<p>Un campione di DNA coincide con quello di una persona trovata nel database e il test dà <em>risultato positivo</em>. Qual è la probabilità che costui sia davvero colpevole? Formalmente, vogliamo <span class="math inline">\(P(C \mid T^+)\)</span>.</p>
<p><em>Passo 1: Calcolare <span class="math inline">\(P(T^+)\)</span>, la probabilità di un test positivo.</em></p>
<p>La probabilità complessiva di un esito positivo deriva da due scenari alternativi:</p>
<ol type="1">
<li>
<em>La persona è colpevole</em> e il test è positivo:<br><span class="math inline">\(P(T^+ \mid C) \times P(C)\)</span>.</li>
<li>
<em>La persona è innocente</em> e il test è positivo per errore (falso positivo):<br><span class="math inline">\(P(T^+ \mid I) \times P(I)\)</span>.</li>
</ol>
<p>Perciò, usando la regola della probabilità totale:</p>
<p><span class="math display">\[
P(T^+)
= P(T^+ \mid C) \, P(C) \;+\; P(T^+ \mid I) \, P(I).
\]</span></p>
<p>Assegniamo i valori numerici:</p>
<ul>
<li>
<span class="math inline">\(P(T^+ \mid C) = 0.99\)</span> (sensibilità).</li>
<li>
<span class="math inline">\(P(C) = 1.54 \times 10^{-8}\)</span>.</li>
<li>
<span class="math inline">\(P(T^+ \mid I) = 1 - P(T^- \mid I) = 1 - 0.9999997 = 0.0000003\)</span>.</li>
<li>
<span class="math inline">\(P(I) = 1 - P(C) \approx 0.99999998\)</span>.</li>
</ul>
<p>Eseguiamo il calcolo:</p>
<p><span class="math display">\[
\begin{aligned}
P(T^+)
&amp;= (0.99 \times 1.54 \times 10^{-8}) + (0.0000003 \times 0.99999998)\\
&amp;= 1.5231 \times 10^{-8} + 2.9999994 \times 10^{-7}\\
&amp;= 3.1523 \times 10^{-7}.
\end{aligned}
\]</span></p>
<p><em>Passo 2: Applicare la regola di Bayes per <span class="math inline">\(P(C \mid T^+)\)</span>.</em></p>
<p>Ora possiamo calcolare la probabilità di essere colpevoli dato che il test è positivo:</p>
<p><span class="math display">\[
P(C \mid T^+)
= \frac{P(T^+ \mid C)\,P(C)}{P(T^+)}.
\]</span></p>
<p>Inseriamo i valori:</p>
<p><span class="math display">\[
\begin{aligned}
P(C \mid T^+)
&amp;= \frac{(0.99 \times 1.54 \times 10^{-8})}{3.1523 \times 10^{-7}}\\
&amp;= \frac{1.5231 \times 10^{-8}}{3.1523 \times 10^{-7}}\\
&amp;\approx 0.0483 \quad (\text{cioè } 4.83\%).
\end{aligned}
\]</span></p>
<p><em>Interpretazione: perché è “solo” il 4.83%?</em></p>
<p>Sebbene <em>sensibilità e specificità</em> del test siano entrambe molto alte, la <em>prevalenza estremamente bassa</em> del colpevole (1 su 65 milioni) riduce notevolmente la probabilità a posteriori <span class="math inline">\(P(C \mid T^+)\)</span>. In una popolazione di 65 milioni di individui, anche un esiguo tasso di falsi positivi <em>(<span class="math inline">\(0.0000003\)</span>)</em> genera un numero assoluto di risultati positivi fra gli innocenti molto più grande del numero di colpevoli reali.</p>
<p>In pratica, pur avendo un test positivo, la probabilità che la persona sia davvero colpevole <em>resta modesta</em> (circa 4.83%), perché i “falsi allarmi” nella massa di individui innocenti superano di gran lunga i (pochi) veri positivi.</p>
<p><strong>Evitare la fallacia del procuratore.</strong></p>
<p>La fallacia del procuratore consiste nel confondere:</p>
<ul>
<li>
<em><span class="math inline">\(P(T^+ \mid I)\)</span></em>: la probabilità che un innocente risulti positivo (falso positivo),</li>
<li>
<em><span class="math inline">\(P(I \mid T^+)\)</span></em>: la probabilità di essere innocenti dopo un test positivo.</li>
</ul>
<p>Questa confusione porta a sovrastimare la colpevolezza di un individuo basandosi su una singola evidenza statistica. Applicando il <em>teorema di Bayes</em>, invece, si comprende che un test positivo non implica automaticamente colpevolezza, soprattutto quando la malattia (o il reato, in questo caso) è molto raro. Nei processi giudiziari, ciò significa che <em>un dato probabilistico deve sempre essere contestualizzato</em> alla popolazione di riferimento: la corretta interpretazione delle prove è fondamentale per evitare errori giudiziari.</p>
<p><strong>Conclusione epistemologica.</strong></p>
<p>L’impiego di test probabilistici in ambito giudiziario richiede un’applicazione rigorosa del <em>teorema di Bayes</em> per evitare distorsioni interpretative. Solo un corretto aggiornamento delle credenze, integrando:</p>
<ul>
<li>
<em>la probabilità pre-test</em> (<span class="math inline">\(P(C)\)</span>, prevalenza del colpevole nella popolazione investigata),<br>
</li>
<li>
<em>la potenza diagnostica del test</em> (sensibilità e specificità),<br>
</li>
<li>
<em>il tasso di errore strumentale</em> (falsi positivi e falsi negativi),</li>
</ul>
<p>consente di ridurre il rischio di <em>errori giudiziari sistematici</em>. In assenza di questa integrazione, anche test estremamente precisi possono condurre a <em>ingiuste condanne</em>, trasformando strumenti scientifici affidabili in fonti di distorsione probatoria.</p>
</div>
</div>
</div>
</section><section id="la-probabilità-inversa" class="level2" data-number="6.4"><h2 data-number="6.4" class="anchored" data-anchor-id="la-probabilità-inversa">
<span class="header-section-number">6.4</span> La probabilità inversa</h2>
<p>Gli esempi precedenti mostrano due tipi di domande probabilistiche fondamentali:</p>
<ol type="1">
<li>
<strong>Probabilità diretta</strong>
<ul>
<li><em>“Qual è la probabilità di osservare un certo risultato, supponendo che l’ipotesi sia vera?”</em></li>
</ul>
</li>
<li>
<strong>Probabilità inversa</strong>
<ul>
<li><em>“Qual è la probabilità che un’ipotesi sia vera, dati i risultati osservati?”</em></li>
</ul>
</li>
</ol>
<p>Questa distinzione è cruciale per comprendere il <em>teorema di Bayes</em> e le differenze tra l’approccio <em>frequentista</em> e quello <em>bayesiano</em> alla probabilità.</p>
<section id="esempi" class="level3" data-number="6.4.1"><h3 data-number="6.4.1" class="anchored" data-anchor-id="esempi">
<span class="header-section-number">6.4.1</span> Esempi</h3>
<p>Prendiamo come esempio il lancio di una moneta:</p>
<ul>
<li><p><strong>Probabilità diretta</strong>:<br>
Se riteniamo che la moneta sia equa (cioè <span class="math inline">\(P(\text{Testa}) = 0{.}5\)</span>), qual è la probabilità di osservare <em>zero teste</em> in cinque lanci? In questo caso, stiamo calcolando <span class="math display">\[
P(D \mid H) = (0.5)^5 = 0.03125,
\]</span> dove <span class="math inline">\(D\)</span> rappresenta il dato (“zero teste in cinque lanci”) e <span class="math inline">\(H\)</span> l’ipotesi (“la moneta è equa”).</p></li>
<li><p><strong>Probabilità inversa</strong>:<br>
Ora poniamo la domanda opposta. Abbiamo lanciato una moneta cinque volte e osservato <em>zero teste</em>. Quanto è probabile che la moneta sia davvero equa? Qui vogliamo conoscere <span class="math inline">\(\displaystyle P(H \mid D)\)</span> (l’ipotesi “la moneta è equa” dopo aver visto il risultato) anziché <span class="math inline">\(P(D \mid H)\)</span>. Per rispondere correttamente, ci occorre il <em>teorema di Bayes</em>, che combina la probabilità dei dati (<span class="math inline">\(P(D \mid H)\)</span>) con una stima iniziale (il <em>prior</em>) su quanto riteniamo probabile l’ipotesi prima dell’osservazione.</p></li>
</ul></section><section id="dalla-probabilità-diretta-alla-probabilità-inversa-il-contributo-di-bayes" class="level3" data-number="6.4.2"><h3 data-number="6.4.2" class="anchored" data-anchor-id="dalla-probabilità-diretta-alla-probabilità-inversa-il-contributo-di-bayes">
<span class="header-section-number">6.4.2</span> Dalla probabilità diretta alla probabilità inversa: il contributo di Bayes</h3>
<p>Per lungo tempo, la teoria della probabilità si è occupata quasi esclusivamente di <em>probabilità diretta</em>: “se l’ipotesi è vera, qual è la probabilità di osservare un certo esito?”. Nel XVIII secolo, Thomas Bayes capovolse la prospettiva, concentrandosi su come determinare la <em>probabilità dell’ipotesi</em> a partire dalle evidenze disponibili, introdusse cioè l’idea di <em>probabilità inversa</em>. Questa svolta ha aperto la strada a ciò che oggi chiamiamo <em>inferenza bayesiana</em>, permettendo di aggiornare in modo sistematico e rigoroso la credibilità di un’ipotesi dopo aver osservato nuovi dati.</p>
</section><section id="limpatto-della-probabilità-inversa" class="level3" data-number="6.4.3"><h3 data-number="6.4.3" class="anchored" data-anchor-id="limpatto-della-probabilità-inversa">
<span class="header-section-number">6.4.3</span> L’impatto della probabilità inversa</h3>
<p>La possibilità di stimare <span class="math inline">\(\displaystyle P(H \mid D)\)</span>, cioè la <em>probabilità di un’ipotesi data l’evidenza osservata</em>, si è rivelata fondamentale in molti ambiti:</p>
<ul>
<li>
<em>Scienza e sperimentazione</em>: quanto è probabile che un’ipotesi sia vera dopo aver raccolto i dati di un esperimento?<br>
</li>
<li>
<em>Medicina</em>: quanto è probabile che un paziente abbia una certa malattia, se il test diagnostico è positivo?<br>
</li>
<li>
<em>Giustizia</em>: quanto è probabile che una persona sia colpevole, se il DNA trovato sulla scena del crimine combacia col suo?</li>
</ul>
<p>In tutti questi casi non basta calcolare la probabilità dei dati “dato un’ipotesi” <span class="math inline">\(\bigl(P(D \mid H)\bigr)\)</span>; occorre invece aggiornare la stima della probabilità dell’ipotesi alla luce dei dati <span class="math inline">\(\bigl(P(H \mid D)\bigr)\)</span>.</p>
<p>In sintesi, l’<em>inferenza bayesiana</em> risponde appunto a questa seconda domanda, passando dalla <em>probabilità diretta</em> alla <em>probabilità inversa</em> in modo rigoroso. Grazie al <em>teorema di Bayes</em>, possiamo combinare in modo coerente le nostre conoscenze pregresse (il cosiddetto <em>prior</em>) con le evidenze raccolte, ottenendo una <em>probabilità a posteriori</em> che rappresenta la nostra nuova convinzione. Senza questa prospettiva, gran parte dei problemi scientifici e delle decisioni pratiche resterebbe priva di un metodo per collegare razionalmente le evidenze empiriche alle ipotesi da verificare.</p>
</section></section><section id="riflessioni-conclusive" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="riflessioni-conclusive">Riflessioni conclusive</h2>
<p>In questo capitolo abbiamo esplorato vari esempi, principalmente nel campo medico e forense, per illustrare come il teorema di Bayes permetta di combinare le informazioni derivate dalle osservazioni con le conoscenze precedenti (priori), aggiornando così il nostro grado di convinzione rispetto a un’ipotesi. Il teorema di Bayes fornisce un meccanismo razionale, noto come “aggiornamento bayesiano”, che ci consente di ricalibrare le nostre convinzioni iniziali alla luce di nuove evidenze.</p>
<p>Una lezione fondamentale che il teorema di Bayes ci insegna, sia nella ricerca scientifica che nella vita quotidiana, è che spesso non ci interessa tanto conoscere la probabilità che qualcosa accada assumendo vera un’ipotesi, quanto piuttosto la probabilità che un’ipotesi sia vera, dato che abbiamo osservato una certa evidenza. In altre parole, la forza del teorema di Bayes sta nella sua capacità di affrontare direttamente il problema inverso, cioè come dedurre la verità di un’ipotesi a partire dalle osservazioni.</p>
<p>Il framework bayesiano per l’inferenza probabilistica offre un approccio generale per comprendere come i problemi di induzione possano essere risolti in linea di principio e, forse, anche come possano essere affrontati dalla mente umana.</p>
<p>In questo capitolo ci siamo concentrati sull’applicazione del teorema di Bayes utilizzando probabilità puntuali. Tuttavia, il teorema esprime pienamente il suo potenziale quando sia l’evidenza che i gradi di certezza a priori delle ipotesi sono rappresentati attraverso distribuzioni di probabilità continue. Questo sarà l’argomento centrale nella prossima sezione della dispensa, dove approfondiremo il flusso di lavoro bayesiano e l’uso di distribuzioni continue nell’aggiornamento bayesiano.</p>
</section><section id="esercizi" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="esercizi">Esercizi</h2>
<p>È facile trovare online esercizi sull’applicazione del teorema di Bayes. Ad esempio, consiglio gli esercizi 1–6 disponibili sulla seguente <a href="https://mathcenter.oxford.emory.edu/site/math117/probSetBayesTheorem/">pagina web</a>.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Informazioni sull'ambiente di sviluppo">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Informazioni sull’ambiente di sviluppo
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/sessionInfo.html">sessionInfo</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; R version 4.5.1 (2025-06-13)</span></span>
<span><span class="co">#&gt; Platform: aarch64-apple-darwin20</span></span>
<span><span class="co">#&gt; Running under: macOS Sequoia 15.6.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Matrix products: default</span></span>
<span><span class="co">#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib </span></span>
<span><span class="co">#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; locale:</span></span>
<span><span class="co">#&gt; [1] C/UTF-8/C/C/C/C</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; time zone: Europe/Rome</span></span>
<span><span class="co">#&gt; tzcode source: internal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attached base packages:</span></span>
<span><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; other attached packages:</span></span>
<span><span class="co">#&gt;  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      </span></span>
<span><span class="co">#&gt;  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     </span></span>
<span><span class="co">#&gt;  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     </span></span>
<span><span class="co">#&gt; [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         </span></span>
<span><span class="co">#&gt; [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           </span></span>
<span><span class="co">#&gt; [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        </span></span>
<span><span class="co">#&gt; [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         </span></span>
<span><span class="co">#&gt; [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            </span></span>
<span><span class="co">#&gt; [25] here_1.0.1           </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span><span class="co">#&gt;  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         </span></span>
<span><span class="co">#&gt;  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     </span></span>
<span><span class="co">#&gt;  [7] digest_0.6.37         timechange_0.3.0      estimability_1.5.1   </span></span>
<span><span class="co">#&gt; [10] lifecycle_1.0.4       survival_3.8-3        magrittr_2.0.3       </span></span>
<span><span class="co">#&gt; [13] compiler_4.5.1        rlang_1.1.6           tools_4.5.1          </span></span>
<span><span class="co">#&gt; [16] knitr_1.50            bridgesampling_1.1-2  htmlwidgets_1.6.4    </span></span>
<span><span class="co">#&gt; [19] curl_7.0.0            pkgbuild_1.4.8        RColorBrewer_1.1-3   </span></span>
<span><span class="co">#&gt; [22] abind_1.4-8           multcomp_1.4-28       withr_3.0.2          </span></span>
<span><span class="co">#&gt; [25] purrr_1.1.0           grid_4.5.1            stats4_4.5.1         </span></span>
<span><span class="co">#&gt; [28] colorspace_2.1-1      xtable_1.8-4          inline_0.3.21        </span></span>
<span><span class="co">#&gt; [31] emmeans_1.11.2-8      scales_1.4.0          MASS_7.3-65          </span></span>
<span><span class="co">#&gt; [34] cli_3.6.5             mvtnorm_1.3-3         rmarkdown_2.29       </span></span>
<span><span class="co">#&gt; [37] ragg_1.5.0            generics_0.1.4        RcppParallel_5.1.11-1</span></span>
<span><span class="co">#&gt; [40] cachem_1.1.0          stringr_1.5.1         splines_4.5.1        </span></span>
<span><span class="co">#&gt; [43] parallel_4.5.1        vctrs_0.6.5           V8_7.0.0             </span></span>
<span><span class="co">#&gt; [46] Matrix_1.7-4          sandwich_3.1-1        jsonlite_2.0.0       </span></span>
<span><span class="co">#&gt; [49] arrayhelpers_1.1-0    systemfonts_1.2.3     glue_1.8.0           </span></span>
<span><span class="co">#&gt; [52] codetools_0.2-20      distributional_0.5.0  lubridate_1.9.4      </span></span>
<span><span class="co">#&gt; [55] stringi_1.8.7         gtable_0.3.6          QuickJSR_1.8.0       </span></span>
<span><span class="co">#&gt; [58] htmltools_0.5.8.1     Brobdingnag_1.2-9     R6_2.6.1             </span></span>
<span><span class="co">#&gt; [61] textshaping_1.0.3     rprojroot_2.1.1       evaluate_1.0.5       </span></span>
<span><span class="co">#&gt; [64] lattice_0.22-7        backports_1.5.0       memoise_2.0.1        </span></span>
<span><span class="co">#&gt; [67] broom_1.0.9           snakecase_0.11.1      rstantools_2.5.0     </span></span>
<span><span class="co">#&gt; [70] coda_0.19-4.1         gridExtra_2.3         nlme_3.1-168         </span></span>
<span><span class="co">#&gt; [73] checkmate_2.3.3       xfun_0.53             zoo_1.8-14           </span></span>
<span><span class="co">#&gt; [76] pkgconfig_2.0.3</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section><section id="bibliografia" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="bibliografia">Bibliografia</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-baker2011bayesian" class="csl-entry" role="listitem">
Baker, C., Saxe, R., &amp; Tenenbaum, J. (2011). Bayesian theory of mind: Modeling joint belief-desire attribution. <em>Proceedings of the annual meeting of the cognitive science society</em>, <em>33</em>.
</div>
<div id="ref-bellhouse2004" class="csl-entry" role="listitem">
Bellhouse, D. R. (2004). <em>The Reverend Thomas Bayes, FRS: a biography to celebrate the tercentenary of his birth</em>.
</div>
<div id="ref-caudek2024fenomeni" class="csl-entry" role="listitem">
Caudek, C., &amp; Bruno, N. (2024). Fenomeni stereocinetici, teorie della percezione e sociologia della scienza. <em>Giornale italiano di psicologia</em>, <em>51</em>(3), 451–466.
</div>
<div id="ref-chivers2024everything" class="csl-entry" role="listitem">
Chivers, T. (2024). <em>Everything is Predictable: How Bayesian Statistics Explain Our World</em>. Simon; Schuster.
</div>
<div id="ref-domini20033" class="csl-entry" role="listitem">
Domini, F., &amp; Caudek, C. (2003). 3-D structure perceived from dynamic information: A new theory. <em>Trends in Cognitive Sciences</em>, <em>7</em>(10), 444–449.
</div>
<div id="ref-griffiths2024bayesian" class="csl-entry" role="listitem">
Griffiths, T. L., Chater, N., &amp; Tenenbaum, J. B. (2024). <em>Bayesian models of cognition: reverse engineering the mind</em>. MIT Press.
</div>
<div id="ref-jesseph1993berkeley" class="csl-entry" role="listitem">
Jesseph, D. M. (1993). <em>Berkeley’s philosophy of mathematics</em>. University of Chicago Press.
</div>
<div id="ref-ma2023bayesian" class="csl-entry" role="listitem">
Ma, W. J., Kording, K. P., &amp; Goldreich, D. (2023). <em>Bayesian models of perception and action: An introduction</em>. MIT press.
</div>
<div id="ref-schervish2014probability" class="csl-entry" role="listitem">
Schervish, M. J., &amp; DeGroot, M. H. (2014). <em>Probability and statistics</em> (Vol. 563). Pearson Education London, UK:
</div>
<div id="ref-spiegelhalter2019art" class="csl-entry" role="listitem">
Spiegelhalter, D. (2019). <em>The art of statistics: Learning from data</em>. Penguin UK.
</div>
<div id="ref-stigler1990history" class="csl-entry" role="listitem">
Stigler, S. M. (1990). <em>The history of statistics: The measurement of uncertainty before 1900</em>. Harvard University Press.
</div>
<div id="ref-yuille2006vision" class="csl-entry" role="listitem">
Yuille, A., &amp; Kersten, D. (2006). Vision as Bayesian inference: analysis by synthesis? <em>Trends in cognitive sciences</em>, <em>10</em>(7), 301–308.
</div>
</div>
</section></main><!-- /main --><script>
document.body.classList.add('classic-book');
document.addEventListener('DOMContentLoaded', function() {
  const paragraphs = document.querySelectorAll('p');
  paragraphs.forEach(p => {
    if (p.textContent.length > 200) {
      p.style.hyphens = 'auto';
      p.style.hyphenateCharacter = '-';
    }
  });
  const headings = document.querySelectorAll('h1, h2, h3, h4, h5, h6');
  headings.forEach(h => {
    h.style.fontFeatureSettings = '"liga" 1, "dlig" 1, "smcp" 1';
  });
});
</script><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiato!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiato!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/utet-prob\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/probability/04_conditional_prob.html" class="pagination-link" aria-label="Probabilità condizionata">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/probability/06_random_var.html" class="pagination-link" aria-label="Variabili casuali">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p><strong>Probabilità per la psicologia</strong> — Modulo di richiamo del progetto UTET a supporto del manuale <em>Metodi bayesiani in psicologia</em>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ccaudek/utet-prob/blob/main/chapters/probability/05_bayes_theorem.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/utet-prob/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>