# Distribuzioni di v.c. discrete {#sec-prob-discrete-prob-distr}

::: callout-important
## In questo capitolo imparerai a:

- comprendere le principali distribuzioni di massa di probabilit√†;
- utilizzare R per manipolare e analizzare queste distribuzioni.
::: 

::: callout-tip
## Prerequisiti

- Leggere il capitolo *Random variables and their distributions* del testo di @blitzstein2019introduction.
- Leggere il capitolo *Special Distributions* [@schervish2014probability].
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(reshape2)
```
:::


## Introduzione

√à importante distinguere tra variabili casuali discrete e continue, perch√© le distribuzioni di probabilit√† associate sono molto diverse nei due casi [si veda il @sec-prob-random-var].

In questo capitolo ci focalizzeremo sulle **distribuzioni di probabilit√† discrete**, strumenti fondamentali per modellare fenomeni aleatori che generano un numero finito o numerabile di possibili esiti. Queste distribuzioni risultano particolarmente efficaci per descrivere eventi che si verificano in contesti discreti, come il numero di successi in un esperimento, l'occorrenza di un evento, o la selezione casuale da un insieme di opzioni finite. 

### Panoramica delle Distribuzioni Discrete

Di seguito, vengono presentate alcune delle principali distribuzioni discrete utilizzate in statistica e nella ricerca psicologica Ogni distribuzione √® descritta in termini di caratteristiche fondamentali, applicazioni pratiche e importanza teorica.

#### Distribuzione Uniforme Discreta

- **Descrizione**: La distribuzione uniforme discreta rappresenta situazioni in cui tutti gli eventi all'interno di un insieme finito hanno la stessa probabilit√† di verificarsi.
- **Applicazioni**: Si applica in contesti di scelta casuale equiprobabile, come:
  - La selezione casuale di uno stimolo da una lista di parole in un esperimento di memoria.
  - L'assegnazione casuale di partecipanti a gruppi sperimentali in uno studio di psicologia sociale.
  - La scelta di un'immagine tra un insieme di stimoli visivi in una ricerca sull'attenzione.
  - La probabilit√† uniforme che un partecipante scelga una delle opzioni in un questionario a risposte multiple, in assenza di preferenze o conoscenze specifiche.
- **Parametri**:
  - Intervallo di supporto: l'insieme finito di valori possibili (ad esempio, $\{1, 2, \dots, k\}$).
- **Importanza**: Funziona come modello di riferimento in situazioni di massima incertezza o mancanza di preferenze. √à utile per definire un punto di partenza in analisi pi√π complesse e per studiare comportamenti casuali.


#### Distribuzione di Bernoulli

- **Descrizione**: La distribuzione di Bernoulli modella esperimenti con due possibili esiti, generalmente etichettati come "successo" (con probabilit√† $p$) e "fallimento" (con probabilit√† $1-p$).
- **Applicazioni**: Si applica a situazioni binarie, come il lancio di una moneta (testa/croce), la risposta a domande dicotomiche (s√¨/no), o l'esito di un evento che pu√≤ verificarsi o meno.
- **Parametro**: 
  - $p$: probabilit√† di successo.
- **Importanza**: Costituisce la base per molte altre distribuzioni discrete, come la distribuzione binomiale e geometrica. √à fondamentale per comprendere fenomeni con esiti dichotomici.

#### Distribuzione Binomiale

- **Descrizione**: La distribuzione binomiale descrive il numero totale di successi in un numero fisso $n$ di prove indipendenti, ciascuna governata da una distribuzione di Bernoulli con probabilit√† di successo $p$.
- **Applicazioni**: Viene utilizzata per analizzare processi ripetuti con esiti binari, ad esempio:
  - Il numero di voti favorevoli in un campione di opinione.
  - Il numero di sintomi osservati in un gruppo di pazienti.
  - Il conteggio di errori in un test di accuratezza.
- **Parametri**:
  - $n$: numero di prove.
  - $p$: probabilit√† di successo in ogni prova.
- **Importanza**: Fornisce uno strumento essenziale per modellare fenomeni ripetuti in condizioni identiche, consentendo analisi probabilistiche avanzate e previsioni statistiche.

#### Distribuzione di Poisson

- **Descrizione**: La distribuzione di Poisson modella il numero di eventi che si verificano in un intervallo fissato di tempo o spazio, quando tali eventi sono rari, indipendenti e accadono a un tasso medio costante $\lambda$.
- **Applicazioni**: Trova impiego in contesti dove gli eventi sono sporadici ma prevedibili, ad esempio:
  - Il numero di episodi di ansia riportati in una settimana.
  - Il numero di interazioni sociali spontanee di un bambino con disturbo dello spettro autistico durante una sessione di osservazione.
  - La frequenza di lapsus verbali durante una presentazione pubblica.
  - Il numero di sogni vividi riportati durante una serie di notti consecutive in uno studio sul sonno.
- **Parametro**:
  - $\lambda$: tasso medio di eventi per unit√† di tempo o spazio.
- **Importanza**: √à cruciale per analizzare fenomeni psicologici o comportamentali rari ma significativi. Aiuta a comprendere i meccanismi sottostanti e a modellare la variabilit√† osservata in contesti clinici, sperimentali o quotidiani.


In conclusione, le distribuzioni discrete sopra descritte rappresentano strumenti fondamentali per modellare una vasta gamma di fenomeni osservati in ambito scientifico, psicologico e applicativo. Ciascuna distribuzione offre una cornice teorica ben definita per interpretare e analizzare situazioni caratterizzate da variabili aleatorie discrete, fornendo cos√¨ le basi per inferenze statistiche robuste e previsioni quantitative affidabili.

## Distribuzioni in R

In R, per ogni distribuzione sono disponibili quattro funzioni principali, i cui nomi iniziano con le lettere:

- **d** (*density*): per calcolare i valori teorici relativi alla distribuzione,  
- **p** (*probability*): per ottenere la probabilit√† cumulativa,  
- **q** (*quantile*): per determinare i quantili,  
- **r** (*random*): per generare campioni casuali.  

Il pacchetto di base `stats` include numerose funzioni dedicate alle principali distribuzioni statistiche, permettendo di calcolare valori teorici e simulare dati in modo semplice e flessibile. Per ulteriori dettagli sulle distribuzioni disponibili e sull'uso delle relative funzioni, √® possibile consultare la documentazione con il comando `?Distributions`.


## Distribuzione Uniforme Discreta

La **distribuzione uniforme discreta** √® una delle pi√π semplici e intuitive distribuzioni di probabilit√†. √à utilizzata per modellare situazioni in cui **tutti gli esiti possibili sono ugualmente probabili**. Si applica, ad esempio, quando si estrae un numero a caso da un insieme finito di interi senza alcuna preferenza.

::: {#def-}

Sia $X$ una variabile casuale che pu√≤ assumere i valori interi da 1 a $N$, tutti con la **stessa probabilit√†**. Allora diciamo che $X$ ha una distribuzione uniforme discreta sull‚Äôintervallo $\{1, 2, \dots, N\}$. In simboli:

$$
X \sim \text{Uniforme Discreta}(1, N) .
$$

Poich√© ci sono $N$ valori possibili e ciascuno ha la stessa probabilit√†, ogni valore ha probabilit√†:

$$
P(X = x) = \frac{1}{N}, \quad \text{per } x \in \{1, 2, \dots, N\}.
$$
:::

### Propriet√† di normalizzazione

La somma delle probabilit√† di tutti gli esiti deve essere pari a 1:

$$
\sum_{x = 1}^{N} P(X = x) = \sum_{x = 1}^{N} \frac{1}{N} = \frac{1}{N} \cdot N = 1.
$$

Questa √® una propriet√† fondamentale di ogni distribuzione di probabilit√†.

### Valore atteso

Il **valore atteso** (o media) ci dice qual √® il risultato medio atteso nel lungo periodo. Si calcola come:

$$
\mathbb{E}(X) = \sum_{x = 1}^{N} x \cdot P(X = x) = \frac{1}{N} \sum_{x = 1}^{N} x.
$$

La somma dei primi $N$ numeri naturali √®:

$$
\sum_{x = 1}^{N} x = \frac{N(N + 1)}{2}.
$$

Quindi:

$$
\mathbb{E}(X) = \frac{1}{N} \cdot \frac{N(N + 1)}{2} = \frac{N + 1}{2}.
$$

In conclusione, il valore atteso di una variabile uniforme discreta su $\{1, \dots, N\}$ √® $\frac{N + 1}{2}$.

### Varianza

La varianza della distribuzione uniforme discreta √®:

$$
\mathbb{V}(X) = \frac{(N + 1)(N - 1)}{12}.
$$

::: {.callout-tip title="Dimostrazione" collapse="true"}

La **varianza** misura quanto i valori di $X$ si discostano in media dalla media $\mathbb{E}(X)$. Si calcola come:

$$
\mathbb{V}(X) = \mathbb{E}(X^2) - \left[\mathbb{E}(X)\right]^2.
$$

1. Calcolo di $\mathbb{E}(X^2)$. 

Poich√© tutti i valori hanno la stessa probabilit√† $\frac{1}{N}$, otteniamo:

$$
\mathbb{E}(X^2) = \frac{1}{N} \sum_{x = 1}^{N} x^2.
$$

La somma dei quadrati dei primi $N$ interi √®:

$$
\sum_{x = 1}^{N} x^2 = \frac{N(N + 1)(2N + 1)}{6}
$$

(per una dimostrazione, si veda la [ pagina di Wikipedia sui numeri piramidali quadrati](https://it.wikipedia.org/wiki/Numero_piramidale_quadrato)).

Quindi:

$$
\mathbb{E}(X^2) = \frac{1}{N} \cdot \frac{N(N + 1)(2N + 1)}{6} = \frac{(N + 1)(2N + 1)}{6}.
$$

2. Calcolo della varianza. 

Sostituendo nella formula della varianza:

$$
\begin{aligned}
\mathbb{V}(X) &= \frac{(N + 1)(2N + 1)}{6} - \left(\frac{N + 1}{2}\right)^2 \\
&= \frac{(N + 1)(2N + 1)}{6} - \frac{(N + 1)^2}{4}
\end{aligned}
$$

Per semplificare, portiamo tutto allo stesso denominatore:

$$
\begin{aligned}
\mathbb{V}(X) &= \frac{2(N + 1)(2N + 1)}{12} - \frac{3(N + 1)^2}{12} \\
&= \frac{(N + 1) \left[2(2N + 1) - 3(N + 1)\right]}{12} \\
&= \frac{(N + 1)(4N + 2 - 3N - 3)}{12} \\
&= \frac{(N + 1)(N - 1)}{12}.
\end{aligned}
$$

In conclusione, la varianza della distribuzione uniforme discreta √®:

$$
\mathbb{V}(X) = \frac{(N + 1)(N - 1)}{12}.
$$
:::

**In sintesi**, per una variabile casuale $X$ uniformemente distribuita su $\{1, 2, \dots, N\}$:

| Propriet√†        | Formula                                 |
|------------------|------------------------------------------|
| Media            | $\mathbb{E}(X) = \dfrac{N + 1}{2}$       |
| Varianza         | $\mathbb{V}(X) = \dfrac{(N + 1)(N - 1)}{12}$ |

Questa distribuzione √® utile ogni volta che **non c‚Äô√® alcuna ragione per preferire un valore a un altro** all‚Äôinterno di un insieme finito di numeri interi.

::: {#exm-}
Supponiamo che $X$ sia una variabile casuale con distribuzione uniforme discreta tra 1 e 10, ovvero:

$$
X \sim \text{Uniforme Discreta}(1, 10) .
$$

Vogliamo:

1. generare un grande campione casuale,
2. calcolare la media e la varianza osservate,
3. confrontarle con i valori teorici.

**Codice R.**

```{r}
set.seed(123)  # Per rendere la simulazione riproducibile

# Parametro N
N <- 10

# Simulazione: 100.000 osservazioni dalla distribuzione uniforme discreta
x <- sample(1:N, size = 100000, replace = TRUE)

# Media e varianza empiriche
media_empirica <- mean(x)
varianza_empirica <- var(x)

# Valori teorici
media_teorica <- (N + 1) / 2
varianza_teorica <- ((N + 1) * (N - 1)) / 12

# Risultati
tibble(
  `Media empirica` = media_empirica,
  `Media teorica` = media_teorica,
  `Varianza empirica` = varianza_empirica,
  `Varianza teorica` = varianza_teorica
)
```

Con un campione molto grande, le **statistiche empiriche** (cio√® calcolate dai dati simulati) saranno molto vicine ai **valori teorici**:

|                     | Valore     |
|---------------------|------------|
| Media teorica       | 5.5        |
| Media empirica      | ‚âà 5.5      |
| Varianza teorica    | 8.25       |
| Varianza empirica   | ‚âà 8.25     |

\

**In sintesi**, a simulazione conferma che:

- la media empirica converge verso $\mathbb{E}(X) = \frac{N + 1}{2}$,
- la varianza empirica converge verso $\mathbb{V}(X) = \frac{(N + 1)(N - 1)}{12}$.
:::


## Distribuzione di Bernoulli 

{{< include ./distributions/binomial_distr.qmd >}}

## Distribuzione Binomiale

La distribuzione binomiale √® una distribuzione di probabilit√† discreta che modella il numero di successi $y$ in un numero fissato $n$ di prove di Bernoulli indipendenti e identiche, dove ciascuna prova ha solo due esiti possibili: "successo" (rappresentato da "1") con probabilit√† $p$ o "insuccesso" (rappresentato da "0") con probabilit√† $1 - p$. La notazione utilizzata √® la seguente:

$$
Y \sim \mathcal{Binom}(n, p).
$$

::: {#def-}
La distribuzione binomiale descrive la probabilit√† di osservare esattamente $y$ successi in $n$ prove di Bernoulli indipendenti:

$$
P(Y = y) = \binom{n}{y} p^{y} (1 - p)^{n - y} = \frac{n!}{y!(n - y)!} p^{y} (1 - p)^{n - y},
$$ {#eq-binom-distr}

dove $\binom{n}{y}$, noto come coefficiente binomiale, rappresenta il numero di modi possibili per ottenere $y$ successi in $n$ prove, e $p$ √® la probabilit√† di successo in ciascuna prova.
:::

La distribuzione binomiale si presta bene a esempi classici come il lancio ripetuto di una moneta o l'estrazione di biglie da un'urna. Ad esempio, nel caso del lancio di una moneta, questa distribuzione descrive la probabilit√† di ottenere un determinato numero di "teste" in un certo numero di lanci, con ogni lancio che segue una distribuzione di Bernoulli con probabilit√† di successo $p$.

Una caratteristica interessante della distribuzione binomiale √® la sua *propriet√† di riproducibilit√†*: se due variabili casuali indipendenti, $y_1$ e $y_2$, seguono entrambe distribuzioni binomiali con lo stesso parametro $p$, ma con un diverso numero di prove ($n_1$ e $n_2$), la loro somma, $y = y_1 + y_2$, sar√† ancora distribuita binomialmente, con parametri $n_1 + n_2$ e $p$.

::: callout-note
## Dimostrazione

Per chiarire il calcolo delle probabilit√† nella **distribuzione binomiale**, consideriamo una serie di prove di **Bernoulli**. Supponiamo di avere $n$ prove indipendenti, ciascuna con probabilit√† $p$ di successo, e di osservare esattamente $y$ successi.

Una possibile configurazione dei risultati pu√≤ essere rappresentata come:

$$
\overbrace{SS\dots S}^\text{$y$ successi} \, \overbrace{II\dots I}^\text{$n - y$ insuccessi}
$$

La probabilit√† di ottenere **esattamente $y$ successi in una sequenza specifica** (cio√® in un ordine fissato) √®:

$$
p^y \cdot (1 - p)^{n - y},
$$

dove $p^y$ √® la probabilit√† dei $y$ successi e $(1 - p)^{n - y}$ quella dei $n - y$ insuccessi.

Tuttavia, ci√≤ che ci interessa √® la **probabilit√† complessiva** di ottenere $y$ successi in *qualsiasi ordine*. In altre parole, vogliamo calcolare la probabilit√† dell'**unione** di tutte le possibili sequenze di $n$ prove che contengono esattamente $y$ successi.

Il numero di tali sequenze √® dato dal **coefficiente binomiale** $\binom{n}{y}$, che rappresenta il numero di modi diversi in cui possiamo scegliere le $y$ posizioni dei successi tra le $n$ prove.

Moltiplicando la probabilit√† di una singola sequenza per il numero totale di sequenze possibili, otteniamo la **funzione di probabilit√† della distribuzione binomiale**:

$$
P(Y = y) = \binom{n}{y} p^y (1 - p)^{n - y}.
$$
:::

### Caso particolare $n = 1$

Ora consideriamo il caso particolare in cui $n = 1$. Quando $n = 1$, il coefficiente binomiale diventa:

$$
\binom{1}{y} = \frac{1!}{y! (1-y)!}.
$$

Espandiamo i fattoriali per i due possibili valori di $y$, che pu√≤ assumere solo 0 o 1 (poich√© $y \in \{0, 1, \dots, n\}$).

**Caso 1: $y = 0$**

$$
\binom{1}{0} = \frac{1!}{0! (1-0)!} = \frac{1}{1 \cdot 1} = 1.
$$

Quindi, per $y = 0$:
$$
P(Y = 0) = \binom{1}{0} p^0 (1-p)^{1-0} = 1 \cdot 1 \cdot (1-p) = 1-p.
$$

**Caso 2: $y = 1$**

$$
\binom{1}{1} = \frac{1!}{1! (1-1)!} = \frac{1}{1 \cdot 1} = 1.
$$

Quindi, per $y = 1$:
$$
P(Y = 1) = \binom{1}{1} p^1 (1-p)^{1-1} = 1 \cdot p \cdot 1 = p.
$$

In conclusione, la PMF per la distribuzione binomiale con $n = 1$ diventa:

$$
P(Y = y) =
\begin{cases}
1-p, & \text{se } y = 0, \\
p, & \text{se } y = 1.
\end{cases}
$$

Questa √® esattamente la PMF della distribuzione di Bernoulli con parametro $p$:

$$
P(Y = y) = p^y (1-p)^{1-y}, \quad y \in \{0, 1\}.
$$ 

Pertanto, la distribuzione binomiale con $n = 1$ √® equivalente alla distribuzione di Bernoulli con parametro $p$.

### Applicazioni Pratiche della Distribuzione Binomiale

Per illustrare l‚Äôapplicazione della distribuzione binomiale, consideriamo un esempio semplice. Supponiamo di osservare **2 successi su 4 prove di Bernoulli**, dove la probabilit√† di successo in ogni prova √® $p = 0.2$. La probabilit√† di ottenere esattamente questo risultato si calcola con la formula:

$$
P(Y = 2) = \binom{4}{2} \cdot 0.2^2 \cdot (1 - 0.2)^{2} = 0.1536.
$$

In R, questo calcolo si pu√≤ fare in modo diretto:

```{r}
# Parametri
n <- 4
p <- 0.2
y <- 2

# Calcolo della probabilit√† esatta
prob <- choose(n, y) * p^y * (1 - p)^(n - y)
print(prob)
```

In alternativa, possiamo usare la funzione `dbinom()` per ottenere la stessa probabilit√†:

```{r}
prob <- dbinom(x = y, size = n, prob = p)
print(prob)
```

#### Visualizzazione della distribuzione di probabilit√†

Possiamo rappresentare graficamente la **distribuzione di massa di probabilit√†** per tutti i possibili valori di $y$ da $0$ a $n$:

```{r}
y <- 0:n
probabilities <- dbinom(y, size = n, prob = p)

df <- data.frame(Successi = y, Probabilit√† = probabilities)

df |>
  ggplot(aes(x = Successi, y = Probabilit√†)) +
    geom_segment(
      aes(xend = Successi, yend = 0), lwd = 1.2
      ) +
    geom_point(size = 3) +
    labs(
      x = "Numero di successi y",
      y = "Probabilit√†"
    )
```

#### Generazione di un campione casuale

La funzione `rbinom()` permette di generare un campione casuale da una distribuzione binomiale:

```{r}
set.seed(42)
samples <- rbinom(n = 30, size = 5, prob = 0.5)
print(samples)
```

#### Variazione della distribuzione al variare di $p$

Per esplorare l‚Äôeffetto di diversi valori di $p$ sulla forma della distribuzione, possiamo visualizzare pi√π curve binomiali per $n = 20$ e $p$ variabile:

```{r}
n <- 20
p_values <- seq(0.3, 0.9, by = 0.3)
y <- 0:25

df <- data.frame()

for (p in p_values) {
  binom_dist <- dbinom(y, size = n, prob = p)
  df <- rbind(df, data.frame(y = y, Prob = binom_dist, p = factor(p)))
}

df |>
  ggplot(aes(x = y, y = Prob, color = p)) +
    geom_point() +
    geom_line() +
    labs(
      x = "Numero di successi y",
      y = "Probabilit√†",
      color = expression(p)
    )
```

#### Funzione di ripartizione cumulativa

Possiamo anche rappresentare la **funzione di distribuzione cumulativa** (CDF) per $n = 5$ e $p = 0.5$:

```{r}
n <- 5
p <- 0.5
y <- 0:n

cdf_values <- pbinom(y, size = n, prob = p)
df <- data.frame(y = y, cdf = cdf_values)

df |>
  ggplot(aes(x = y, y = cdf)) +
    geom_line() +
    geom_point() +
    geom_hline(
      yintercept = 1, linetype = "dashed", color = "black", alpha = 0.7
    ) +
    labs(
      x = "Numero di successi y",
      y = "Probabilit√† cumulativa"
    )
```

::: {#exm-}
Supponiamo di lanciare una **moneta equa** (cio√® con probabilit√† $p = 0.5$ di ottenere testa) **5 volte**. Vogliamo calcolare la probabilit√† di ottenere **almeno 2 teste**, ovvero:

$$
P(Y \geq 2) = P(Y = 2) + P(Y = 3) + P(Y = 4) + P(Y = 5).
$$

Possiamo sommare direttamente queste probabilit√† usando `dbinom()`:

```{r}
result <- sum(dbinom(2:5, size = 5, prob = 0.5))
print(result)
```

Un modo alternativo, pi√π efficiente, consiste nel calcolare il **complemento** della probabilit√† di ottenere **meno di 2 teste** (cio√® 0 o 1):

$$
P(Y \geq 2) = 1 - P(Y \leq 1)
$$

In R, possiamo usare la funzione `pbinom()` per calcolare questa probabilit√† cumulativa:

```{r}
result <- 1 - pbinom(q = 1, size = 5, prob = 0.5)
print(result)
```

Entrambi i metodi restituiscono lo stesso risultato numerico, ma il secondo √® spesso preferibile quando $n$ √® grande o quando si vuole calcolare una probabilit√† di coda.
:::

#### Quantili di una distribuzione binomiale

Hai perfettamente ragione ‚Äî grazie per l'osservazione!

Infatti, con i parametri `size = 5`, `prob = 0.5` e `target_probability = 0.60`, la funzione `qbinom()` restituisce **3**, non **2**. Questo perch√© `qbinom()` restituisce **il pi√π piccolo valore di $y$ tale che $P(Y \leq y) \geq p$**. Verifichiamolo in R:

```r
pbinom(2, 5, 0.5)  # = 0.5
pbinom(3, 5, 0.5)  # = 0.8125
```

Quindi:

- $P(Y \leq 2) = 0.5$ ‚Üí troppo poco
- $P(Y \leq 3) = 0.8125$ ‚Üí supera il 60%

Pertanto, `qbinom(0.6, 5, 0.5)` restituisce `3`.

#### Quantili di una distribuzione binomiale

La funzione `qbinom()` permette di calcolare il **quantile** di una distribuzione binomiale, cio√® il **numero minimo di successi** $y$ tale che la probabilit√† cumulativa $P(Y \leq y)$ sia **maggiore o uguale** a una certa soglia.

Ad esempio, supponiamo di voler sapere **qual √® il numero minimo di successi** tale che la probabilit√† cumulativa sia **almeno 60%**. Possiamo usare:

```{r}
# Probabilit√† cumulativa desiderata
target_probability <- 0.60

# Calcolo del quantile
result <- qbinom(p = target_probability, size = 5, prob = 0.5)
print(result)
```

Il risultato √® `3`, il che significa che:

$$
P(Y \leq 3) = 0.8125 \geq 0.60,
$$

mentre

$$
P(Y \leq 2) = 0.5 < 0.60.
$$

Quindi, servono **almeno 3 successi** per superare la soglia del 60% di probabilit√† cumulativa.

> üîé `qbinom(p, size, prob)` restituisce il **pi√π piccolo valore di $y$** tale che $P(Y \leq y) \geq p$.

#### Rappresentazione grafica del quantile

Per visualizzare il comportamento della funzione di ripartizione cumulativa e individuare il quantile per $p = 0.60$, possiamo usare il seguente codice in R:

```{r}
# Parametri
n <- 5
p <- 0.5
target_probability <- 0.60

# Asse y: numero di successi
y <- 0:n

# Calcolo dei valori cumulativi
cdf <- pbinom(y, size = n, prob = p)

# Calcolo del quantile
q <- qbinom(target_probability, size = n, prob = p)

# Data frame
df <- data.frame(Successi = y, CDF = cdf)

# Grafico
df |>
  ggplot(aes(x = Successi, y = CDF)) +
  geom_step(direction = "hv", linewidth = 1.1) +
  geom_point(size = 2) +
  geom_hline(
    yintercept = target_probability, linetype = "dashed", color = "red"
  ) +
  geom_vline(xintercept = q, linetype = "dotted", color = "blue") +
  annotate(
    "text",
    x = q + 0.4, y = 0.05, label = paste("quantile =", q),
    color = "blue"
  ) +
  annotate(
    "text",
    x = 0.5, y = target_probability + 0.05,
    label = paste("soglia =", target_probability), color = "red"
  ) +
  labs(
    x = "Numero di successi",
    y = "Probabilit√† cumulativa"
  ) +
  ylim(0, 1.05)
```

In questo grafico:

- la **linea rossa tratteggiata** rappresenta la soglia di probabilit√† desiderata (es. 0.60);
- la **linea blu tratteggiata verticale** indica il quantile corrispondente, cio√® il pi√π piccolo valore di $y$ per cui $P(Y \leq y) \geq 0.60$;
- il valore calcolato √® `3`, quindi con **al massimo 3 successi**, la probabilit√† cumulativa supera il 60%.

::: {#exm-}
Consideriamo una **distribuzione binomiale** con $n = 10$ prove e probabilit√† di successo $p = 0.2$. Supponiamo di voler calcolare la probabilit√† di ottenere **al massimo 4 successi**. In termini matematici, vogliamo calcolare:

$$
P(Y \leq 4) .
$$

In R, questo si ottiene con la funzione `pbinom()`:

```{r}
# Calcolo della probabilit√† cumulativa fino a 4 successi
p_cumulativa <- pbinom(4, size = 10, prob = 0.2)
print(p_cumulativa)
```

Il risultato indica che c'√® circa l'**97%** di probabilit√† di ottenere 4 o meno successi su 10 prove, quando la probabilit√† di successo in ciascuna prova √® 0.2.

Ora facciamo il **passaggio inverso**: immaginiamo di conoscere la probabilit√† cumulativa (per esempio, 0.97) e vogliamo sapere **quanti successi** bisogna considerare per raggiungere quella probabilit√†.

Per questo usiamo la funzione `qbinom()`, che ci restituisce il **pi√π piccolo numero di successi $y$ tale che $P(Y \leq y) \geq$ quella probabilit√†**:

```{r}
# Calcolo del numero di successi associato alla probabilit√† cumulativa
numero_successi <- qbinom(p_cumulativa, size = 10, prob = 0.2)
print(numero_successi)
```

Il valore ottenuto sar√† `4`, cio√® il **minimo numero di successi** per cui la probabilit√† cumulativa √® almeno il 97%.

**Riepilogo concetti chiave**:

- `pbinom(y, n, p)` calcola la probabilit√† di ottenere **al massimo $y$ successi**;
- `qbinom(prob, n, p)` calcola **il numero minimo di successi** necessari per raggiungere almeno quella probabilit√†.

In sintesi, `pbinom()` e `qbinom()` sono strumenti complementari: `pbinom` ci d√† la probabilit√† di ottenere fino a un certo numero di successi, mentre `qbinom` ci dice fino a quanti successi possiamo ottenere per raggiungere una certa probabilit√†. Nell‚Äôanalisi di una distribuzione binomiale (e di molte altre distribuzioni) queste funzioni aiutano a calcolare e interpretare facilmente probabilit√† cumulate e quantili in R, rendendo pi√π semplice l‚Äôanalisi di eventi aleatori.
:::

### Valore atteso e deviazione standard nella distribuzione binomiale 

Nella distribuzione binomiale, possiamo calcolare facilmente due quantit√† molto importanti:

- **il valore atteso** (o media), che ci dice **quanti successi ci aspettiamo in media** su un certo numero di prove;
- **la deviazione standard**, che ci dice **quanto i risultati tendono a variare** attorno alla media.

Le formule sono le seguenti:

$$
\text{Media (valore atteso):} \quad \mu = n p ,
$$ {#eq-binom-distr-expval}

$$
\text{Deviazione standard:} \quad \sigma = \sqrt{n p (1 - p)} ,
$$ {#eq-binom-distr-var}

dove:

- $n$ √® il numero di prove (per esempio, il numero di lanci di una moneta),
- $p$ √® la probabilit√† di successo in ogni prova.

::: callout-note
**Dimostrazione.**

La variabile $Y$ rappresenta il numero di successi in $n$ prove di Bernoulli indipendenti. Possiamo scriverla come somma di $n$ variabili casuali indipendenti:

$$
Y = Y_1 + Y_2 + \cdots + Y_n,
$$

dove ciascuna $Y_i \sim \text{Bernoulli}(p)$, cio√®:

$$
Y_i =
\begin{cases}
1 & \text{con probabilit√† } p \\
0 & \text{con probabilit√† } 1 - p
\end{cases}
$$

**Valore atteso di $Y_i$.**

Per definizione del valore atteso:

$$
\mathbb{E}(Y_i) = 1 \cdot p + 0 \cdot (1 - p) = p.
$$

**Valore atteso di $Y_i^2$.**

Poich√© $Y_i$ assume solo i valori 0 e 1, si ha $Y_i^2 = Y_i$. Quindi:

$$
\mathbb{E}(Y_i^2) = \mathbb{E}(Y_i) = p.
$$

La **varianza** di una variabile casuale si definisce come:

$$
\operatorname{Var}(Y_i) = \mathbb{E}(Y_i^2) - [\mathbb{E}(Y_i)]^2.
$$

Sostituendo i valori trovati sopra:

$$
\operatorname{Var}(Y_i) = p - p^2 = p(1 - p).
$$

Ricordiamo che $Y = \sum_{i=1}^{n} Y_i$ e che le $Y_i$ sono **indipendenti**. Una propriet√† fondamentale della varianza √® che se $Z_1, \dots, Z_n$ sono indipendenti:

$$
\operatorname{Var}(Z_1 + \cdots + Z_n) = \operatorname{Var}(Z_1) + \cdots + \operatorname{Var}(Z_n).
$$

Applichiamola al nostro caso:

$$
\operatorname{Var}(Y) = \sum_{i=1}^{n} \operatorname{Var}(Y_i).
$$

Poich√© tutte le $Y_i$ hanno la stessa varianza $p(1 - p)$, la somma diventa:

$$
\operatorname{Var}(Y) = n \cdot p(1 - p).
$$

Abbiamo dimostrato da definizione che, se $Y \sim \text{Bin}(n, p)$, allora:

$$
\operatorname{Var}(Y) = n \cdot p \cdot (1 - p).
$$

Questa formula descrive la dispersione attesa nel numero di successi su $n$ prove indipendenti, ciascuna con probabilit√† di successo $p$.
:::

::: {#exm-binom-var}
Supponiamo di lanciare **4 volte una moneta truccata** che ha una probabilit√† di successo (es. ottenere *testa*) pari a $p = 0.2$.

Vogliamo calcolare:

- la media attesa del numero di teste,
- la varianza,
- e la deviazione standard.

1. Calcolo del valore atteso (media):

$$
\mu = n \cdot p = 4 \cdot 0.2 = 0.8 .
$$

Quindi, **in media**, ci aspettiamo di ottenere **0.8 teste ogni 4 lanci** (cio√® meno di 1, ma ricordiamo che si tratta di una **media**).

2. Calcolo della varianza:

$$
\text{Varianza} = n \cdot p \cdot (1 - p) = 4 \cdot 0.2 \cdot 0.8 = 0.64 .
$$

3. Calcolo della deviazione standard:

$$
\sigma = \sqrt{0.64} \approx 0.8 .
$$

La deviazione standard ci d√† un'idea della **variabilit√†** dei risultati: in questo caso, i valori osservati (numero di teste su 4 lanci) si discostano dalla media di circa 0.8 in media.
:::

### Verifica con una simulazione in R

Per vedere se i calcoli teorici dell'@exm-binom-var funzionano anche nella pratica, possiamo **simulare** l‚Äôesperimento in R: lanciamo 4 monete, ma lo facciamo **tantissime volte** (ad esempio 1 milione) e calcoliamo la media e la varianza dei risultati ottenuti.

```{r}
set.seed(42)

# Generiamo 1 milione di esperimenti: 4 lanci con probabilit√† di successo 0.2
x <- rbinom(n = 1e6, size = 4, prob = 0.2)

# Calcoliamo la media empirica
mean(x)
# [1] circa 0.8

# Calcoliamo la varianza empirica
var(x)
# [1] circa 0.64
```

Come possiamo vedere, i risultati ottenuti dalla simulazione sono **molto vicini ai valori teorici**: la media √® circa $\mu = 0.8$ e la varianza circa $0.64$, proprio come previsto dalle formule.

Questo non solo conferma che le **formule per media e varianza nella distribuzione binomiale sono corrette**, ma ci aiuta anche a capire meglio **cosa significano**: 

- il **valore atteso** rappresenta la **media dei risultati** se ripetiamo l'esperimento moltissime volte;  
- la **varianza** (e la sua radice quadrata, la **deviazione standard**) misura **quanto i risultati si allontanano dalla media**.

La simulazione mostra quindi in modo concreto che **il valore atteso e la varianza descrivono il comportamento "medio" della variabile aleatoria**, quando viene osservata in un numero molto grande di situazioni. In altre parole, questi concetti non sono solo teorici: ci dicono cosa aspettarci nella pratica, se ripetiamo molte volte lo stesso esperimento.


## Funzioni R per le distribuzioni di probabilit√†

In R, le distribuzioni di probabilit√† (sia discrete che continue) sono gestite in modo sistematico. Per ogni distribuzione, esistono quattro funzioni principali, ognuna con un prefisso diverso che indica il tipo di operazione desiderata:

- **`d*`**: calcola la *densit√†* (per distribuzioni continue) o la *probabilit√†* (per distribuzioni discrete);  
- **`p*`**: calcola la *funzione di ripartizione cumulativa* (CDF), cio√® $P(Y \leq y)$;  
- **`q*`**: calcola la *funzione quantile* (inversa della CDF);  
- **`r*`**: genera *valori casuali* secondo la distribuzione specificata.

Questa struttura √® identica per tutte le distribuzioni implementate in R. La tabella seguente mostra un confronto tra le funzioni disponibili per due distribuzioni fondamentali: la **binomiale** (discreta) e la **normale** (continua).

| Tipo di funzione                   | Binomiale ($Y \sim \text{Bin}(n, p)$)         | Normale ($Y \sim \mathcal{N}(\mu, \sigma)$)         |
|:----------------------------------|:----------------------------------------------|:--------------------------------------------------|
| Densit√† o probabilit√† esatta      | `dbinom(y, size = n, prob = p)`               | `dnorm(y, mean = mu, sd = sigma)`                 |
| $P(Y = y)$                        | `dbinom(...)`                                 | ‚ùå Non definita: per variabili continue si usa la densit√† |
| Probabilit√† cumulativa            | `pbinom(y, size = n, prob = p)`               | `pnorm(y, mean = mu, sd = sigma)`                |
| $P(Y \geq y)$                     | `1 - pbinom(y - 1, ...)`                       | `1 - pnorm(y, ...)`                               |
| $P(y_1 < Y < y_2)$                | `pbinom(y2, ...) - pbinom(y1, ...)`            | `pnorm(y2, ...) - pnorm(y1, ...)`                |
| Quantile (inversa della CDF)      | `qbinom(q, size = n, prob = p)`               | `qnorm(q, mean = mu, sd = sigma)`                |
| Simulazione di dati casuali       | `rbinom(n, size = trials, prob = p)`          | `rnorm(n, mean = mu, sd = sigma)`                |

::: {.callout-tip title="Nota"}
- Per le **distribuzioni discrete** (come la binomiale), `dbinom(y, ...)` restituisce la **probabilit√† esatta** di osservare il valore $y$: ad esempio, $P(Y = 2)$.
- Per le **distribuzioni continue** (come la normale), `dnorm(y, ...)` restituisce la **densit√†** in $y$, che non rappresenta direttamente una probabilit√†, ma √® utile per visualizzare la forma della distribuzione.
- Le probabilit√† cumulative (funzioni `p*`) e i quantili (funzioni `q*`) sono sempre definiti, sia per distribuzioni discrete che continue.
- La generazione di dati casuali con `r*` √® molto utile per simulazioni e verifiche empiriche.

Pi√π avanti, vedremo altre distribuzioni (Uniforme, Beta, Poisson, ecc.), tutte con lo stesso schema di funzioni: `d`, `p`, `q`, `r`.

Questa coerenza rende molto semplice imparare a usare le distribuzioni in R: una volta compreso lo schema, lo si pu√≤ applicare a qualsiasi caso.
:::

::: {#exm-}

1. Calcolare la probabilit√† di esattamente $y = 3$ successi su $n = 5$ prove con $p = 0.5$:

```{r}
dbinom(3, size = 5, prob = 0.5)
```

2. Calcolare la probabilit√† cumulativa $P(Y \leq 3)$:
   
```{r}
pbinom(3, size = 5, prob = 0.5)
```

3. Calcolare il valore minimo $y$ tale che $P(Y \leq y) \geq 0.9$:

```{r}
qbinom(0.9, size = 5, prob = 0.5)
```

4. Generare un campione di 100 numeri casuali da una distribuzione binomiale:

```{r}
rbinom(100, size = 5, prob = 0.5)
```
:::


## Distribuzione di Poisson 

La **distribuzione di Poisson** √® utilizzata per modellare il numero di eventi che si verificano in un determinato intervallo di tempo o spazio, con eventi indipendenti e un tasso costante di occorrenza.

La funzione di massa di probabilit√† (PMF) √® data da:

$$
P(Y = y \mid \lambda) = \frac{\lambda^y \cdot e^{-\lambda}}{y!}, \quad y = 0, 1, 2, \ldots
$$

dove $\lambda$ rappresenta il tasso medio di eventi e $y$ √® il numero di eventi.

La distribuzione di Poisson pu√≤ essere derivata come il limite di una distribuzione binomiale quando il numero di prove, $n$, tende all'infinito e la probabilit√† di successo in ciascuna prova, $p$, tende a zero, in modo tale che $np = \lambda$. 

::: callout-note
## Dimostrazione

Partiamo dalla funzione di probabilit√† binomiale:

$$
p(k) = \frac{n!}{k!(n - k)!} p^k (1 - p)^{n - k}.
$$

Impostiamo $np = \lambda$, il che implica che $p = \frac{\lambda}{n}$. Sostituendo $p$ con $\frac{\lambda}{n}$ nella formula binomiale, otteniamo:

$$
p(k) = \frac{n!}{k!(n - k)!} \left(\frac{\lambda}{n}\right)^k \left(1 - \frac{\lambda}{n}\right)^{n - k}.
$$

Ora, separiamo i termini per rendere pi√π chiara la semplificazione. Possiamo riscrivere $\left(\frac{\lambda}{n}\right)^k$ come $\frac{\lambda^k}{n^k}$, e $\left(1 - \frac{\lambda}{n}\right)^{n - k}$ come $\left(1 - \frac{\lambda}{n}\right)^n \cdot \left(1 - \frac{\lambda}{n}\right)^{-k}$. Quindi, l'espressione diventa:

$$
p(k) = \frac{n!}{k!(n - k)!} \cdot \frac{\lambda^k}{n^k} \cdot \left(1 - \frac{\lambda}{n}\right)^n \cdot \left(1 - \frac{\lambda}{n}\right)^{-k}.
$$

Ora, separiamo ulteriormente i termini:

$$
p(k) = \frac{\lambda^k}{k!} \cdot \frac{n!}{(n - k)! n^k} \cdot \left(1 - \frac{\lambda}{n}\right)^n \cdot \left(1 - \frac{\lambda}{n}\right)^{-k}.
$$

Questo passaggio mostra come la funzione di probabilit√† binomiale, sotto le condizioni $np = \lambda$ e $n \to \infty$, si trasformi gradualmente nella forma che conduce alla distribuzione di Poisson.

Quando $n \to \infty$:

$$
\frac{\lambda}{n} \to 0
$$

$$
\frac{n!}{(n - k)! n^k} \to 1
$$

$$
\left(1 - \frac{\lambda}{n}\right)^n \to e^{-\lambda}
$$

$$
\left(1 - \frac{\lambda}{n}\right)^{-k} \to 1
$$

Si ottiene quindi:

$$
p(k) \to \frac{\lambda^k e^{-\lambda}}{k!}
$$

che √® la funzione di Poisson.
:::

::: callout-note
## Dimostrazione

Analizziamo il limite:

$$
\left(1 - \frac{\lambda}{n}\right)^n \to e^{-\lambda} \quad \text{quando} \quad n \to \infty.
$$

Un limite fondamentale in analisi matematica √®:

$$
\lim_{n \to \infty} \left(1 + \frac{a}{n}\right)^n = e^a,
$$

dove $e$ √® la base del logaritmo naturale ($e \approx 2.71828$) e $a$ √® una costante. Questo limite √® alla base della definizione della funzione esponenziale.

Nel nostro caso, abbiamo l'espressione:

$$
\left(1 - \frac{\lambda}{n}\right)^n .
$$

Notiamo che questa √® molto simile al limite notevole, ma con un segno negativo. Possiamo riscriverla come:

$$
\left(1 - \frac{\lambda}{n}\right)^n = \left(1 + \frac{-\lambda}{n}\right)^n.
$$

Applicando il limite notevole con $a = -\lambda$, otteniamo:

$$
\lim_{n \to \infty} \left(1 + \frac{-\lambda}{n}\right)^n = e^{-\lambda}.
$$

Quindi, quando $n$ diventa molto grande, l'espressione $\left(1 - \frac{\lambda}{n}\right)^n$ si avvicina sempre di pi√π a $e^{-\lambda}$. 
:::

### Propriet√† principali

- **Media**: $\mathbb{E}[Y] = \lambda$
- **Varianza**: $\text{Var}(Y) = \lambda$

Di seguito, presentiamo esempi di calcolo e simulazione con R.


### Grafico della distribuzione di Poisson con $\lambda = 2$

```{r}
# Parametro lambda
lambda <- 2

# Valori di y (numero di eventi)
y <- 0:10

# Calcolo delle probabilit√†
probabilities <- dpois(y, lambda = lambda)

# Creazione di un dataframe per ggplot
data <- data.frame(
  Numero_eventi = y,
  Probabilita = probabilities
)

# Grafico della funzione di massa di probabilit√† 
ggplot(data, aes(x = Numero_eventi, y = Probabilita)) +
  geom_col() +  
  labs(
    x = "Numero di eventi (k)",
    y = "Probabilit√†"
  ) 
```

### Calcolo della probabilit√† per un numero specifico di eventi

Per calcolare la probabilit√† di osservare esattamente 3 eventi con $\lambda = 2$:

```{r}
prob <- dpois(3, lambda = 2)
print(prob)
```

### Calcolo della probabilit√† cumulativa $P(Y \leq 3)$

Per calcolare $P(Y \leq 3)$, la probabilit√† cumulativa:

```{r}
cum_prob <- ppois(3, lambda = 2)
print(cum_prob)
```

### Trovare il quantile corrispondente a una probabilit√† data

Per trovare il numero massimo di eventi per cui la probabilit√† cumulativa √® al massimo $0.8125$:

```{r}
quantile <- qpois(0.8125, lambda = 2)
print(quantile)
```

### Generazione di numeri casuali

Per generare un campione di 1.000.000 di osservazioni da una distribuzione di Poisson con $\lambda = 2$:

```{r}
set.seed(42)
sample <- rpois(1000000, lambda = 2)

# Calcolo di media e varianza del campione
mean_sample <- mean(sample)
var_sample <- var(sample)

print(mean_sample)
print(var_sample)
```



::: {#exm-poisson-v2-london}

Un esempio classico dell'uso della distribuzione di Poisson viene dalla **Seconda Guerra Mondiale**.

**Il contesto storico.** Tra il 1944 e il 1945, Londra fu colpita da centinaia di **missili V1 e V2** lanciati dalla Germania nazista. Le autorit√† britanniche si chiesero se i bombardamenti seguissero una **strategia mirata**: i missili venivano forse lanciati intenzionalmente su certi quartieri? O si trattava invece di **un comportamento casuale**, come se fossero stati distribuiti a caso?

Per rispondere a questa domanda, il Ministero della Guerra britannico **divise Londra in 576 aree di uguale superficie** (ogni area misurava 0.25 km¬≤) e **registr√≤ quanti missili avevano colpito ciascuna area**. I dati furono poi analizzati dal matematico R. D. Clarke, che li pubblic√≤ nel 1946.

**I dati osservati.** Ecco una sintesi della distribuzione osservata:

| Missili per area | Numero di aree | Frequenza relativa |
|------------------|----------------|---------------------|
| 0                | 229            | 0.398               |
| 1                | 211            | 0.367               |
| 2                | 93             | 0.161               |
| 3                | 35             | 0.061               |
| 4                | 7              | 0.012               |
| ‚â•5               | 1              | 0.002               |

Il **numero medio di missili per area** era $\lambda \approx 0.93$. L'idea era confrontare queste frequenze con le probabilit√† teoriche previste da una distribuzione di Poisson con media $\lambda = 0.93$.

**Interpretazione con la distribuzione di Poisson.** Utilizzando la funzione `dpois()` in R, possiamo calcolare le probabilit√† teoriche per ciascun valore osservato, da 0 a 4 missili per area (valori superiori sono troppo rari per essere trattati separatamente).

```{r}
# Parametro medio osservato
lambda <- 0.93

# Valori possibili di missili per area
y <- 0:4

# Probabilit√† teoriche secondo la distribuzione di Poisson
prob_teoriche <- dpois(y, lambda = lambda)

# Aggiungiamo la probabilit√† per y >= 5
prob_teoriche <- c(prob_teoriche, 1 - sum(prob_teoriche))  # y >= 5

# Visualizziamo
data.frame(
  Missili_per_area = c(0:4, ">=5"),
  Probabilita_teorica = round(prob_teoriche, 3)
)
```

Confrontando le probabilit√† teoriche della Poisson con quelle osservate nei dati reali, **i risultati erano sorprendentemente simili**. Questo suggeriva che **i missili non erano lanciati su bersagli specifici**, ma seguivano un comportamento **statisticamente compatibile con una distribuzione casuale**.

```{r}
# Frequenze osservate (dati originali di Clarke, 1946)
frequenze_osservate <- c(229, 211, 93, 35, 7, 1)
valori_missili <- c(0, 1, 2, 3, 4, "‚â•5")

# Calcolo frequenze teoriche con Poisson (lambda = 0.93)
lambda <- 0.93
prob_teoriche <- dpois(0:4, lambda)
prob_teoriche <- c(prob_teoriche, 1 - sum(prob_teoriche))  # Per y >= 5

# Numero totale di aree (come somma delle osservazioni)
n_aree <- sum(frequenze_osservate)

# Frequenze attese = probabilit√† teoriche * numero totale di aree
frequenze_attese <- round(prob_teoriche * n_aree)

# Costruzione del data frame
df <- data.frame(
  Missili_per_area = factor(valori_missili, levels = c("0", "1", "2", "3", "4", "‚â•5")),
  Osservate = frequenze_osservate,
  Attese = frequenze_attese
)

# Conversione in formato lungo per ggplot2
df_long <- reshape2::melt(df, id.vars = "Missili_per_area", variable.name = "Tipo", value.name = "Frequenza")

# Creazione del grafico
ggplot(df_long, aes(x = Missili_per_area, y = Frequenza, fill = Tipo)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    x = "Numero di missili per area",
    y = "Numero di aree",
    fill = "Frequenza"
  ) 
```

- Le **barre blu** rappresentano le **frequenze osservate** (quante aree hanno ricevuto 0, 1, 2... missili).
- Le **barre rosse** mostrano le **frequenze attese** se i missili fossero stati lanciati in modo **completamente casuale**, seguendo una distribuzione di **Poisson con $\lambda = 0.93$**.

La sovrapposizione tra i due andamenti √® molto buona, il che rafforza l‚Äôidea che i bombardamenti fossero distribuiti casualmente ‚Äî **senza un pattern strategico** apparente.

**Cosa ci insegna questo esempio?**

- La distribuzione di Poisson √® adatta quando vogliamo **modellare eventi rari e indipendenti** nello spazio o nel tempo.
- I dati dei missili su Londra mostrano come un fenomeno che a prima vista potrebbe sembrare non casuale (per via della concentrazione locale degli eventi) possa invece essere **ben descritto da un modello probabilistico semplice**, se considerato su una scala adatta.
:::

:::{#exm-}
Supponiamo di avere osservato, nel corso degli anni, che la **frequenza relativa di bocciature** all‚Äôesame di **Psicometria** √® di circa **10%** (cio√® $p = 0.1$). Tuttavia, il **numero di studenti iscritti a ciascun appello varia in modo estremo**: al primo appello dell‚Äôanno partecipano pi√π di 200 studenti, mentre negli ultimi appelli solo 2 o 3.

Questo rende **inadeguato l‚Äôuso della distribuzione binomiale**, che richiede un numero di prove ($n$) fisso o noto per ciascun appello.

In questi casi, possiamo **modellare il numero di bocciature per appello** usando una distribuzione di Poisson.

Per ogni appello, possiamo stimare $\lambda$ moltiplicando il **numero di studenti iscritti** ($n$) per la **frequenza attesa di bocciature** ($p = 0.1$). A quel punto, il numero di bocciature osservate pu√≤ essere approssimato da:

$$
Y \sim \text{Poisson}(\lambda = n \cdot p) .
$$

Quindi la distribuzione cambia da appello ad appello, perch√© **$\lambda$ cambia con $n$**, ma il **modello rimane Poissoniano**.

Supponiamo di avere osservato i seguenti dati.

| Appello | Numero iscritti ($n$) | $\lambda = n \cdot p$ | Distribuzione di bocciature |
|--------:|-----------------------:|-----------------------:|-----------------------------|
| 1       | 220                   | $220 \cdot 0.1 = 22$   | $Y \sim \text{Poisson}(22)$ |
| 2       | 95                    | $95 \cdot 0.1 = 9.5$    | $Y \sim \text{Poisson}(9.5)$ |
| 8       | 3                     | $3 \cdot 0.1 = 0.3$     | $Y \sim \text{Poisson}(0.3)$ |

Per ogni appello, possiamo usare la funzione `dpois()` in R per calcolare la probabilit√† di osservare un certo numero di bocciature, dato il valore di $\lambda$ specifico per quell‚Äôappello.

Ad esempio, possiamo chiederci quale sia la probabilit√† che, nel secondo appello (95 iscritti), si registrino esattamente 8 bocciature.

```{r}
lambda <- 95 * 0.1  # = 9.5
dpois(8, lambda = lambda)
```

Questa funzione calcola $P(Y = 8)$ per una variabile $Y \sim \text{Poisson}(9.5)$, cio√® la probabilit√† di osservare esattamente 8 bocciature su 95 iscritti.

Supponiamo di voler simulare il numero di bocciature in **8 appelli** con numeri di iscritti variabili. Possiamo fare cos√¨:

```{r}
set.seed(42)

# Numero iscritti per ciascun appello
n_iscritti <- c(220, 95, 60, 45, 20, 12, 6, 3)

# Probabilit√† storica di bocciatura
p <- 0.1

# Parametri lambda per ogni appello
lambda <- n_iscritti * p

# Simulazione delle bocciature per ciascun appello
bocciature <- rpois(length(lambda), lambda = lambda)

data.frame(
  Appello = 1:8, 
  Iscritti = n_iscritti, 
  Lambda = lambda, 
  Bocciature = bocciature
)
```

üìà **Visualizzazione.**

```{r}
df <- data.frame(Appello = factor(1:8), Bocciature = bocciature)

ggplot(df, aes(x = Appello, y = Bocciature)) +
  geom_col() +
  labs(
    x = "Appello",
    y = "Numero di bocciature"
  )
```

In sintesi,

- quando il numero di studenti iscritti a un appello **non √® noto a priori** o **varia fortemente**, non √® adeguato usare la distribuzione binomiale;
- se conosciamo la **frequenza relativa di bocciature** (es. $p = 0.1$), possiamo usare la distribuzione di **Poisson con $\lambda = n \cdot p$**, adattandola a ciascun appello;
- questo approccio √® particolarmente utile per fare **stima e simulazione** del numero di bocciature attese, senza dover modellare tutti i singoli esiti.
:::

::: {#exm-}
Uno degli esempi pi√π comuni per introdurre la distribuzione di Poisson riguarda il numero di **nascite giornaliere** in un ospedale.

Supponiamo che, in un grande ospedale, la **media storica** sia di **4.5 nascite al giorno**. Possiamo allora descrivere il numero di nascite in un giorno con una **variabile casuale Poisson** con parametro $\lambda = 4.5$:

$$
Y \sim \text{Poisson}(\lambda = 4.5) .
$$

Ci chiediamo, ad esempio: qual √® la probabilit√† che in un giorno nascano **esattamente 6 bambini**?

Possiamo calcolarla con la funzione `dpois()`:

```{r}
# Parametro medio: 4.5 nascite al giorno
lambda <- 4.5

# Probabilit√† di osservare esattamente 6 nascite
prob <- dpois(6, lambda = lambda)
print(prob)
```

Questo valore rappresenta la probabilit√† che, in un giorno qualsiasi, si verifichino **esattamente 6 nascite**.

**Simulazione.** Simuliamo ora il numero di nascite in **365 giorni consecutivi**, supponendo che la media rimanga costante a 4.5:

```{r}
set.seed(42)  # Per rendere i risultati riproducibili

n_days <- 365
simulated_births <- rpois(n_days, lambda = lambda)

# Proporzione di giorni con esattamente 6 nascite
proportion_six_births <- mean(simulated_births == 6)
print(proportion_six_births)
```

Questo ci dice, tra i 365 giorni simulati, **quanta parte dell‚Äôanno ha avuto esattamente 6 nascite**. Il valore ottenuto pu√≤ essere confrontato con la probabilit√† teorica calcolata prima.

**Visualizzazione.** Possiamo rappresentiamo graficamente i dati simulati con un istogramma:

```{r}
# Costruzione del data frame
data <- data.frame(Nascite = simulated_births)

# Istogramma
ggplot(data, aes(x = Nascite)) +
  geom_histogram(
    breaks = seq(-0.5, max(simulated_births) + 0.5, by = 1)
  ) +
  labs(
    x = "Numero di nascite per giorno",
    y = "Frequenza (numero di giorni)"
  )
```

L‚Äôistogramma mostra quante volte si sono verificati 0, 1, 2, ..., 10 o pi√π nascite in un giorno, evidenziando la **variabilit√† naturale** attorno alla media.

Calcoliamo ora quanto √® probabile che si verifichino **pi√π di 6 nascite in un giorno**.

Probabilit√† teorica:

```{r}
prob_more_than_six <- 1 - ppois(6, lambda = lambda)
print(prob_more_than_six)
```

Proporzione osservata nella simulazione:

```{r}
proportion_more_than_six <- mean(simulated_births > 6)
print(proportion_more_than_six)
```

Il confronto tra probabilit√† teorica e proporzione simulata mostra come la distribuzione di Poisson **riproduca bene i fenomeni reali**, quando gli eventi sono **indipendenti**, **discreti** e **relativamente frequenti ma non troppo**.
:::

::: {#exm-}
Questo esempio √® tratto dal celebre lavoro di **Ladislaus von Bortkiewicz** del 1898, spesso citato come una delle prime applicazioni reali della **distribuzione di Poisson**.

Von Bortkiewicz studi√≤ un evento piuttosto inusuale: le **morti causate da calci di cavallo** all‚Äôinterno della **cavalleria dell‚Äôesercito prussiano**. L'obiettivo era capire se questi eventi, seppur rari, potessero essere considerati **casuali e indipendenti**, oppure se fossero distribuiti in modo irregolare e non prevedibile.

Per farlo, raccolse i dati su **10 squadroni** osservati per **20 anni consecutivi**, ottenendo cos√¨ **200 unit√† di osservazione**, che possiamo chiamare **"squadroni-anno"**.

**I dati raccolti.** Per ogni squadrone-anno, fu registrato il **numero di morti per calci di cavallo**. I dati furono poi raggruppati per numero di decessi:

| Numero di decessi annui | Frequenza osservata | Frequenza relativa | Probabilit√† teorica (Poisson) |
|-------------------------|---------------------|--------------------|-------------------------------|
| 0                       | 109                 | 0.545              | 0.543                         |
| 1                       | 65                  | 0.325              | 0.331                         |
| 2                       | 22                  | 0.110              | 0.101                         |
| 3                       | 3                   | 0.015              | 0.021                         |
| 4                       | 1                   | 0.005              | 0.003                         |

- **Frequenza osservata**: Quante volte ciascun numero di decessi √® stato osservato tra i 200 squadroni-anno.
- **Frequenza relativa**: Frequenza osservata divisa per 200.
- **Probabilit√† teorica**: Calcolata con la **distribuzione di Poisson con parametro $\lambda = 0.61$**, pari alla media osservata dei decessi annui.

La distribuzione di Poisson √® perfetta per questo tipo di situazione perch√©:

- stiamo contando il numero di **eventi rari** (decessi accidentali),
- che si verificano in **unit√† di tempo o spazio fisse** (lo "squadrone-anno"),
- e presumiamo che questi eventi siano **indipendenti tra loro**.

In questo caso, $\lambda = 0.61$ rappresenta il **numero medio di decessi per squadrone in un anno**. La variabilit√† intorno a questo valore pu√≤ essere descritta dalla distribuzione di Poisson, che assegna a ciascun possibile numero di decessi (0, 1, 2, ‚Ä¶) una **probabilit√† teorica**.

**Confronto tra dati osservati e modello di Poisson.** Come si pu√≤ notare dalla tabella, **le frequenze osservate sono sorprendentemente simili** alle probabilit√† teoriche ottenute dal modello di Poisson. Ad esempio:

- la proporzione di squadroni-anno con **zero decessi** √® 0.545, contro una probabilit√† teorica di 0.543;
- per **un decesso**, la frequenza relativa √® 0.325, vicina alla probabilit√† teorica di 0.331;
- anche le classi meno frequenti (2, 3 e 4 decessi) sono coerenti con i valori attesi.

Questo esempio dimostra che la distribuzione di Poisson **non solo √® utile per modellare eventi rari**, ma fornisce anche una buona descrizione **quantitativa** del comportamento osservato nel mondo reale.

In sintesi,

- il lavoro di von Bortkiewicz √® uno dei primi esempi storici di **modellizzazione di dati reali con la teoria delle probabilit√†**;
- la distribuzione di Poisson si √® rivelata efficace nel **descrivere un fenomeno raro, ma regolare**, suggerendo che i decessi fossero eventi **casuali e indipendenti**, non dovuti a fattori sistematici;
- ancora oggi, questo esempio viene usato per insegnare che anche gli eventi accidentali e poco frequenti possono essere **prevedibili in media** e descritti in modo elegante da un modello probabilistico.

Qui di seguito viene fornito il **codice R** che riproduce l‚Äôanalisi di von Bortkiewicz, calcola le **probabilit√† teoriche** secondo la distribuzione di Poisson con parametro $\lambda = 0.61$ e confronta visivamente le **frequenze osservate** con le **frequenze attese**.

```{r}
# Dati osservati da von Bortkiewicz
decessi <- 0:4
frequenze_osservate <- c(109, 65, 22, 3, 1)
n_total <- sum(frequenze_osservate)  # Totale = 200 squadroni-anno

# Frequenze relative
frequenze_relative <- frequenze_osservate / n_total
```

Calcolo delle probabilit√† teoriche con la distribuzione di Poisson:

```{r}
# Parametro medio osservato
lambda <- 0.61

# Calcolo delle probabilit√† teoriche di Poisson
prob_poisson <- dpois(decessi, lambda = lambda)
```

Confronto: osservato vs teorico.

```{r}
# Frequenze attese = probabilit√† teoriche * numero totale di casi
frequenze_attese <- round(prob_poisson * n_total)

# Creazione del data frame per il confronto
df <- data.frame(
  Decessi = factor(decessi),
  Osservato = frequenze_osservate,
  Atteso = frequenze_attese
)
df
```

Visualizzazione: confronto tra frequenze osservate e attese.

```{r}
# Conversione da wide a long format con pivot_longer()
df_long <- df |> 
  pivot_longer(
    cols = c(Osservato, Atteso),
    names_to = "Tipo",
    values_to = "Frequenza"
  )

# Mostra le prime righe
head(df_long)
```

```{r}
# Grafico a barre affiancate
ggplot(df_long, aes(x = Decessi, y = Frequenza, fill = Tipo)) +
  geom_bar(stat = "identity", position = "dodge", color = "black") +
  labs(
    x = "Numero di decessi per squadrone-anno",
    y = "Frequenza",
    fill = "Tipo"
  ) 
```

- Le **barre blu** mostrano i dati **osservati** da von Bortkiewicz.
- Le **barre rosse** indicano le **frequenze attese** se il numero di decessi segue una distribuzione di Poisson con media $\lambda = 0.61$.
- La **buona corrispondenza visiva** tra le due serie supporta l‚Äôidea che i decessi siano **eventi rari, indipendenti e distribuiti casualmente**.

:::

## Distribuzione Beta-Binomiale

La distribuzione beta-binomiale rappresenta una estensione della distribuzione binomiale che tiene conto della variabilit√† nella probabilit√† di successo tra i vari tentativi. Viene descritta da tre parametri principali: $N$, $\alpha$ e $\beta$.

Nel dettaglio, la funzione di massa di probabilit√† per la distribuzione beta-binomiale √® data da:

$$
\text{BetaBinomiale}(y | N, \alpha, \beta) = \binom{N}{y} \cdot \frac{B(y + \alpha, N - y + \beta)}{B(\alpha, \beta)},
$$ {#eq-beta-binom-formula}

dove:

- $y$ indica il numero di successi osservati.
- $N$ rappresenta il numero totale di tentativi.
- $\alpha$ e $\beta$ sono i parametri della distribuzione beta, che modellano la variabilit√† nella probabilit√† di successo tra i tentativi.

La funzione $B(u, v)$, nota come funzione beta, √® definita tramite l'uso della funzione gamma $\Gamma$, secondo la formula:

$$
B(u, v) = \frac{\Gamma(u) \Gamma(v)}{\Gamma(u + v)},
$$

dove la funzione gamma $\Gamma$ generalizza il concetto di fattoriale a numeri reali e complessi.

L'importanza della distribuzione beta-binomiale deriva dalla sua capacit√† di modellare situazioni in cui la probabilit√† di successo non √® fissa, ma segue una distribuzione di probabilit√†, specificatamente una distribuzione beta. Ci√≤ la rende particolarmente adatta per applicazioni in cui le probabilit√† di successo cambiano in maniera incerta da un tentativo all'altro, come pu√≤ avvenire in contesti di ricerca clinica o in studi comportamentali. Rispetto alla distribuzione binomiale, che assume una probabilit√† di successo costante per tutti i tentativi, la beta-binomiale offre una rappresentazione pi√π realistica e flessibile per dati empirici che presentano variabilit√† nelle probabilit√† di successo.

## Riflessioni Conclusive

In questo capitolo, abbiamo approfondito alcune delle distribuzioni discrete pi√π importanti, ognuna con caratteristiche uniche e campi di applicazione specifici. Abbiamo iniziato con la **distribuzione di Bernoulli**, che modella esperimenti con due soli esiti possibili, per poi passare alla **distribuzione Binomiale**, che generalizza la Bernoulli considerando un numero fisso di prove indipendenti. Successivamente, abbiamo esaminato la **distribuzione di Poisson**, utile per descrivere eventi rari in un intervallo di tempo o spazio, e la **distribuzione Beta-Binomiale**, un'estensione della Binomiale che incorpora la variabilit√† nella probabilit√† di successo, rendendola particolarmente adatta per modellare situazioni in cui tale probabilit√† non √® fissa. Infine, abbiamo discusso la **distribuzione Discreta Uniforme**, che assegna la stessa probabilit√† a ciascun evento in un insieme finito e discreto.

Queste distribuzioni rappresentano il fondamento dell'analisi statistica discreta e trovano applicazione in numerosi ambiti. In particolare, nel contesto dell'**inferenza bayesiana**, la comprensione della distribuzione Binomiale e della sua estensione Beta-Binomiale √® essenziale. Queste distribuzioni, infatti, forniscono gli strumenti necessari per l'**aggiornamento bayesiano**, un processo chiave che permette di rivedere le nostre credenze iniziali alla luce di nuovi dati. Questo concetto sar√† ulteriormente esplorato nei capitoli successivi, dove approfondiremo come le distribuzioni a priori e a posteriori interagiscono nel quadro bayesiano.

## Esercitazione in Classe

Valutate le emozioni che verranno presentate sullo schermo usando questo [link](https://docs.google.com/forms/d/e/1FAIpQLScWZD9XQoWJvf58fvQ6KGPJ562JxiKeg_azCIWagi1_BVtdpg/viewform?usp=header).

Scala di risposta:

- Rabbia: 1
- Disgusto: 2
- Paura: 3
- Felicit√†: 4
- Tristezza: 5


## Esercizi {.unnumbered} 

::: {.callout-important title="Problemi 1" collapse="true"}
Per ciascuna delle distribuzioni di massa di probabilit√† discusse, utilizzare R per:

- creare un grafico della funzione, scegliendo opportunamente i parametri;
- estrarre un campione di 1000 valori casuali dalla distribuzione e visualizzarlo con un istogramma;
- calcolare la media e la deviazione standard dei campioni e confrontarle con i valori teorici attesi;
- stimare l'intervallo centrale del 94% utilizzando i campioni simulati;
- determinare i quantili della distribuzione per gli ordini 0.05, 0.25, 0.75 e 0.95;
- scegliendo un valore della distribuzione pari alla media pi√π una deviazione standard, calcolare la probabilit√† che la variabile aleatoria assuma un valore minore o uguale a questo valore.
:::

::: {.callout-important title="Problemi 2" collapse="true"}
Esercizi sulla distribuzione binomiale, risolvibili usando R, sono disponibili sulla seguente [pagina web](https://mathcenter.oxford.emory.edu/site/math117/probSetBinomialProbabilities/). 
:::

## Informazioni sull'Ambiente di Sviluppo {.unnumbered} 

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}

