{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catene di Markov {#sec-appendix-markov-first-order}\n",
    "\n",
    "Il processo di Markov di primo ordine è un concetto fondamentale in molti campi, tra cui l'intelligenza artificiale, la statistica e la teoria delle probabilità. Questo modello probabilistico rappresenta un equilibrio tra la semplicità delle variabili casuali indipendenti e la complessità delle interazioni tra variabili. Per chiarire il concetto, consideriamo una sequenza temporale di eventi rappresentata da variabili casuali $X_0, X_1, ..., X_n, ...$. In molti fenomeni reali, queste variabili non sono né completamente indipendenti né totalmente interdipendenti. Una catena di Markov rappresenta un compromesso tra questi due estremi.\n",
    "\n",
    "In questo contesto, ci concentreremo su catene di Markov con stati discreti e tempo discreto. Ciò significa che le variabili $X_n$ possono assumere valori in un insieme finito, tipicamente indicato come $\\{1, 2, ..., M\\}$, e che gli eventi si verificano in momenti distinti e numerabili. La proprietà fondamentale di una catena di Markov può essere espressa matematicamente con la seguente equazione:\n",
    "\n",
    "$$\n",
    "P(X_{n+1} = j | X_n = i, X_{n-1} = i_{n-1}, ..., X_0 = i_0) = P(X_{n+1} = j | X_n = i).\n",
    "$$\n",
    "\n",
    "Questa equazione indica che il futuro (rappresentato da $X_{n+1}$) dipende solo dal presente ($X_n$) e non dal passato ($X_{n-1}, ..., X_0$). La proprietà di Markov può essere vista come un primo allentamento dell'assunzione di indipendenza: le variabili casuali sono dipendenti in un modo specifico che risulta matematicamente conveniente.\n",
    "\n",
    "Quantità importanti associate a una catena di Markov sono le probabilità condizionate, chiamate *probabilità di transizione*:\n",
    "\n",
    "$$\n",
    "P(X_{n+1} = j \\mid X_n = i).\n",
    "$$\n",
    "\n",
    "La probabilità $P(X_{n+1} = j \\mid X_n = i)$, nota come *probabilità di transizione*, rappresenta la probabilità di passare dallo stato $i$ allo stato $j$ in un singolo passo.\n",
    "\n",
    "Per descrivere completamente una catena di Markov, si utilizza una matrice $Q$, chiamata *matrice di transizione*. Questa è una matrice $M \\times M$ in cui ogni elemento $q_{ij}$ rappresenta la probabilità di transizione dallo stato $i$ allo stato $j$. Un'importante caratteristica della matrice di transizione è che la somma degli elementi di ogni riga deve essere pari a 1, poiché partendo da uno stato qualsiasi, il sistema deve necessariamente transitare in uno degli stati possibili.\n",
    "\n",
    "Per chiarire ulteriormente il concetto, consideriamo un modello di previsione del tempo a Firenze con tre possibili condizioni meteorologiche: soleggiato, piovoso e nebbioso. Di seguito è riportata una matrice di transizione che rappresenta le probabilità di passaggio da un tipo di tempo all'altro:\n",
    "\n",
    "| Tempo Oggi  | Soleggiato | Piovoso | Nebbioso |\n",
    "|-------------|------------|---------|----------|\n",
    "| **Soleggiato** | 0.8        | 0.15    | 0.05     |\n",
    "| **Piovoso**    | 0.2        | 0.6     | 0.2      |\n",
    "| **Nebbioso**   | 0.2        | 0.3     | 0.5      |\n",
    "\n",
    "Vediamo come calcolare alcune probabilità:\n",
    "\n",
    "- Probabilità di avere due giorni consecutivi (prima soleggiato, poi piovoso) dato che oggi è soleggiato:\n",
    "\n",
    "   $$ \n",
    "   P(\\text{Soleggiato domani, Piovoso dopodomani} \\mid \\text{Soleggiato oggi}) = 0.8 \\times 0.15 = 0.12.\n",
    "   $$\n",
    "\n",
    "- Probabilità di pioggia tra due giorni, dato che oggi è nebbioso:\n",
    "\n",
    "   $$\n",
    "   P(\\text{Piovoso dopodomani} \\mid \\text{Nebbioso oggi}) = 0.5 \\times 0.2 + 0.3 \\times 0.6 + 0.2 \\times 0.15 = 0.31.\n",
    "   $$\n",
    "\n",
    "Il modello di Markov di base assume che le probabilità di transizione rimangano costanti nel tempo, una proprietà nota come *omogeneità temporale*. Questo significa che, per esempio, la probabilità di passare dallo stato \"soleggiato\" allo stato \"piovoso\" è la stessa in qualsiasi periodo dell'anno.\n",
    "\n",
    "In sintesi, l'utilità del modello di Markov risiede nella sua capacità di semplificare notevolmente i calcoli probabilistici. Invece di considerare l'intera storia passata del sistema, è sufficiente conoscere solo lo stato attuale per fare previsioni sul futuro. Questa caratteristica, nota come \"assenza di memoria\", rende il modello estremamente utile in molte applicazioni pratiche, dalla modellazione di fenomeni naturali alla progettazione di algoritmi di apprendimento automatico. Il processo di Markov di primo ordine offre uno strumento potente per analizzare e prevedere il comportamento di sistemi complessi nel tempo, bilanciando la necessità di catturare le dipendenze temporali con la semplicità computazionale. La sua versatilità e applicabilità in diversi campi lo rendono un concetto chiave per comprendere e modellare molti fenomeni del mondo reale.\n",
    "\n",
    "## Classificazione degli Stati\n",
    "\n",
    "Consideriamo ora la terminologia usata per descrivere le varie caratteristiche di una catena di Markov. \n",
    "\n",
    "### Catene di Markov Irriducibili\n",
    "\n",
    "Una catena di Markov è detta *irriducibile* se, per qualsiasi coppia di stati $i$ e $j$, esiste una probabilità positiva di passare dallo stato $i$ allo stato $j$ in un numero finito di passi. In altre parole, una catena irriducibile non ha stati isolati: ogni stato è raggiungibile da qualsiasi altro stato.\n",
    "\n",
    "### Stati Ricorrenti\n",
    "\n",
    "Uno *stato* $i$ di una catena di Markov si dice *ricorrente* se, partendo da $i$, la probabilità di ritornarvi è uguale a 1. Questo implica che una volta raggiunto lo stato $i$, è garantito che la catena tornerà in $i$ prima o poi. Gli stati ricorrenti possono essere ulteriormente classificati come:\n",
    "\n",
    "- *Positivamente ricorrenti*: uno stato è positivamente ricorrente se il tempo medio atteso per ritornare in quello stato, partendo da esso, è finito.\n",
    "- *Nullamente ricorrenti*: uno stato è nullamente ricorrente se il tempo medio atteso per ritornare in quello stato è infinito.\n",
    "- *Ricorrenti di Harris*: sono stati ricorrenti che vengono visitati infinite volte quando il tempo tende all'infinito, garantendo una certa frequenza di visita.\n",
    "\n",
    "### Aperiodicità\n",
    "\n",
    "Uno *stato* $i$ si dice *aperiodico* se il massimo comun divisore dei tempi di ritorno in $i$ è 1. In altre parole, uno stato è aperiodico se non esiste un ciclo deterministico che vincola i tempi in cui la catena può ritornare in quello stato. Una catena di Markov è aperiodica se tutti i suoi stati sono aperiodici. L'aperiodicità evita che la catena rimanga bloccata in una sequenza ciclica fissa, permettendo un comportamento più variegato nel tempo.\n",
    "\n",
    "### Stazionarietà\n",
    "\n",
    "Nella teoria delle catene di Markov, una *distribuzione stazionaria* $\\pi$ è una distribuzione di probabilità sugli stati tale che, se la catena parte con questa distribuzione iniziale, la distribuzione delle probabilità rimane invariata nel tempo. Matematicamente, se $\\pi$ è la distribuzione stazionaria, allora $\\pi P = \\pi$, dove $P$ è la matrice di transizione della catena di Markov. La stazionarietà è fondamentale per analizzare il comportamento a lungo termine della catena, poiché una volta raggiunta, la distribuzione di probabilità sugli stati rimane costante.\n",
    "\n",
    "### Ergodicità\n",
    "\n",
    "Una catena di Markov si dice *ergodica* se è irriducibile e aperiodica, e tutti i suoi stati sono positivamente ricorrenti. Questo implica che la catena, nel lungo termine, visita tutti gli stati secondo una distribuzione di probabilità che diventa stabile (stazionaria) e non dipende dallo stato iniziale. La proprietà di ergodicità è cruciale in quanto garantisce che le medie temporali di una funzione sugli stati della catena convergono alle medie rispetto alla distribuzione stazionaria.\n",
    "\n",
    "### Convergenza\n",
    "\n",
    "La *convergenza* di una catena di Markov si riferisce al processo mediante il quale la distribuzione di probabilità degli stati si avvicina alla distribuzione stazionaria $\\pi$ man mano che il numero di passi $n$ tende all'infinito. In altre parole, indipendentemente dalla distribuzione iniziale degli stati, la distribuzione della catena dopo un lungo periodo di tempo sarà vicina a $\\pi$. Questo concetto è strettamente legato alla stazionarietà, poiché la convergenza descrive il percorso verso l'equilibrio, mentre la stazionarietà rappresenta lo stato di equilibrio raggiunto.\n",
    "\n",
    "## Sommario \n",
    "\n",
    "Una catena di Markov è una sequenza di variabili aleatorie $X_0, X_1, X_2, \\ldots$ che soddisfa la cosiddetta proprietà di Markov. Questa proprietà stabilisce che, dato lo stato attuale della catena, il futuro è indipendente dal passato. Formalmente, questa proprietà si esprime come:\n",
    "\n",
    "$$\n",
    "P(X_{n+1} = j \\mid X_n = i, X_{n-1} = i_{n-1}, \\ldots, X_0 = i_0) = P(X_{n+1} = j \\mid X_n = i) = q_{ij},\n",
    "$$\n",
    "\n",
    "dove $q_{ij}$ rappresenta la probabilità di passare dallo stato $i$ allo stato $j$ in un singolo passo. Queste probabilità di transizione sono organizzate in una matrice di transizione $Q = (q_{ij})$, in cui ogni riga corrisponde a una distribuzione di probabilità condizionata sui possibili stati futuri dato l'attuale stato della catena. \n",
    "\n",
    "La distribuzione di probabilità degli stati della catena dopo $n$ passi può essere calcolata moltiplicando la matrice di transizione $Q$ elevata alla potenza $n$ per il vettore di probabilità iniziale $s$, che descrive la distribuzione di probabilità degli stati al tempo $0$. In simboli, questo è rappresentato da $sQ^n$, che fornisce la distribuzione di probabilità marginale degli stati dopo $n$ passi. \n",
    "\n",
    "Gli stati di una catena di Markov possono essere classificati come ricorrenti o transitori. Uno stato è *ricorrente* se la catena torna a questo stato ripetutamente nel tempo; è *transitorio* se la catena potrebbe lasciare questo stato per non ritornarvi mai più. Gli stati possono anche avere un periodo associato, definito come il massimo comun divisore dei numeri di passi necessari per ritornare allo stato stesso. Una catena di Markov è detta *irriducibile* se è possibile raggiungere qualsiasi stato da qualsiasi altro stato in un numero finito di passi, ed è *aperiodica* se ogni stato ha periodo 1.\n",
    "\n",
    "Una *distribuzione stazionaria* di una catena di Markov è una distribuzione di probabilità che rimane invariata nel tempo. Se la catena inizia con questa distribuzione, continuerà a mantenerla in ogni passo successivo. Questa condizione si esprime matematicamente come $sQ = s$, dove $s$ è il vettore di probabilità stazionaria e $Q$ è la matrice di transizione. Per una catena di Markov finita che è irriducibile e aperiodica, esiste una distribuzione stazionaria unica verso la quale la catena converge indipendentemente dalla distribuzione iniziale.\n",
    "\n",
    "Un concetto importante nelle catene di Markov è quello di *reversibilità*. Una catena di Markov è detta *reversibile* se esiste una distribuzione di probabilità $s$ tale che, per ogni coppia di stati $i$ e $j$, la condizione di reversibilità $s_i q_{ij} = s_j q_{ji}$ sia soddisfatta. Questa condizione garantisce che $s$ sia una distribuzione stazionaria per la catena. Le catene di Markov reversibili sono particolarmente utili in applicazioni pratiche come gli algoritmi di simulazione Monte Carlo, ad esempio l'algoritmo di Metropolis-Hastings, poiché permettono di progettare catene che convergono rapidamente alla distribuzione di probabilità desiderata.\n",
    "\n",
    "## Letteratura\n",
    "\n",
    "I metodi markoviani sono stati ampiamente utilizzati in vari ambiti della psicologia e dell'educazione. Una delle applicazioni più comuni di questi metodi è il clustering dei dati sequenziali (@tormanen2022person; @tormanen2023affective; @fincham2018study). Ad esempio, i modelli nascosti di Markov (HMM) sono stati utilizzati per raggruppare le sequenze di dati tracciati da sistemi di gestione dell'apprendimento (LMS) degli studenti, al fine di identificare i loro schemi di attività, ovvero le tattiche e strategie di apprendimento (@fincham2018study). Un uso molto diffuso dei modelli di Markov di primo ordine è quello di mappare le transizioni degli studenti tra diverse attività di apprendimento. Per esempio, @matcha2020analytics ha utilizzato modelli di Markov di primo ordine per studiare i processi di transizione degli studenti tra diverse strategie di apprendimento. Altri usi includono lo studio delle transizioni tra diverse strategie di scrittura accademica (@peeters2020applying), tra eventi di apprendimento autoregolato (@lim2023effects), o all'interno di contesti di apprendimento collaborativo (@saqr2023temporal). Esempi più specifici nell'ambito psicologico comprendono lo studio delle influenze reciproche tra stati affettivi (@cipresso2023affects) e l'analisi degli effetti psicologici che dipendono dal tempo nelle sequenze di decision-making (@gunawan2022time).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
